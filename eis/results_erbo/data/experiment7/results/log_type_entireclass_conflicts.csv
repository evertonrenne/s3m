/experiment_ambrose/projects/ambrose/revisions/rev_abe0afe_682cd4a/rev_abe0afe-682cd4a/pig/src/main/java/com/twitter/ambrose/service/impl/SugiyamaLayoutTransformer.java;<<<<<<< MINE
=======
/*
Copyright 2012 Twitter, Inc.

Licensed under the Apache License, Version 2.0 (the "License");
you may not use this file except in compliance with the License.
You may obtain a copy of the License at

http://www.apache.org/licenses/LICENSE-2.0

Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an "AS IS" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.
*/
package com.twitter.ambrose.service.impl;

import azkaban.common.utils.Props;
import azkaban.workflow.Flow;
import azkaban.workflow.flow.DagLayout;
import azkaban.workflow.flow.FlowNode;
import azkaban.workflow.flow.SugiyamaLayout;
import com.twitter.ambrose.service.DAGNode;
import com.twitter.ambrose.service.DAGTransformer;

import java.util.ArrayList;
import java.util.Arrays;
import java.util.Collection;

/**
 * Transformer that wraps Azkaban's SugiyamaLayout to create level, X and Y values for the nodes in
 * the DAG.
 * @author billg
 */
public class SugiyamaLayoutTransformer implements DAGTransformer {

  private boolean landscape;

  /**
   * Create an instance of this class to generate top-down coordinates
   */
  public SugiyamaLayoutTransformer() {
    this(false);
  }

  /**
   * Create an instance of this class to generate coordinates. If <pre>landscape=true</pre>, then
   * the graph will layout from left to right. Otherwise it will layout from top to bottom.
   */
  public SugiyamaLayoutTransformer(boolean landscape) {
    this.landscape = landscape;
  }

  @Override
  public Collection<DAGNode> transform(Collection<DAGNode> nodes) {
    Flow flow = new Flow("sample flow", new Props());

    if(nodes.size() == 1) {
      flow.addDependencies(nodes.iterator().next().getName(), new ArrayList<String>());
    } else {
      for(DAGNode node : nodes) {
        for(String successor : node.getSuccessorNames()) {
          flow.addDependencies(successor, Arrays.asList(node.getName()));
        }
      }
    }

    flow.validateFlow(); // this sets levels
    flow.printFlow();
    if (!flow.isLayedOut()) {
      DagLayout layout = new SugiyamaLayout(flow);
      layout.setLayout();
    }

    for(DAGNode node : nodes) {
      FlowNode flowNode = flow.getFlowNode(node.getName());

      // invert X/Y if we're rendering in landscape
      if (landscape) {
        node.setX(flowNode.getY());
        node.setY(flowNode.getX());
      }
      else {
        node.setX(flowNode.getX());
        node.setY(flowNode.getY());
      }

      node.setDagLevel(flowNode.getLevel());
    }

    return nodes;
  }
}>>>>>>> YOURS
/experiment_ambrose/projects/ambrose/revisions/rev_01f0331_a35dfeb/rev_01f0331-a35dfeb/hive/src/main/java/com/twitter/ambrose/hive/AmbroseHiveStatPublisher.java;<<<<<<< MINE
=======
package com.twitter.ambrose.hive;

import java.io.IOException;
import java.util.HashMap;
import java.util.Map;

import org.apache.commons.logging.Log;
import org.apache.commons.logging.LogFactory;
import org.apache.hadoop.conf.Configuration;
import org.apache.hadoop.hive.conf.HiveConf;
import org.apache.hadoop.hive.ql.session.SessionState;
import org.apache.hadoop.hive.ql.stats.ClientStatsPublisher;
import org.apache.hadoop.mapred.JobClient;
import org.apache.hadoop.mapred.JobConf;
import org.apache.hadoop.mapred.JobID;
import org.apache.hadoop.mapred.RunningJob;

import com.twitter.ambrose.model.DAGNode;
import com.twitter.ambrose.model.Event;
import com.twitter.ambrose.model.Event.WorkflowProgressField;
import com.twitter.ambrose.model.Job;
import com.twitter.ambrose.model.hadoop.MapReduceJobState;

/**
 * Hook that is invoked every <tt>hive.exec.counters.pull.interval</tt> seconds
 * to report a given job's status to {@link com.twitter.ambrose.hive.HiveProgressReporter HiveProgressReporter}
 * <br>
 * If <tt>hive.exec.parallel</tt> is set each thread obtain an instance from this class.
 * 
 * @author Lorand Bendig <lbendig@gmail.com>
 *
 */
public class AmbroseHiveStatPublisher implements ClientStatsPublisher {

    private static final Log LOG = LogFactory.getLog(AmbroseHiveStatPublisher.class);
    
    /** Running job information */
    private final JobClient jobClient;
    private RunningJob rj;
    private HiveMapReduceJobState jobProgress;

    private String nodeId;
    private JobID jobId;
    
    private int totalReduceTasks;
    private int totalMapTasks;
    
    private final Map<WorkflowProgressField, String> eventData = 
        new HashMap<WorkflowProgressField, String>(1);

    private boolean init = true;

    private static class HiveMapReduceJobState extends MapReduceJobState {
        
        public HiveMapReduceJobState(String jobIdStr, RunningJob rj, int totalMapTasks,
                int totalReduceTasks) throws IOException {

            setJobId(jobIdStr);
            setJobName(rj.getJobName());
            setTrackingURL(rj.getTrackingURL());
            setComplete(rj.isComplete());
            setSuccessful(rj.isSuccessful());
            setMapProgress(rj.mapProgress());
            setReduceProgress(rj.reduceProgress());
            setTotalMappers(totalMapTasks);
            setTotalReducers(totalReduceTasks);
        }
        
        public boolean update(RunningJob rj) throws IOException {

            boolean complete = rj.isComplete();
            boolean successful = rj.isSuccessful();
            float mapProgress = rj.mapProgress();
            float reduceProgress = rj.reduceProgress();

            boolean update = !(isComplete() == complete
                    && isSuccessful() == successful
                    && AmbroseHiveUtil.isEqual(getMapProgress(), mapProgress)
                    && AmbroseHiveUtil.isEqual(getReduceProgress(), reduceProgress));
            
            if (update) {
                setComplete(complete);
                setSuccessful(successful);
                setMapProgress(mapProgress);
                setReduceProgress(reduceProgress);
            }
            return update;
        }
        
        public int getProgress() {
            float result =((getMapProgress() + getReduceProgress()) * 100) / 2;
            return (int)result;
        }
        
    }

    public AmbroseHiveStatPublisher() throws IOException {
        Configuration conf = SessionState.get().getConf();
        this.jobClient = new JobClient(new JobConf(conf));
    }

    @Override
    public void run(Map<String, Double> counterValues, String jobIdStr) {
        if (init) {
            init(jobIdStr);
            init = false;
        }
        //send job statistics to the Ambrose server
        send(jobIdStr, counterValues);
    }

    private void init(String jobIDStr) {
        try {
            jobId = JobID.forName(jobIDStr);
            rj = jobClient.getJob(jobId);
            nodeId = AmbroseHiveUtil.getNodeIdFromJob(SessionState.get().getConf(), rj);
            totalMapTasks = jobClient.getMapTaskReports(jobId).length;
            totalReduceTasks = jobClient.getReduceTaskReports(jobId).length;
        }
        catch (IOException e) {
            LOG.error("Error getting running job for id : " + jobIDStr, e);
        }
    }
    
    private void send(String jobIDStr, Map<String, Double> counterValues) {
       
        HiveProgressReporter reporter = HiveProgressReporter.get();
        Configuration conf = SessionState.get().getConf();
        String queryId = AmbroseHiveUtil.getHiveQueryId(conf);
        Map<String, DAGNode<Job>> nodeIdToDAGNode = reporter.getNodeIdToDAGNode();

        DAGNode<Job> dagNode = nodeIdToDAGNode.get(nodeId);
        if (dagNode == null) {
            LOG.warn("jobStartedNotification - unrecorgnized operator name found for " + "jobId "
                    + jobIDStr);
            return;
        }
        HiveJob job = (HiveJob) dagNode.getJob();
        //a job has been started
        if (job.getId()== null) {
            //job identifier on GUI
            job.setId(AmbroseHiveUtil.asDisplayId(queryId, jobIDStr, nodeId));
            reporter.addJobIdToNodeId(jobIDStr, nodeId);
            reporter.pushEvent(queryId, new Event.JobStartedEvent(dagNode));
        }
        try {

            boolean update = false;
            if (jobProgress == null) {
                jobProgress = new HiveMapReduceJobState(jobIDStr, rj, totalMapTasks, totalReduceTasks);
                update = true;
            }
            else {
                update = jobProgress.update(rj);
            }
            
            if (update && !reporter.getCompletedJobIds().contains(jobIDStr)) {
                reporter.addJobIdToProgress(jobIDStr, jobProgress.getProgress());
                job.setMapReduceJobState(jobProgress);
                pushWorkflowProgress(queryId, reporter);
                reporter.pushEvent(queryId, new Event.JobProgressEvent(dagNode));
                if (jobProgress.isComplete()) {
                    reporter.addCompletedJobIds(jobIDStr);
                    job.setJobStats(counterValues, jobProgress.getTotalMappers(),
                            jobProgress.getTotalReducers());
                    job.setConfiguration(((HiveConf) conf).getAllProperties());
                    reporter.addJob(job);
                    reporter.pushEvent(queryId, new Event.JobFinishedEvent(dagNode));
                }
            }
        }
        catch (IOException e) {
            LOG.error("Error getting job info!", e);
        }
    }

    private void pushWorkflowProgress(String queryId, HiveProgressReporter reporter) {
        eventData.put(WorkflowProgressField.workflowProgress,
                Integer.toString(reporter.getOverallProgress()));
        reporter.pushEvent(queryId, new Event.WorkflowProgressEvent(eventData));
    }
    
}>>>>>>> YOURS
/experiment_ambrose/projects/ambrose/revisions/rev_01f0331_a35dfeb/rev_01f0331-a35dfeb/hive/src/main/java/com/twitter/ambrose/hive/HiveJob.java;<<<<<<< MINE
=======
package com.twitter.ambrose.hive;

import java.util.HashMap;
import java.util.Map;

import org.apache.commons.logging.Log;
import org.apache.commons.logging.LogFactory;

import com.fasterxml.jackson.annotation.JsonCreator;
import com.fasterxml.jackson.annotation.JsonIgnore;
import com.fasterxml.jackson.annotation.JsonProperty;
import com.fasterxml.jackson.annotation.JsonSubTypes;
import com.fasterxml.jackson.annotation.JsonTypeName;
import com.twitter.ambrose.model.Job;
import com.twitter.ambrose.model.hadoop.CounterGroup;
import com.twitter.ambrose.model.hadoop.MapReduceJobState;
import com.twitter.ambrose.util.JSONUtil;

/**
 * Subclass of Job used to hold initialization logic and Hive-specific bindings
 * for a Job. Encapsulates all information related to a run of a Hive job. A job
 * might have counters, job configuration and job metrics. Job metrics is
 * metadata about the job run that isn't set in the job configuration.
 * 
 * @author Lorand Bendig <lbendig@gmail.com>
 * 
 */
@JsonTypeName("hive")
public class HiveJob extends Job {
    
    private static final Log LOG = LogFactory.getLog(HiveJob.class);

    private final String[] aliases;
    private final String[] features;
    private MapReduceJobState mapReduceJobState;

    private Map<String, CounterGroup> counterGroupMap;

    public HiveJob(String[] aliases, String[] features) {
        super();
        this.aliases = aliases;
        this.features = features;
        // TODO: inputInfoList and outputInfoList?
    }

    @JsonCreator
    public HiveJob(@JsonProperty("id") String id, 
            @JsonProperty("aliases") String[] aliases,
            @JsonProperty("features") String[] features,
            @JsonProperty("mapReduceJobState") MapReduceJobState mapReduceJobState,
            @JsonProperty("counterGroupMap") Map<String, CounterGroup> counterGroupMap) {
        this(aliases, features);
        setId(id);
        this.mapReduceJobState = mapReduceJobState;
        this.counterGroupMap = counterGroupMap;
    }
    
    public String[] getAliases() {
        return aliases;
    }

    public String[] getFeatures() {
        return features;
    }

    public MapReduceJobState getMapReduceJobState() {
        return mapReduceJobState;
    }

    public void setMapReduceJobState(MapReduceJobState mapReduceJobState) {
        this.mapReduceJobState = mapReduceJobState;
    }

    public CounterGroup getCounterGroupInfo(String name) {
        return counterGroupMap == null ? null : counterGroupMap.get(name);
    }

    @JsonIgnore
    public void setJobStats(Map<String, Double> counterNameToValue, int totalMappers,
            int totalReducers) {
        counterGroupMap = AmbroseHiveUtil.counterGroupInfoMap(counterNameToValue);

        // job metrics
        Map<String, Number> metrics = new HashMap<String, Number>();
        metrics.put("numberMaps", totalMappers);
        metrics.put("numberReduces", totalReducers);
        metrics.put("avgMapTime",
                getCounterValue(counterNameToValue, MetricsCounter.SLOTS_MILLIS_MAPS)
                        / totalMappers);
        metrics.put("avgMapTime",
                getCounterValue(counterNameToValue, MetricsCounter.SLOTS_MILLIS_REDUCES)
                        / totalReducers);
        metrics.put("bytesWritten",
                getCounterValue(counterNameToValue, MetricsCounter.FILE_BYTES_WRITTEN));
        metrics.put("hdfsBytesWritten",
                getCounterValue(counterNameToValue, MetricsCounter.HDFS_BYTES_WRITTEN));
        metrics.put("mapInputRecords",
                getCounterValue(counterNameToValue, MetricsCounter.MAP_INPUT_RECORDS));
        metrics.put("mapOutputRecords",
                getCounterValue(counterNameToValue, MetricsCounter.MAP_OUTPUT_RECORDS));
        metrics.put("proactiveSpillCountRecs",
                getCounterValue(counterNameToValue, MetricsCounter.SPILLED_RECORDS));
        metrics.put("reduceInputRecords",
                getCounterValue(counterNameToValue, MetricsCounter.REDUCE_INPUT_RECORDS));
        metrics.put("reduceOutputRecords",
                getCounterValue(counterNameToValue, MetricsCounter.REDUCE_OUTPUT_RECORDS));
        setMetrics(metrics);
    }

    /**
     * This is a hack to get around how the json library requires subtype info to be defined on the
     * super-class, which doesn't always have access to the subclasses at compile time. Since the
     * mixinAnnotations method replaces the existing annotation, this means that an action like this
     * will need to be taken once upon app startup to register all known types. If this action
     * happens multiple times, calls will override each other.
     * @see com.twitter.ambrose.pig.HiveJob#mixinJsonAnnotations()
     */
    public static void mixinJsonAnnotations() {
        LOG.info("Mixing in JSON annotations for HiveJob and Job into Job");
        JSONUtil.mixinAnnotatons(Job.class, AnnotationMixinClass.class);
    }

    @JsonSubTypes({
            @JsonSubTypes.Type(value = com.twitter.ambrose.model.Job.class, name = "default"),
            @JsonSubTypes.Type(value = com.twitter.ambrose.hive.HiveJob.class, name = "hive") })
    private static class AnnotationMixinClass {}
    
    
    private Double getCounterValue(Map<String, Double> counterNameToValue, MetricsCounter hjc) {
        String [] keys = MetricsCounter.get(hjc);
        return (counterNameToValue.get(keys[0]) == null) ?
                counterNameToValue.get(keys[1]) : counterNameToValue.get(keys[0]);
    }

}>>>>>>> YOURS
/experiment_ambrose/projects/ambrose/revisions/rev_01f0331_a35dfeb/rev_01f0331-a35dfeb/hive/src/main/java/com/twitter/ambrose/hive/AmbroseHivePreHook.java;<<<<<<< MINE
=======
package com.twitter.ambrose.hive;

import java.util.HashMap;
import java.util.Map;

import org.apache.commons.logging.Log;
import org.apache.commons.logging.LogFactory;
import org.apache.hadoop.conf.Configuration;
import org.apache.hadoop.hive.ql.hooks.ExecuteWithHookContext;
import org.apache.hadoop.hive.ql.hooks.HookContext;

import com.twitter.ambrose.model.DAGNode;
import com.twitter.ambrose.model.Event;
import com.twitter.ambrose.model.Event.WorkflowProgressField;
import com.twitter.ambrose.model.Job;

/**
 * Hook invoked before running a workflow. <br>
 * Constructs DAGNode representation and initializes
 * {@link com.twitter.ambrose.hive.HiveProgressReporter HiveProgressReporter} <br>
 * Called by the main thread
 * 
 * @author Lorand Bendig <lbendig@gmail.com>
 * 
 */
public class AmbroseHivePreHook implements ExecuteWithHookContext {

    private static final Log LOG = LogFactory.getLog(AmbroseHivePreHook.class);

    /** Timeout in seconds for waiting between two workflows */
    private static final String WF_BETWEEN_SLEEP_SECS_PARAM = "ambrose.wf.between.sleep.seconds";
    private static final String SCRIPT_STARTED_PARAM = "ambrose.script.started";

    @Override
    public void run(HookContext hookContext) throws Exception {

        String queryId = AmbroseHiveUtil.getHiveQueryId(hookContext.getConf());
        HiveProgressReporter reporter = HiveProgressReporter.get();
        HiveDAGTransformer transformer = new HiveDAGTransformer(hookContext);
       
        //conditional tasks may be filtered out by Hive at runtime. We them as
        //'complete'
        Map<String, DAGNode<Job>> nodeIdToDAGNode = reporter.getNodeIdToDAGNode();
        sendFilteredJobsStatus(queryId, nodeIdToDAGNode);
        if (transformer.getTotalMRJobs() == 0) {
            return;
        }

        waitBetween(hookContext, queryId);

        nodeIdToDAGNode = transformer.getNodeIdToDAGNode();
        reporter.setNodeIdToDAGNode(nodeIdToDAGNode);
        reporter.setTotalMRJobs(transformer.getTotalMRJobs());
        reporter.sendDagNodeNameMap(queryId, nodeIdToDAGNode);

    }

    /**
     * Waiting <tt>ambrose.wf.between.sleep.seconds</tt> before processing the
     * next statement (workflow) in the submitted script
     * 
     * @param hookContext
     * @param queryId
     */
    private void waitBetween(HookContext hookContext, String queryId) {

        Configuration conf = hookContext.getConf();
        boolean justStarted = conf.getBoolean(SCRIPT_STARTED_PARAM, true);
        if (justStarted) {
            conf.setBoolean(SCRIPT_STARTED_PARAM, false);
        }
        else {
            // sleeping between workflows
            int sleepTimeMs = conf.getInt(WF_BETWEEN_SLEEP_SECS_PARAM, 10);
            try {

                LOG.info("One workflow complete, sleeping for " + sleepTimeMs
                        + " sec(s) before moving to the next one if exists. Hit ctrl-c to exit.");
                Thread.sleep(sleepTimeMs * 1000L);
                HiveProgressReporter.get().saveEventStack();
                HiveProgressReporter.reset();
            }
            catch (InterruptedException e) {
                LOG.warn("Sleep interrupted", e);
            }
        }
    }

    private void sendFilteredJobsStatus(String queryId, Map<String, DAGNode<Job>> nodeIdToDAGNode) {
        
        if (nodeIdToDAGNode == null) {
            return;
        }
        
        HiveProgressReporter reporter = HiveProgressReporter.get();
        Map<WorkflowProgressField, String> eventData = 
            new HashMap<Event.WorkflowProgressField, String>(1);

        int skipped = 0;
        for (DAGNode<Job> dagNode : nodeIdToDAGNode.values()) {
            Job job = dagNode.getJob();
            // filtered jobs don't have assigned jobId
            if (job.getId() != null) {
                continue;
            }
            String nodeId = dagNode.getName();
            job.setId(AmbroseHiveUtil.asDisplayId(queryId, "filtered out", nodeId));
            reporter.addJobIdToProgress(nodeId, 100);
            reporter.pushEvent(queryId, new Event.JobFinishedEvent(dagNode));
            skipped++;
        }
        // sleep so that all these events will be visible on GUI before going on
        try {
            Thread.sleep(skipped * 1000L);
        }
        catch (InterruptedException e) {
            LOG.warn("Sleep interrupted", e);
        }

        eventData.put(WorkflowProgressField.workflowProgress,
                Integer.toString(reporter.getOverallProgress()));
        reporter.pushEvent(queryId, new Event.WorkflowProgressEvent(eventData));

    }

}>>>>>>> YOURS
/experiment_ambrose/projects/ambrose/revisions/rev_01f0331_a35dfeb/rev_01f0331-a35dfeb/hive/src/main/java/com/twitter/ambrose/hive/AmbroseHiveFailHook.java;<<<<<<< MINE
=======
package com.twitter.ambrose.hive;

import java.io.Serializable;
import java.lang.reflect.Field;
import java.util.List;
import java.util.Properties;

import org.apache.commons.logging.Log;
import org.apache.commons.logging.LogFactory;
import org.apache.hadoop.hive.conf.HiveConf;
import org.apache.hadoop.hive.ql.exec.Task;
import org.apache.hadoop.hive.ql.exec.TaskResult;
import org.apache.hadoop.hive.ql.exec.TaskRunner;
import org.apache.hadoop.hive.ql.hooks.ExecuteWithHookContext;
import org.apache.hadoop.hive.ql.hooks.HookContext;

import com.twitter.ambrose.model.DAGNode;
import com.twitter.ambrose.model.Event;
import com.twitter.ambrose.model.Job;
import com.twitter.ambrose.model.hadoop.MapReduceJobState;

/**
 * Hook invoked when a job fails.
 * Updates job event to 'FAILED' and waits for <code>{@value #POST_SCRIPT_SLEEP_SECS_PARAM}</code> seconds
 * before exiting.
 * <br>
 * Called by the main thread
 * 
 * @author Lorand Bendig <lbendig@gmail.com>
 *
 */
public class AmbroseHiveFailHook implements ExecuteWithHookContext {
    
    private static final Log LOG = LogFactory.getLog(AmbroseHiveFailHook.class);
    private static final String POST_SCRIPT_SLEEP_SECS_PARAM = "ambrose.post.script.sleep.seconds";

    @Override
    public void run(HookContext hookContext) throws Exception {

        HiveConf conf = hookContext.getConf();
        Properties allConfProps = conf.getAllProperties();
        String queryId = AmbroseHiveUtil.getHiveQueryId(conf);

        HiveProgressReporter reporter = HiveProgressReporter.get();

        List<TaskRunner> completeTaskList = hookContext.getCompleteTaskList();
        Field _taskResultField = accessTaskResultField();
        for (TaskRunner taskRunner : completeTaskList) {
            TaskResult taskResult = (TaskResult) _taskResultField.get(taskRunner);
            //get non-running, failed jobs
            if (!taskResult.isRunning() && taskResult.getExitVal() != 0) {
                Task<? extends Serializable> task = taskRunner.getTask();
                String nodeId = AmbroseHiveUtil.getNodeIdFromNodeName(conf, task.getId());
                DAGNode<Job> dagNode = reporter.getDAGNodeFromNodeId(nodeId);
                HiveJob job = (HiveJob) dagNode.getJob();
                job.setConfiguration(allConfProps);
                MapReduceJobState mrJobState = getJobState(job);
                mrJobState.setSuccessful(false);
                reporter.addJob((Job) job);
                reporter.pushEvent(queryId, new Event.JobFailedEvent(dagNode));
            }
        }
        
        reporter.restoreEventStack();
        String sleepTime = System.getProperty(POST_SCRIPT_SLEEP_SECS_PARAM, "10");
        try {
            int sleepTimeSeconds = Integer.parseInt(sleepTime);

            LOG.info("Script failed but sleeping for " + sleepTimeSeconds
                    + " seconds to keep the HiveStats REST server running. Hit ctrl-c to exit.");
            
            Thread.sleep(sleepTimeSeconds * 1000L);
            reporter.stopServer();

        }
        catch (NumberFormatException e) {
            LOG.warn(POST_SCRIPT_SLEEP_SECS_PARAM + " param is not a valid number, not sleeping: "
                    + sleepTime);
        }
        catch (InterruptedException e) {
            LOG.warn("Sleep interrupted", e);
        }
    }
    
    /**
     * Accessess the TaskResult of the completed task
     * @return
     */
    private Field accessTaskResultField() {
        Field field = null;
        try {
            field = TaskRunner.class.getDeclaredField("result");
            field.setAccessible(true);
        }
        catch (Exception e) {
            LOG.fatal("Can't access to TaskResult at " + TaskRunner.class.getName() + "!");
            throw new RuntimeException("Incompatible Hive API found!", e);
        }
        return field;
    }
    
    private MapReduceJobState getJobState(HiveJob job) {
        MapReduceJobState jobState = job.getMapReduceJobState();
        if (jobState != null) {
            return jobState;
        }
        //if job fails immediately right after its submission 
        jobState = new MapReduceJobState();
        jobState.setJobId(job.getId());
        return jobState;
    }

}>>>>>>> YOURS
/experiment_ambrose/projects/ambrose/revisions/rev_01f0331_a35dfeb/rev_01f0331-a35dfeb/hive/src/main/java/com/twitter/ambrose/hive/AmbroseHiveUtil.java;<<<<<<< MINE
=======
package com.twitter.ambrose.hive;

import java.lang.reflect.Field;
import java.util.Map;
import java.util.regex.Matcher;
import java.util.regex.Pattern;

import org.apache.hadoop.conf.Configuration;
import org.apache.hadoop.hive.conf.HiveConf;
import org.apache.hadoop.hive.conf.HiveConf.ConfVars;
import org.apache.hadoop.mapred.Counters;
import org.apache.hadoop.mapred.Counters.Counter;
import org.apache.hadoop.mapred.RunningJob;

import com.twitter.ambrose.model.hadoop.CounterGroup;

/**
 * Utility for Ambrose-Hive related operations
 * 
 * @author Lorand Bendig <lbendig@gmail.com>
 * 
 */
public class AmbroseHiveUtil {

    private static final Pattern STAGEID_PATTERN = Pattern.compile("^.*\\((Stage\\-\\d+)\\)$",
            Pattern.DOTALL);

    private AmbroseHiveUtil() {
        throw new AssertionError("shouldn't be instantiated!");
    }

    /**
     * Constructs the jobTracker url based on the jobId.
     * @param jobID
     * @param conf
     * @return
     * @see org.apache.hadoop.hive.hwi#getJobTrackerURL(String)
     */
    public static String getJobTrackerURL(String jobID, HiveConf conf) {
        String jt = conf.get("mapred.job.tracker");
        String jth = conf.get("mapred.job.tracker.http.address");
        String[] jtparts = null;
        String[] jthttpParts = null;
        if (jt.equalsIgnoreCase("local")) {
            jtparts = new String[2];
            jtparts[0] = "local";
            jtparts[1] = "";
        }
        else {
            jtparts = jt.split(":");
        }
        if (jth.contains(":")) {
            jthttpParts = jth.split(":");
        }
        else {
            jthttpParts = new String[2];
            jthttpParts[0] = jth;
            jthttpParts[1] = "";
        }
        return jtparts[0] + ":" + jthttpParts[1] + "/jobdetails.jsp?jobid=" + jobID + "&refresh=30";
    }

    /**
     * Constructs Countergroups from job runtime statistics
     * 
     * @param counterNameToValue
     * @return
     */
    public static Map<String, CounterGroup> counterGroupInfoMap(
            Map<String, Double> counterNameToValue) {

        Counters counters = new Counters();
        for (Map.Entry<String, ? extends Number> entry : counterNameToValue.entrySet()) {

            String[] cNames = entry.getKey().split("::");
            String groupName = cNames[0];
            String counterName = cNames[1];
            Counter counter = counters.findCounter(groupName, counterName);
            counter.setValue(entry.getValue().longValue());
        }
        return CounterGroup.counterGroupInfoMap(counters);
    }

    public static String asDisplayId(String queryId, String jobIDStr, String nodeId) {
        String stageName = nodeId.substring(0, nodeId.indexOf('_'));
        String wfIdLastPart = queryId.substring(queryId.lastIndexOf('-') + 1, queryId.length());
        String displayJobId = String.format(jobIDStr + " (%s, query-id: ...%s)", 
                stageName, wfIdLastPart);
        return displayJobId;
    }
    
    public static String getNodeIdFromNodeName(Configuration conf, String nodeName) {
        return nodeName + "_" + getHiveQueryId(conf);
    }
    
    /**
     * Returns the nodeId of the given running job
     * <br>
     * E.g: Stage-1_[queryId]
     * 
     * @param conf
     * @param runningJob
     * @return
     */
    public static String getNodeIdFromJob(Configuration conf, RunningJob runningJob) {
        return getNodeIdFromJobName(conf, runningJob.getJobName());
    }

    /**
     * Retrieves the nodeId from the Hive SQL command
     * <br>
     * 
     * @param conf
     * @param jobName
     * @return
     */
    private static String getNodeIdFromJobName(Configuration conf, String jobName) {
        Matcher matcher = STAGEID_PATTERN.matcher(jobName);
        if (matcher.find()) {
            return getNodeIdFromNodeName(conf, matcher.group(1));
        }
        return null;
    }

    /**
     * Returns the Hive query id which identifies the current workflow
     * <br> 
     * Format: hive_[queryId]
     * 
     * @param conf
     * @return
     */
    public static String getHiveQueryId(Configuration conf) {
        return HiveConf.getVar(conf, ConfVars.HIVEQUERYID);
    }
    
    /**
     * Gets the temporary directory of the given job
     * 
     * @param conf
     * @return
     */
    public static String getJobTmpDir(Configuration conf) {
        String fsNameVar = HiveConf.getVar(conf, ConfVars.HADOOPFS);
        String fsName = fsNameVar.substring(0, fsNameVar.length() - 1);
        return fsName + HiveConf.getVar(conf, ConfVars.SCRATCHDIR, "");
    }
    
    /**
     * Gets the temporary local directory of the given job
     * 
     * @param conf
     * @return
     */
    public static String getJobTmpLocalDir(Configuration conf) {
        String fsNameVar = HiveConf.getVar(conf, ConfVars.HADOOPFS);
        String fsName = fsNameVar.substring(0, fsNameVar.length() - 1);
        return fsName + HiveConf.getVar(conf, ConfVars.LOCALSCRATCHDIR, "");
    }
    
    /**
     * Gets (non-accessible) field of a class 
     * 
     * @param clazz
     * @param fieldName
     * @return
     * @throws Exception
     */
    public static Field getInternalField(Class<?> clazz, String fieldName)
            throws Exception {
        Field field = clazz.getDeclaredField(fieldName);
        field.setAccessible(true);
        return field;
    }
    
    /**
     * Compares two float values
     * @param f1
     * @param f2
     * @return true if f1 and f2 are equal
     */
    public static boolean isEqual(float f1, float f2) {
        final float delta = 0.001f;
        return (Math.abs(f1 - f2) < delta) ? true : false;
    }
}>>>>>>> YOURS
/experiment_ambrose/projects/ambrose/revisions/rev_01f0331_a35dfeb/rev_01f0331-a35dfeb/hive/src/main/java/com/twitter/ambrose/hive/HiveDAGTransformer.java;<<<<<<< MINE
=======
package com.twitter.ambrose.hive;

import java.io.IOException;
import java.io.Serializable;
import java.util.ArrayList;
import java.util.Collections;
import java.util.HashSet;
import java.util.LinkedHashMap;
import java.util.List;
import java.util.Map;
import java.util.Set;
import java.util.concurrent.ConcurrentHashMap;
import java.util.concurrent.ConcurrentSkipListMap;
import java.util.regex.Matcher;
import java.util.regex.Pattern;

import org.apache.commons.logging.Log;
import org.apache.commons.logging.LogFactory;
import org.apache.hadoop.conf.Configuration;
import org.apache.hadoop.hive.ql.QueryPlan;
import org.apache.hadoop.hive.ql.exec.ExecDriver;
import org.apache.hadoop.hive.ql.exec.Operator;
import org.apache.hadoop.hive.ql.exec.Task;
import org.apache.hadoop.hive.ql.exec.Utilities;
import org.apache.hadoop.hive.ql.hooks.HookContext;
import org.apache.hadoop.hive.ql.plan.MapredWork;
import org.apache.hadoop.hive.ql.plan.api.Adjacency;
import org.apache.hadoop.hive.ql.plan.api.Graph;
import org.apache.hadoop.hive.ql.plan.api.OperatorType;

import com.twitter.ambrose.model.DAGNode;
import com.twitter.ambrose.model.Job;

/**
 * Creates a DAGNode representation from a Hive query plan
 * 
 * <pre>
 * This involves:
 * - collecting aliases, features for a given job
 * - getting dependencies between jobs
 * - creating DAGNodes for each job
 * </pre>
 * 
 * @author Lorand Bendig <lbendig@gmail.com>
 * 
 */
public class HiveDAGTransformer {

    private static final Log LOG = LogFactory.getLog(HiveDAGTransformer.class);
    private static final Pattern SUBQUERY_ALIAS = Pattern.compile("-subquery\\d+\\:([^\\-]+)");
    private static final String PENDING_JOB = "N/A";
    private static final String TEMP_JOB_ID = "temp. intermediate data";
    
    private final String tmpDir;
    private final String localTmpDir;
    private final QueryPlan queryPlan;
    private final List<ExecDriver> allTasks;
    private Map<String, DAGNode<Job>> nodeIdToDAGNode;

    private final Configuration conf;

    private static final String[] EMPTY_ARR = {};
    
    public HiveDAGTransformer(HookContext hookContext) {

        conf = hookContext.getConf();
        tmpDir = AmbroseHiveUtil.getJobTmpDir(conf);
        localTmpDir = AmbroseHiveUtil.getJobTmpLocalDir(conf);
        queryPlan = hookContext.getQueryPlan();
        allTasks = Utilities.getMRTasks(queryPlan.getRootTasks());
        if (!allTasks.isEmpty()) {
            createNodeIdToDAGNode();
        }
    }

    /**
     * Constructs DAGNodes for each Hive MR task
     * 
     * @return nodeId - DAGNode pairs
     */
    public Map<String, DAGNode<Job>> getNodeIdToDAGNode() {
        return nodeIdToDAGNode;
    }

    public int getTotalMRJobs() {
        return allTasks.size();
    }
    
    /**
     * Constructs DAGNodes for each Hive MR task
     */
    private void createNodeIdToDAGNode() {

        // creates DAGNodes: each node represents a MR job
        nodeIdToDAGNode = new ConcurrentSkipListMap<String, DAGNode<Job>>();
        for (Task<? extends Serializable> task : allTasks) {
            if (task.getWork() instanceof MapredWork) {
                DAGNode<Job> dagNode = asDAGNode(task);
                nodeIdToDAGNode.put(dagNode.getName(), dagNode);
            }
        }

        // get job dependencies
        Map<String, List<String>> nodeIdToDependencies = getNodeIdToDependencies();

        // wire DAGNodes
        for (Map.Entry<String, List<String>> entry : nodeIdToDependencies.entrySet()) {
            String nodeId = entry.getKey();
            List<String> successorIds = entry.getValue();
            DAGNode<Job> dagNode = nodeIdToDAGNode.get(nodeId);
            List<DAGNode<? extends Job>> dagSuccessors = new ArrayList<DAGNode<? extends Job>>(
                    successorIds.size());

            for (String sId : successorIds) {
                DAGNode<Job> successor = nodeIdToDAGNode.get(sId);
                dagSuccessors.add(successor);
            }
            dagNode.setSuccessors(dagSuccessors);
        }
    }

    /**
     * Converts job properties to a DAGNode representation
     * 
     * @param task
     * @return
     */
    private DAGNode<Job> asDAGNode(Task<? extends Serializable> task) {

        MapredWork mrWork = (MapredWork) task.getWork();
        List<String> indexTableAliases = getAllJobAliases(mrWork.getPathToAliases());
        String[] features = getFeatures(mrWork.getAllOperators(), task.getTaskTag());
        String[] displayAliases = getDisplayAliases(indexTableAliases);

        // DAGNode's name of a workflow is unique among all workflows in a
        DAGNode<Job> dagNode = new DAGNode<Job>(
                AmbroseHiveUtil.getNodeIdFromNodeName(conf, task.getId()),
                new HiveJob(displayAliases, features));
        // init empty successors
        dagNode.setSuccessors(new ArrayList<DAGNode<? extends Job>>());
        return dagNode;
    }

    /**
     * Get all job aliases displayed on the GUI
     * 
     * @param indexTableAliases
     * @return
     */
    private String[] getDisplayAliases(List<String> indexTableAliases) {
        if (indexTableAliases.isEmpty()) {
            return EMPTY_ARR;
        }
        Set<String> result = new HashSet<String>();
        for (String alias : indexTableAliases) {
            if (alias.startsWith(tmpDir) || alias.startsWith(localTmpDir)) {
                result.add(TEMP_JOB_ID);
            }
            else if (alias.contains("subquery")) {
                Matcher m = SUBQUERY_ALIAS.matcher(alias);
                String dot = "";
                StringBuilder sb = new StringBuilder();
                while (m.find()) {
                    sb.append(dot);
                    dot = ".";
                    sb.append(m.group(1));
                }
                result.add(sb.toString());
            }
            else if (!alias.contains(":")) {
                result.add(alias);
            }
            else {
                String[] parts = alias.split(":");
                if (parts.length == 2) {
                    result.add(parts[1]);
                }
                else {
                    result.add(PENDING_JOB);
                }
            }
        }
        return result.toArray(new String[result.size()]);
    }

    /**
     * Creates job feature list: consists of a tasktag and a set of operators
     * 
     * @param ops
     * @param taskTagId
     * @return
     */
    private String[] getFeatures(List<Operator<?>> ops, int taskTagId) {
        if (ops == null) {
            return EMPTY_ARR;
        }
        Set<String> features = new HashSet<String>();
        for (Operator<?> op : ops) {
            OperatorType opType = op.getType();
            // some operators are discarded
            if (!skipType(opType)) {
                features.add(opType.toString());
            }
        }

        // if taskTag is other than 'NO_TAG', include it in the feature list
        if (taskTagId == Task.NO_TAG) {
            return features.toArray(new String[features.size()]);
        }
        String[] result = features.toArray(new String[features.size() + 1]);
        result[result.length - 1] = TaskTag.get(taskTagId);
        return result;
    }

    private boolean skipType(OperatorType opType) {
        return (opType == OperatorType.FILESINK || 
                opType == OperatorType.REDUCESINK || 
                opType == OperatorType.TABLESCAN);
    }

    /**
     * Gets all job aliases
     * 
     * @param pathToAliases
     * @return
     */
    private List<String> getAllJobAliases(LinkedHashMap<String, ArrayList<String>> pathToAliases) {
        if (pathToAliases == null || pathToAliases.isEmpty()) {
            return Collections.emptyList();
        }
        List<String> result = new ArrayList<String>();
        for (List<String> aliases : pathToAliases.values()) {
            if (aliases != null && !aliases.isEmpty()) {
                result.addAll(aliases);
            }
        }
        return result;
    }

    /**
     * Collects dependencies for each node
     * 
     * @return
     */
    private Map<String, List<String>> getNodeIdToDependencies() {
        Map<String, List<String>> result = new ConcurrentHashMap<String, List<String>>();
        try {
            Graph stageGraph = queryPlan.getQueryPlan().getStageGraph();
            if (stageGraph == null) {
                return result;
            }
            List<Adjacency> adjacencies = stageGraph.getAdjacencyList();
            if (adjacencies == null) {
                return result;
            }
            for (Adjacency adj : adjacencies) {
                String nodeId = AmbroseHiveUtil.getNodeIdFromNodeName(conf, adj.getNode());
                if (!nodeIdToDAGNode.containsKey(nodeId)) {
                    continue;
                }
                List<String> children = adj.getChildren();
                if (children == null || children.isEmpty()) {
                    return result; // TODO check!
                }
                List<String> filteredAdjacencies = getMRAdjacencies(children, nodeIdToDAGNode);
                result.put(nodeId, filteredAdjacencies);
            }
        }
        catch (IOException e) {
            LOG.error("Couldn't get queryPlan!", e);
        }
        return result;
    }

    /**
     * Filters adjacency children not being MR jobs
     * 
     * @param adjChildren
     * @param nodeIdToDAGNode
     * @return list of nodeIds referring to MR jobs
     */
    private List<String> getMRAdjacencies(List<String> adjChildren,
            Map<String, DAGNode<Job>> nodeIdToDAGNode) {
        List<String> result = new ArrayList<String>();
        for (String nodeName : adjChildren) {
            String nodeId = AmbroseHiveUtil.getNodeIdFromNodeName(conf, nodeName);
            if (nodeIdToDAGNode.containsKey(nodeId)) {
                result.add(nodeId);
            }
        }
        return result;
    }

}>>>>>>> YOURS
/experiment_ambrose/projects/ambrose/revisions/rev_01f0331_a35dfeb/rev_01f0331-a35dfeb/hive/src/main/java/com/twitter/ambrose/hive/AmbroseHiveFinishHook.java;<<<<<<< MINE
=======
package com.twitter.ambrose.hive;

import java.io.File;
import java.io.FileNotFoundException;
import java.io.IOException;
import java.util.List;
import java.util.Map;
import java.util.Scanner;
import java.util.regex.Pattern;

import org.apache.commons.lang.StringUtils;
import org.apache.commons.logging.Log;
import org.apache.commons.logging.LogFactory;
import org.apache.hadoop.conf.Configuration;
import org.apache.hadoop.hive.cli.CliSessionState;
import org.apache.hadoop.hive.ql.HiveDriverRunHook;
import org.apache.hadoop.hive.ql.HiveDriverRunHookContext;
import org.apache.hadoop.hive.ql.MapRedStats;
import org.apache.hadoop.hive.ql.session.SessionState;

import com.twitter.ambrose.model.Workflow;

/**
 * Hook invoked when a workflow succeeds.
 * If the last statement (workflow) of the script was executed, it waits for 
 * <code>{@value #POST_SCRIPT_SLEEP_SECS_PARAM}</code> seconds before exiting otherwise
 * returns and the processing moves on to the next workflow.
 * <br>
 * Called by the main thread
 * 
 * @author Lorand Bendig <lbendig@gmail.com>
 * 
 */
public class AmbroseHiveFinishHook implements HiveDriverRunHook {

    private static final Log LOG = LogFactory.getLog(AmbroseHiveFinishHook.class);
    private static final String POST_SCRIPT_SLEEP_SECS_PARAM = "ambrose.post.script.sleep.seconds";

    /** Last workflow in the script to be processed */
    private final String lastCmd;

    public AmbroseHiveFinishHook() {
        lastCmd = getLastCmd();
    }

    @Override
    public void preDriverRun(HiveDriverRunHookContext hookContext) throws Exception {}

    @Override
    public void postDriverRun(HiveDriverRunHookContext hookContext) {

        Configuration conf = hookContext.getConf();
        HiveProgressReporter reporter = HiveProgressReporter.get();
        String workflowVersion = reporter.getWorkflowVersion();

        String queryId = AmbroseHiveUtil.getHiveQueryId(conf);
        if (workflowVersion == null) {
            LOG.warn("ScriptFingerprint not set for this script - not saving stats.");
        }
        else {
            Workflow workflow = new Workflow(queryId, workflowVersion, reporter.getJobs());
            outputStatsData(workflow);
            reporter.flushJsonToDisk();
        }
        displayStatistics();

        if (!isLastCommandProcessed(hookContext)) {
            return;
        }
        
        reporter.restoreEventStack();
        String sleepTime = System.getProperty(POST_SCRIPT_SLEEP_SECS_PARAM, "10");
        try {
            int sleepTimeSeconds = Integer.parseInt(sleepTime);

            LOG.info("Script complete but sleeping for " + sleepTimeSeconds
                    + " seconds to keep the HiveStats REST server running. Hit ctrl-c to exit.");
            Thread.sleep(sleepTimeSeconds * 1000L);
            reporter.stopServer();

        }
        catch (NumberFormatException e) {
            LOG.warn(POST_SCRIPT_SLEEP_SECS_PARAM + " param is not a valid number, not sleeping: "
                    + sleepTime);
        }
        catch (InterruptedException e) {
            LOG.warn("Sleep interrupted", e);
        }

    }

    private void outputStatsData(Workflow workflowInfo) {
        try {
            if (LOG.isDebugEnabled()) {
                LOG.debug("Collected stats for script:\n" + Workflow.toJSON(workflowInfo));
            }
        }
        catch (IOException e) {
            LOG.error("Error while outputting workflowInfo", e);
        }
    }

    private void displayStatistics() {
        HiveProgressReporter reporter = HiveProgressReporter.get();
        Map<String, String> jobIdToNodeId = reporter.getJobIdToNodeId();
        LOG.info("MapReduce Jobs Launched: ");
        List<MapRedStats> lastMapRedStats = SessionState.get().getLastMapRedStatsList();
        for (int i = 0; i < lastMapRedStats.size(); i++) {
            MapRedStats mrStats = lastMapRedStats.get(i);
            String jobId = mrStats.getJobId();
            String nodeId = jobIdToNodeId.get(jobId);
            StringBuilder sb = new StringBuilder();
            sb.append("Job ").append(i)
              .append(" (")
              .append(jobId)
              .append(", ")
              .append(nodeId)
              .append("): ")
              .append(mrStats);
            LOG.info(sb.toString());
        }
    }

    private boolean isLastCommandProcessed(HiveDriverRunHookContext hookContext) {
        String currentCmd = hookContext.getCommand();
        currentCmd = StringUtils.trim(currentCmd.replaceAll("\\n", ""));
        if (currentCmd.equals(lastCmd)) {
            return true;
        }
        return false;
    }

    private String getLastCmd() {
        CliSessionState cliss = (CliSessionState) SessionState.get();
        Scanner scanner = null;
        try {
            scanner = new Scanner(new File(cliss.fileName));
        }
        catch (FileNotFoundException e) {
            LOG.error("Can't find Hive script", e);
        }
        if (scanner == null) {
            return null;
        }
        Pattern delim = Pattern.compile(";");
        scanner.useDelimiter(delim);
        String lastLine = null;
        while (scanner.hasNext()) {
            String line = StringUtils.trim(scanner.next().replaceAll("\\n", ""));
            if (line.length() != 0 && !line.startsWith("--")) {
                lastLine = line;
            }
        }
        return lastLine;
    }

}>>>>>>> YOURS
/experiment_ambrose/projects/ambrose/revisions/rev_01f0331_a35dfeb/rev_01f0331-a35dfeb/hive/src/test/java/com/twitter/ambrose/model/HiveJobTest.java;<<<<<<< MINE
=======
package com.twitter.ambrose.model;

import static org.junit.Assert.assertArrayEquals;
import static org.junit.Assert.assertEquals;
import static org.junit.Assert.assertNotNull;
import static org.junit.Assert.assertTrue;

import java.io.IOException;
import java.util.HashMap;
import java.util.Map;
import java.util.Properties;

import org.junit.Before;
import org.junit.Test;

import com.twitter.ambrose.hive.HiveJob;

/**
 * Unit tests for {@link com.twitter.ambrose.model.HiveJobTest}.
 */
public class HiveJobTest {
    static {
        HiveJob.mixinJsonAnnotations();
    }

    private HiveJob hiveJob;

    @Before
    public void setUp() throws Exception {
        Map<String, Number> metrics = new HashMap<String, Number>();
        metrics.put("somemetric", 6);
        Properties properties = new Properties();
        properties.setProperty("someprop", "propvalue");
        String[] aliases = new String[] { "alias1" };
        String[] features = new String[] { "feature1" };
        hiveJob = new HiveJob(aliases, features);
    }

    @Test
    public void testHiveJobRoundTrip() throws IOException {
        doTestRoundTrip(hiveJob);
    }

    private void doTestRoundTrip(HiveJob expected) throws IOException {
        String asJson = expected.toJson();
        Job asJobAgain = Job.fromJson(asJson);

        // assert that if we get a HiveJob without having to ask for it
        // explicitly
        assertTrue(asJobAgain instanceof HiveJob);
        assertJobEquals(expected, (HiveJob) asJobAgain);
    }

    @Test
    public void testDAGNodeHiveJobRoundTrip() throws IOException {
        DAGNode<HiveJob> node = new DAGNode<HiveJob>("dag name", hiveJob);
        doTestRoundTrip(node);
    }

    @Test
    public void testFromJson() throws IOException {
        String json = 
        "{" +
        "  \"type\" : \"JOB_STARTED\"," +
        "  \"payload\" : {" +
        "    \"name\" : \"Stage-1_user_20130723105858_3f0d530c-34a6-4bb9-8964-22c4ea289895\"," +
        "    \"job\" : {" +
        "      \"runtime\" : \"hive\"," +
        "      \"id\" : \"job_201307231015_0004 (Stage-1, query-id: ...22c4ea289895)\"," +
        "      \"aliases\" : [ \"src\" ]," +
        "      \"features\" : [ \"SELECT\", \"FILTER\" ]" +
        "    }," +
        "    \"successorNames\" : [ ]" +
        "  }," +
        "  \"id\" : 1," +
        "  \"timestamp\" : 1374569908714" +
        "}, {" +
        "  \"type\" : \"WORKFLOW_PROGRESS\"," +
        "  \"payload\" : {" +
        "    \"workflowProgress\" : \"0\"" +
        "  }," +
        "  \"id\" : 2," +
        "  \"timestamp\" : 1374569908754" +
        "}";
      
        Event<?> event = Event.fromJson(json);
        @SuppressWarnings("unchecked")
        HiveJob job = ((DAGNode<HiveJob>) event.getPayload()).getJob();
        assertEquals("job_201307231015_0004 (Stage-1, query-id: ...22c4ea289895)", job.getId());
        assertArrayEquals(new String[] { "src" }, job.getAliases());
        assertArrayEquals(new String[] { "SELECT", "FILTER" },
                job.getFeatures());
    }

    private void doTestRoundTrip(DAGNode<HiveJob> expected) throws IOException {
        String asJson = expected.toJson();
        DAGNode<? extends Job> asDAGNodeAgain = DAGNode.fromJson(asJson);
        assertEquals(expected.getName(), asDAGNodeAgain.getName());
        assertNotNull(asDAGNodeAgain.getJob());

        // assert that it's an instance of HiveJob
        assertNotNull(asDAGNodeAgain.getJob() instanceof HiveJob);

        assertJobEquals(expected.getJob(), (HiveJob) asDAGNodeAgain.getJob());
    }

    public static void assertJobEquals(HiveJob expected, HiveJob found) {
        assertEquals(expected.getId(), found.getId());
        assertArrayEquals(expected.getAliases(), found.getAliases());
        assertArrayEquals(expected.getFeatures(), found.getFeatures());
        assertEquals(expected.getMetrics(), found.getMetrics());
        assertEquals(expected.getConfiguration(), found.getConfiguration());
    }
}>>>>>>> YOURS
