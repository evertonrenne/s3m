/home/ramdisk/experiment3/projects/astyanax/revisions/rev_eac3380_e943beb/rev_eac3380-e943beb/src/main/java/com/netflix/astyanax/serializers/StringCoercibleSerializer.java;<<<<<<< MINE
=======
package com.netflix.astyanax.serializers;

import com.netflix.astyanax.mapping.StringCoercible;
import java.nio.ByteBuffer;
import java.nio.charset.Charset;

import org.apache.cassandra.db.marshal.UTF8Type;

/**
 * A StringCoercibleSerializer translates the byte[] to and from string using utf-8
 * encoding.
 * 
 * @author Dave Johnson
 * 
 */
public final class StringCoercibleSerializer extends AbstractSerializer<StringCoercible> {

    private static final String UTF_8 = "UTF-8";
    private static final StringCoercibleSerializer instance = new StringCoercibleSerializer();
    private static final Charset charset = Charset.forName(UTF_8);

    public static StringCoercibleSerializer get() {
        return instance;
    }

    @Override
    public ByteBuffer toByteBuffer(StringCoercible obj) {
        if (obj == null) {
            return null;
        }
        return ByteBuffer.wrap(obj.serializeToString().getBytes(charset));
    }

    public StringCoercible fromByteBuffer(ByteBuffer byteBuffer, Class clazz) {
        if (byteBuffer == null) {
            return null;
        }
        String s = charset.decode(byteBuffer).toString();
		try {
			StringCoercible obj = (StringCoercible)clazz.newInstance();
			obj.initFromString(s);
			return obj;
		} catch (Exception ex) {
			return null;
		}
    }

	@Override
    public StringCoercible fromByteBuffer(ByteBuffer byteBuffer) {
		return this.fromByteBuffer(byteBuffer, null);
	}

	public StringCoercible fromBytes(byte[] bytes, Class clazz) {
        return fromByteBuffer(ByteBuffer.wrap(bytes), clazz);
    }

    @Override
    public ComparatorType getComparatorType() {
        return ComparatorType.UTF8TYPE;
    }

    @Override
    public ByteBuffer fromString(String str) {
        return UTF8Type.instance.fromString(str);
    }

    @Override
    public String getString(ByteBuffer byteBuffer) {
        return UTF8Type.instance.getString(byteBuffer);
    }
}>>>>>>> YOURS
/home/ramdisk/experiment3/projects/astyanax/revisions/rev_eac3380_e943beb/rev_eac3380-e943beb/src/main/java/com/netflix/astyanax/mapping/StringCoercible.java;<<<<<<< MINE
=======
/******************************************************************************
 * Copyright 2012 Dave Johnson.
 *
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 *      http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 ******************************************************************************/

package com.netflix.astyanax.mapping;

/**
 * StringCoercible objects can be written as string and initialized from a string.
 * @author Dave Johnson
 */
public interface StringCoercible {
	public void initFromString(String s);
	public String serializeToString();
}>>>>>>> YOURS
/home/ramdisk/experiment3/projects/astyanax/revisions/rev_7b2d506_3a9a24e/rev_7b2d506-3a9a24e/src/test/java/com/netflix/astyanax/thrift/ThrifeKeyspaceImplTest.java;<<<<<<< MINE
package com.netflix.astyanax.thrift;

import java.io.IOException;
import java.io.PrintWriter;
import java.io.Serializable;
import java.io.StringReader;
import java.nio.ByteBuffer;
import java.util.ArrayList;
import java.util.Arrays;
import java.util.Collection;
import java.util.Iterator;
import java.util.List;
import java.util.Map;
import java.util.Map.Entry;
import java.util.Random;
import java.util.UUID;
import java.util.concurrent.ConcurrentMap;
import java.util.concurrent.ExecutionException;
import java.util.concurrent.ExecutorService;
import java.util.concurrent.Executors;
import java.util.concurrent.Future;
import java.util.concurrent.TimeUnit;
import java.util.concurrent.TimeoutException;
import java.util.concurrent.atomic.AtomicLong;

import javax.annotation.Nullable;

import junit.framework.Assert;

import org.apache.cassandra.utils.Pair;
import org.apache.commons.lang.RandomStringUtils;
import org.junit.AfterClass;
import org.junit.BeforeClass;
import org.junit.Ignore;
import org.junit.Test;
import org.slf4j.Logger;
import org.slf4j.LoggerFactory;

import com.google.common.base.Function;
import com.google.common.base.Supplier;
import com.google.common.base.Suppliers;
import com.google.common.collect.ImmutableMap;
import com.google.common.collect.Lists;
import com.google.common.collect.Maps;
import com.netflix.astyanax.AstyanaxContext;
import com.netflix.astyanax.Cluster;
import com.netflix.astyanax.ColumnListMutation;
import com.netflix.astyanax.ExceptionCallback;
import com.netflix.astyanax.Keyspace;
import com.netflix.astyanax.MutationBatch;
import com.netflix.astyanax.RowCallback;
import com.netflix.astyanax.Serializer;
import com.netflix.astyanax.SerializerPackage;
import com.netflix.astyanax.connectionpool.Host;
import com.netflix.astyanax.connectionpool.NodeDiscoveryType;
import com.netflix.astyanax.connectionpool.OperationResult;
import com.netflix.astyanax.connectionpool.TokenRange;
import com.netflix.astyanax.connectionpool.exceptions.ConnectionException;
import com.netflix.astyanax.connectionpool.exceptions.NotFoundException;
import com.netflix.astyanax.connectionpool.impl.ConnectionPoolConfigurationImpl;
import com.netflix.astyanax.connectionpool.impl.ConnectionPoolType;
import com.netflix.astyanax.connectionpool.impl.CountingConnectionPoolMonitor;
import com.netflix.astyanax.cql.CqlStatementResult;
import com.netflix.astyanax.ddl.ColumnFamilyDefinition;
import com.netflix.astyanax.ddl.FieldMetadata;
import com.netflix.astyanax.ddl.KeyspaceDefinition;
import com.netflix.astyanax.impl.AstyanaxConfigurationImpl;
import com.netflix.astyanax.impl.FilteringHostSupplier;
import com.netflix.astyanax.impl.RingDescribeHostSupplier;
import com.netflix.astyanax.model.Column;
import com.netflix.astyanax.model.ColumnFamily;
import com.netflix.astyanax.model.ColumnList;
import com.netflix.astyanax.model.ColumnSlice;
import com.netflix.astyanax.model.ConsistencyLevel;
import com.netflix.astyanax.model.CqlResult;
import com.netflix.astyanax.model.Equality;
import com.netflix.astyanax.model.Row;
import com.netflix.astyanax.model.Rows;
import com.netflix.astyanax.query.IndexQuery;
import com.netflix.astyanax.query.PreparedIndexExpression;
import com.netflix.astyanax.query.RowQuery;
import com.netflix.astyanax.recipes.UUIDStringSupplier;
import com.netflix.astyanax.recipes.locks.BusyLockException;
import com.netflix.astyanax.recipes.locks.ColumnPrefixDistributedRowLock;
import com.netflix.astyanax.recipes.locks.StaleLockException;
import com.netflix.astyanax.recipes.queue.CountingQueueStats;
import com.netflix.astyanax.recipes.queue.Message;
import com.netflix.astyanax.recipes.queue.MessageConsumer;
import com.netflix.astyanax.recipes.queue.MessageContext;
import com.netflix.astyanax.recipes.queue.MessageHistory;
import com.netflix.astyanax.recipes.queue.MessageProducer;
import com.netflix.astyanax.recipes.queue.MessageQueue;
import com.netflix.astyanax.recipes.queue.MessageQueueDispatcher;
import com.netflix.astyanax.recipes.queue.MessageQueueException;
import com.netflix.astyanax.recipes.queue.SendMessageResponse;
import com.netflix.astyanax.recipes.queue.ShardedDistributedMessageQueue;
import com.netflix.astyanax.recipes.queue.triggers.RepeatingTrigger;
import com.netflix.astyanax.recipes.reader.AllRowsReader;
import com.netflix.astyanax.recipes.uniqueness.ColumnPrefixUniquenessConstraint;
import com.netflix.astyanax.recipes.uniqueness.DedicatedMultiRowUniquenessConstraint;
import com.netflix.astyanax.recipes.uniqueness.MultiRowUniquenessConstraint;
import com.netflix.astyanax.recipes.uniqueness.NotUniqueException;
import com.netflix.astyanax.recipes.uniqueness.RowUniquenessConstraint;
import com.netflix.astyanax.retry.ExponentialBackoff;
import com.netflix.astyanax.serializers.AnnotatedCompositeSerializer;
import com.netflix.astyanax.serializers.ByteBufferSerializer;
import com.netflix.astyanax.serializers.LongSerializer;
import com.netflix.astyanax.serializers.ObjectSerializer;
import com.netflix.astyanax.serializers.PrefixedSerializer;
import com.netflix.astyanax.serializers.SerializerPackageImpl;
import com.netflix.astyanax.serializers.StringSerializer;
import com.netflix.astyanax.serializers.TimeUUIDSerializer;
import com.netflix.astyanax.serializers.UnknownComparatorException;
import com.netflix.astyanax.test.EmbeddedCassandra;
import com.netflix.astyanax.test.SessionEvent;
import com.netflix.astyanax.util.ColumnarRecordWriter;
import com.netflix.astyanax.util.CsvColumnReader;
import com.netflix.astyanax.util.CsvRecordReader;
import com.netflix.astyanax.util.JsonRowsWriter;
import com.netflix.astyanax.util.RangeBuilder;
import com.netflix.astyanax.util.RecordReader;
import com.netflix.astyanax.util.RecordWriter;
import com.netflix.astyanax.util.SingletonEmbeddedCassandra;
import com.netflix.astyanax.util.TimeUUIDUtils;

public class ThrifeKeyspaceImplTest {

    private static Logger LOG = LoggerFactory.getLogger(ThrifeKeyspaceImplTest.class);

    private static Keyspace                  keyspace;
    private static AstyanaxContext<Keyspace> keyspaceContext;

    private static String TEST_CLUSTER_NAME  = "cass_sandbox";
    private static String TEST_KEYSPACE_NAME = "AstyanaxUnitTests";
    private static String SCHEDULER_NAME_CF_NAME = "SchedulerQueue";

    private static ColumnFamily<String, String> CF_USER_INFO = ColumnFamily.newColumnFamily(
            "Standard1", // Column Family Name
            StringSerializer.get(), // Key Serializer
            StringSerializer.get()); // Column Serializer

    private static ColumnFamily<Long, Long> CF_DELETE = ColumnFamily
            .newColumnFamily(
                    "delete", 
                    LongSerializer.get(),
                    LongSerializer.get());
    
    private static ColumnFamily<Long, String> CF_USERS = ColumnFamily
            .newColumnFamily(
                    "users", 
                    LongSerializer.get(),
                    StringSerializer.get());

    private static ColumnFamily<String, String> CF_TTL = ColumnFamily
            .newColumnFamily(
                    "ttl", 
                    StringSerializer.get(),
                    StringSerializer.get());

    public static ColumnFamily<String, String> CF_STANDARD1 = ColumnFamily
            .newColumnFamily(
                    "Standard1", 
                    StringSerializer.get(),
                    StringSerializer.get());

    public static ColumnFamily<String, Long> CF_LONGCOLUMN = ColumnFamily
            .newColumnFamily(
                    "LongColumn1", 
                    StringSerializer.get(),
                    LongSerializer.get());

    public static ColumnFamily<String, String> CF_STANDARD2 = ColumnFamily
            .newColumnFamily(
                    "Standard2", 
                    StringSerializer.get(),
                    StringSerializer.get());

    public static ColumnFamily<String, String> CF_COUNTER1 = ColumnFamily
            .newColumnFamily(
                    "Counter1", 
                    StringSerializer.get(),
                    StringSerializer.get());

    public static ColumnFamily<String, String> CF_NOT_DEFINED = ColumnFamily
            .newColumnFamily(
                    "NotDefined", 
                    StringSerializer.get(),
                    StringSerializer.get());

    public static ColumnFamily<String, String> CF_EMPTY = ColumnFamily
            .newColumnFamily(
                    "NotDefined", 
                    StringSerializer.get(),
                    StringSerializer.get());

    public static AnnotatedCompositeSerializer<MockCompositeType> M_SERIALIZER = new AnnotatedCompositeSerializer<MockCompositeType>(
            MockCompositeType.class);
    
    public static ColumnFamily<String, MockCompositeType> CF_COMPOSITE = ColumnFamily
            .newColumnFamily(
                    "CompositeColumn", 
                    StringSerializer.get(),
                    M_SERIALIZER);

    public static ColumnFamily<ByteBuffer, ByteBuffer> CF_COMPOSITE_CSV = ColumnFamily
            .newColumnFamily(
                    "CompositeCsv", 
                    ByteBufferSerializer.get(),
                    ByteBufferSerializer.get());

    public static ColumnFamily<MockCompositeType, String> CF_COMPOSITE_KEY = ColumnFamily
            .newColumnFamily(
                    "CompositeKey",
                    M_SERIALIZER, 
                    StringSerializer.get());

    public static ColumnFamily<String, UUID> CF_TIME_UUID = ColumnFamily
            .newColumnFamily(
                    "TimeUUID1", 
                    StringSerializer.get(),
                    TimeUUIDSerializer.get());

    public static ColumnFamily<String, UUID> CF_USER_UNIQUE_UUID = ColumnFamily
            .newColumnFamily(
                    "UserUniqueUUID", 
                    StringSerializer.get(),
                    TimeUUIDSerializer.get());
    
    public static ColumnFamily<String, UUID> CF_EMAIL_UNIQUE_UUID = ColumnFamily
            .newColumnFamily(
                    "EmailUniqueUUID", 
                    StringSerializer.get(),
                    TimeUUIDSerializer.get());
    
    private static ColumnFamily<String, String> UNIQUE_CF = ColumnFamily
            .newColumnFamily(
                    "UniqueCf", 
                    StringSerializer.get(), 
                    StringSerializer.get());

    public static AnnotatedCompositeSerializer<SessionEvent> SE_SERIALIZER = new AnnotatedCompositeSerializer<SessionEvent>(
            SessionEvent.class);

    public static ColumnFamily<String, SessionEvent> CF_CLICK_STREAM = ColumnFamily
            .newColumnFamily("ClickStream", StringSerializer.get(),
                    SE_SERIALIZER);

    private static ColumnFamily<String, String> LOCK_CF_LONG   = 
            ColumnFamily.newColumnFamily("LockCfLong", StringSerializer.get(), StringSerializer.get(), LongSerializer.get());
    
    private static ColumnFamily<String, String> LOCK_CF_STRING = 
            ColumnFamily.newColumnFamily("LockCfString", StringSerializer.get(), StringSerializer.get(), StringSerializer.get());
    
    private static final String SEEDS = "localhost:9160";

    private static final long   CASSANDRA_WAIT_TIME = 3000;
    private static final int    TTL                 = 20;
    private static final int    TIMEOUT             = 10;
    
    @BeforeClass
    public static void setup() throws Exception {
        System.out.println("TESTING THRIFT KEYSPACE");

        SingletonEmbeddedCassandra.getInstance();
        
        Thread.sleep(CASSANDRA_WAIT_TIME);
        
        createKeyspace();
    }

    @AfterClass
    public static void teardown() throws Exception {
        if (keyspaceContext != null)
            keyspaceContext.shutdown();
        
        Thread.sleep(CASSANDRA_WAIT_TIME);
    }

    public static void createKeyspace() throws Exception {
        keyspaceContext = new AstyanaxContext.Builder()
                .forCluster(TEST_CLUSTER_NAME)
                .forKeyspace(TEST_KEYSPACE_NAME)
                .withAstyanaxConfiguration(
                        new AstyanaxConfigurationImpl()
                                .setDiscoveryType(NodeDiscoveryType.RING_DESCRIBE)
                                .setConnectionPoolType(ConnectionPoolType.TOKEN_AWARE)
                                .setDiscoveryDelayInSeconds(60000))
                .withConnectionPoolConfiguration(
                        new ConnectionPoolConfigurationImpl(TEST_CLUSTER_NAME
                                + "_" + TEST_KEYSPACE_NAME)
                                .setSocketTimeout(30000)
                                .setMaxTimeoutWhenExhausted(2000)
                                .setMaxConnsPerHost(20)
                                .setInitConnsPerHost(10)
                                .setSeeds(SEEDS))
                .withConnectionPoolMonitor(new CountingConnectionPoolMonitor())
                .buildKeyspace(ThriftFamilyFactory.getInstance());

        keyspaceContext.start();
        
        keyspace = keyspaceContext.getEntity();
        
        try {
            keyspace.dropKeyspace();
        }
        catch (Exception e) {
            e.printStackTrace();
        }
        
        keyspace.createKeyspace(ImmutableMap.<String, Object>builder()
                .put("strategy_options", ImmutableMap.<String, Object>builder()
                        .put("replication_factor", "1")
                        .build())
                .put("strategy_class",     "SimpleStrategy")
                .build()
                );
        
        keyspace.createColumnFamily(CF_STANDARD1, ImmutableMap.<String, Object>builder()
                .put("column_metadata", ImmutableMap.<String, Object>builder()
                        .put("Index1", ImmutableMap.<String, Object>builder()
                                .put("validation_class", "UTF8Type")
                                .put("index_name",       "Index1")
                                .put("index_type",       "KEYS")
                                .build())
                        .put("Index2", ImmutableMap.<String, Object>builder()
                                .put("validation_class", "UTF8Type")
                                .put("index_name",       "Index2")
                                .put("index_type",       "KEYS")
                                .build())
                         .build())
                     .build());
        
        keyspace.createColumnFamily(CF_TTL,        null);
        keyspace.createColumnFamily(CF_STANDARD2,  null);
        keyspace.createColumnFamily(CF_LONGCOLUMN, null);
        keyspace.createColumnFamily(CF_DELETE,     null);
        keyspace.createColumnFamily(CF_COUNTER1, ImmutableMap.<String, Object>builder()
                .put("default_validation_class", "CounterColumnType")
                .build());
        keyspace.createColumnFamily(CF_CLICK_STREAM, ImmutableMap.<String, Object>builder()
                .put("comparator_type", "CompositeType(UTF8Type, TimeUUIDType)")
                .build());
        keyspace.createColumnFamily(CF_COMPOSITE_CSV, ImmutableMap.<String, Object>builder()
                .put("default_validation_class", "UTF8Type")
                .put("key_validation_class",     "UTF8Type")
                .put("comparator_type",          "CompositeType(UTF8Type, LongType)")
                .build());
        keyspace.createColumnFamily(CF_COMPOSITE, ImmutableMap.<String, Object>builder()
                .put("comparator_type", "CompositeType(AsciiType, IntegerType(reversed=true), IntegerType, BytesType, UTF8Type)")
                .build());
        keyspace.createColumnFamily(CF_COMPOSITE_KEY, ImmutableMap.<String, Object>builder()
                .put("key_validation_class", "BytesType")
                .build());
        keyspace.createColumnFamily(CF_TIME_UUID,         null);
        keyspace.createColumnFamily(CF_USER_UNIQUE_UUID,  null);
        keyspace.createColumnFamily(CF_EMAIL_UNIQUE_UUID, null);
        keyspace.createColumnFamily(CF_USERS, ImmutableMap.<String, Object>builder()
                .put("default_validation_class", "UTF8Type")
                .put("column_metadata", ImmutableMap.<String, Object>builder()
                        .put("firstname",  ImmutableMap.<String, Object>builder()
                                .put("validation_class", "UTF8Type")
                                .put("index_name",       "firstname")
                                .put("index_type",       "KEYS")
                                .build())
                        .put("lastname", ImmutableMap.<String, Object>builder()
                                .put("validation_class", "UTF8Type")
                                .put("index_name",       "lastname")
                                .put("index_type",       "KEYS")
                                .build())
                        .put("age", ImmutableMap.<String, Object>builder()
                                .put("validation_class", "LongType")
                                .put("index_name",       "age")
                                .put("index_type",       "KEYS")
                                .build())
                        .build())
                     .build());
        
        keyspace.createColumnFamily(LOCK_CF_LONG, ImmutableMap.<String, Object>builder()
                .put("default_validation_class", "LongType")
                .put("key_validation_class",     "UTF8Type")
                .put("comparator_type",          "UTF8Type")
                .build());
        
        keyspace.createColumnFamily(LOCK_CF_STRING, ImmutableMap.<String, Object>builder()
                .put("default_validation_class", "UTF8Type")
                .put("key_validation_class",     "UTF8Type")
                .put("comparator_type",          "UTF8Type")
                .build());
        
        keyspace.createColumnFamily(UNIQUE_CF, null);
        
        KeyspaceDefinition ki = keyspaceContext.getEntity().describeKeyspace();
        System.out.println("Describe Keyspace: " + ki.getName());

        try {
            //
            // CF_Super :
            // 'A' :
            // 'a' :
            // 1 : 'Aa1',
            // 2 : 'Aa2',
            // 'b' :
            // ...
            // 'z' :
            // ...
            // 'B' :
            // ...
            //
            // CF_Standard :
            // 'A' :
            // 'a' : 1,
            // 'b' : 2,
            // ...
            // 'z' : 26,
            // 'B' :
            // ...
            //

            MutationBatch m;
            OperationResult<Void> result;
            m = keyspace.prepareMutationBatch();

            for (char keyName = 'A'; keyName <= 'Z'; keyName++) {
                String rowKey = Character.toString(keyName);
                ColumnListMutation<String> cfmStandard = m.withRow(
                        CF_STANDARD1, rowKey);
                for (char cName = 'a'; cName <= 'z'; cName++) {
                    cfmStandard.putColumn(Character.toString(cName),
                            (int) (cName - 'a') + 1, null);
                }
                cfmStandard
                        .putColumn("Index1", (int) (keyName - 'A') + 1, null);
                cfmStandard.putColumn("Index2", 42, null);
                m.execute();
            }

            m.withRow(CF_STANDARD1, "Prefixes").putColumn("Prefix1_a", 1, null)
                    .putColumn("Prefix1_b", 2, null)
                    .putColumn("prefix2_a", 3, null);

            result = m.execute();

            String rowKey = "A";
            ColumnListMutation<Long> cfmLong = m.withRow(CF_LONGCOLUMN, rowKey);
            for (Long l = -10L; l < 10L; l++) {
                cfmLong.putEmptyColumn(l, null);
            }
            cfmLong.putEmptyColumn(Long.MAX_VALUE, null);
            result = m.execute();

            m.withRow(CF_USER_INFO, "acct1234")
                .putColumn("firstname", "john", null)
                .putColumn("lastname", "smith", null)
                .putColumn("address", "555 Elm St", null)
                .putColumn("age", 30, null)
                .putEmptyColumn("empty");

            m.execute();

        } catch (Exception e) {
            System.out.println(e.getMessage());
            Assert.fail();
        }
    }

    @Test
    public void testMultiColumnDelete() throws Exception {
        MutationBatch mb = keyspace.prepareMutationBatch();
        mb.withRow(CF_DELETE, 1L)
            .setTimestamp(1).putEmptyColumn(1L, null)
            .setTimestamp(10).putEmptyColumn(2L, null)
            ;
        mb.execute();
        
        ColumnList<Long> result1 = keyspace.prepareQuery(CF_DELETE).getRow(1L).execute().getResult();
        Assert.assertEquals(2, result1.size());
        Assert.assertNotNull(result1.getColumnByName(1L));
        Assert.assertNotNull(result1.getColumnByName(2L));
        
        logColumnList("Insert", result1);
        
        mb = keyspace.prepareMutationBatch();
        mb.withRow(CF_DELETE,  1L)
            .setTimestamp(result1.getColumnByName(1L).getTimestamp()-1)
            .deleteColumn(1L)
            .setTimestamp(result1.getColumnByName(2L).getTimestamp()-1)
            .deleteColumn(2L)
            .putEmptyColumn(3L, null);
        
        mb.execute();
        
        result1 = keyspace.prepareQuery(CF_DELETE).getRow(1L).execute().getResult();
        logColumnList("Delete with older timestamp", result1);
        Assert.assertEquals(3, result1.size());
        
        LOG.info("Delete L2 with TS: " + (result1.getColumnByName(2L).getTimestamp()+1));
        mb.withRow(CF_DELETE,  1L)
            .setTimestamp(result1.getColumnByName(1L).getTimestamp()+1)
            .deleteColumn(1L)
            .setTimestamp(result1.getColumnByName(2L).getTimestamp()+1)
            .deleteColumn(2L);
        mb.execute();
        
        result1 = keyspace.prepareQuery(CF_DELETE).getRow(1L).execute().getResult();
        logColumnList("Delete with newer timestamp", result1);
        Assert.assertEquals(1, result1.size());
    }
    
    <T> void logColumnList(String label, ColumnList<T> cl) {
        LOG.info(">>>>>> " + label);
        for (Column<T> c : cl) {
            LOG.info(c.getName() + " " + c.getTimestamp());
        }
        LOG.info("<<<<<<");
    }
    
    @Test
    public void testCqlComposite() throws Exception {
        CqlStatementResult result = keyspace.prepareCqlStatement()
            .withCql("SELECT * FROM " + CF_COMPOSITE_CSV.getName())
            .execute()
            .getResult();
        
        result.getSchema();
        result.getRows(CF_COMPOSITE_CSV);
    }
    
    @Test
    public void testHasValue() throws Exception {
        ColumnList<String> response = keyspace.prepareQuery(CF_USER_INFO).getRow("acct1234").execute().getResult();
        Assert.assertEquals("firstname", response.getColumnByName("firstname").getName());
        Assert.assertEquals("firstname", response.getColumnByName("firstname").getName());
        Assert.assertEquals("john", response.getColumnByName("firstname").getStringValue());
        Assert.assertEquals("john", response.getColumnByName("firstname").getStringValue());
        Assert.assertEquals(true,  response.getColumnByName("firstname").hasValue());
        Assert.assertEquals(false, response.getColumnByName("empty").hasValue());
        
    }
    
    @Test
    public void getKeyspaceDefinition() throws Exception {
        KeyspaceDefinition def = keyspaceContext.getEntity().describeKeyspace();
        Collection<String> fieldNames = def.getFieldNames();
        LOG.info("Getting field names");
        for (String field : fieldNames) {
            LOG.info(field);
        }
        LOG.info(fieldNames.toString());
        
        System.out.println(fieldNames.toString());
        
        for (FieldMetadata field : def.getFieldsMetadata()) {
            System.out.println(field.getName() + " = " + def.getFieldValue(field.getName()) + " (" + field.getType() + ")");
        }
        
        for (ColumnFamilyDefinition cfDef : def.getColumnFamilyList()) {
            LOG.info("----------" );
            for (FieldMetadata field : cfDef.getFieldsMetadata()) {
                LOG.info(field.getName() + " = " + cfDef.getFieldValue(field.getName()) + " (" + field.getType() + ")");
            }
        }
    }
    
    @Test
    public void testNonExistentKeyspace()  {
        AstyanaxContext<Keyspace> ctx = new AstyanaxContext.Builder()
            .forCluster(TEST_CLUSTER_NAME)
            .forKeyspace(TEST_KEYSPACE_NAME + "_NonExistent")
            .withAstyanaxConfiguration(
                    new AstyanaxConfigurationImpl()
                            .setDiscoveryType(NodeDiscoveryType.RING_DESCRIBE)
                            .setConnectionPoolType(ConnectionPoolType.TOKEN_AWARE)
                            .setDiscoveryDelayInSeconds(60000))
            .withConnectionPoolConfiguration(
                    new ConnectionPoolConfigurationImpl(TEST_CLUSTER_NAME
                            + "_" + TEST_KEYSPACE_NAME)
                            .setSocketTimeout(30000)
                            .setMaxTimeoutWhenExhausted(2000)
                            .setMaxConnsPerHost(20)
                            .setInitConnsPerHost(10)
                            .setSeeds(SEEDS))
            .withConnectionPoolMonitor(new CountingConnectionPoolMonitor())
            .buildKeyspace(ThriftFamilyFactory.getInstance());        
        
        ctx.start();
        
        try {
            KeyspaceDefinition keyspaceDef = ctx.getEntity().describeKeyspace();
        } catch (ConnectionException e) {
            // TODO Auto-generated catch block
            e.printStackTrace();
        }
        
    }
    
    @Test
    public void testDescribeRing() throws Exception {
        // [TokenRangeImpl [startToken=0, endToken=0, endpoints=[127.0.0.1]]]
        List<TokenRange> ring = keyspaceContext.getEntity().describeRing();
        LOG.info(ring.toString());
        
        // 127.0.0.1
        RingDescribeHostSupplier ringSupplier = new RingDescribeHostSupplier(keyspaceContext.getEntity(), 9160);
        List<Host> hosts = ringSupplier.get();
        Assert.assertEquals(1, hosts.get(0).getTokenRanges().size());
        LOG.info(hosts.toString());
        
        Supplier<List<Host>> sourceSupplier1 = Suppliers.ofInstance((List<Host>)Lists.newArrayList(new Host("127.0.0.1", 9160)));
        Supplier<List<Host>> sourceSupplier2 = Suppliers.ofInstance((List<Host>)Lists.newArrayList(new Host("127.0.0.2", 9160)));
        
        // 127.0.0.1
        LOG.info(sourceSupplier1.get().toString());
        
        // 127.0.0.2
        LOG.info(sourceSupplier2.get().toString());
        
        hosts = new FilteringHostSupplier(ringSupplier, sourceSupplier1).get();
        LOG.info(hosts.toString());
        
        Assert.assertEquals(1, hosts.size());
        Assert.assertEquals(1, hosts.get(0).getTokenRanges().size());
        hosts = new FilteringHostSupplier(ringSupplier, sourceSupplier2).get();
        LOG.info(hosts.toString());
        Assert.assertEquals(1, hosts.size());
    }
    
    @Test
    public void paginateColumns() {
        String column = "";
        ColumnList<String> columns;
        int pageize = 10;
        try {
            RowQuery<String, String> query = keyspace
                    .prepareQuery(CF_STANDARD1)
                    .getKey("A")
                    .autoPaginate(true)
                    .withColumnRange(
                            new RangeBuilder().setStart(column)
                                    .setLimit(pageize).build());

            while (!(columns = query.execute().getResult()).isEmpty()) {
                for (Column<String> c : columns) {
                }
                // column = Iterables.getLast(columns).getName() + "\u0000";
            }
        } catch (ConnectionException e) {
            System.out.println(e.getMessage());
        }
    }

    @Test
    public void example() {
        AstyanaxContext<Keyspace> context = new AstyanaxContext.Builder()
                .forCluster(TEST_CLUSTER_NAME)
                .forKeyspace(TEST_KEYSPACE_NAME)
                .withAstyanaxConfiguration(
                        new AstyanaxConfigurationImpl()
                                .setDiscoveryType(NodeDiscoveryType.NONE))
                .withConnectionPoolConfiguration(
                        new ConnectionPoolConfigurationImpl("MyConnectionPool")
                                .setMaxConnsPerHost(1).setSeeds(
                                        "127.0.0.1:9160"))
                .withConnectionPoolMonitor(new CountingConnectionPoolMonitor())
                .buildKeyspace(ThriftFamilyFactory.getInstance());

        context.start();
        Keyspace keyspace = context.getEntity();

        MutationBatch m = keyspace.prepareMutationBatch();

        // m.withRow(CF_USER_STATS, "acct1234")
        // .incrementCounterColumn("loginCount", 1);

        try {
            OperationResult<Void> result = m.execute();
        } catch (ConnectionException e) {
            System.out.println(e);
        }

        try {
            OperationResult<ColumnList<String>> result = keyspace
                    .prepareQuery(CF_USER_INFO).getKey("acct1234").execute();
            ColumnList<String> columns = result.getResult();

            // Lookup columns in response by name
            int age = columns.getColumnByName("age").getIntegerValue();
            String address = columns.getColumnByName("address")
                    .getStringValue();

            // Or, iterate through the columns
            for (Column<String> c : result.getResult()) {
                System.out.println(c.getName());
            }
        } catch (ConnectionException e) {
            System.out.println(e);
        }

    }
    
    @Test
    public void testMultiRowUniqueness() {
        DedicatedMultiRowUniquenessConstraint<UUID> constraint = new DedicatedMultiRowUniquenessConstraint<UUID>
                  (keyspace, TimeUUIDUtils.getUniqueTimeUUIDinMicros())
                  .withConsistencyLevel(ConsistencyLevel.CL_ONE)
                  .withRow(CF_USER_UNIQUE_UUID, "user1")
                  .withRow(CF_EMAIL_UNIQUE_UUID, "user1@domain.com");
        
        DedicatedMultiRowUniquenessConstraint<UUID> constraint2 = new DedicatedMultiRowUniquenessConstraint<UUID>
                  (keyspace, TimeUUIDUtils.getUniqueTimeUUIDinMicros())
                  .withConsistencyLevel(ConsistencyLevel.CL_ONE)
                  .withRow(CF_USER_UNIQUE_UUID, "user1")
                  .withRow(CF_EMAIL_UNIQUE_UUID, "user1@domain.com");
        
        try {
            Column<UUID> c = constraint.getUniqueColumn();
            Assert.fail();
        }
        catch (Exception e) {
            LOG.info(e.getMessage());
        }
        
        try {
            constraint.acquire();
            
            Column<UUID> c = constraint.getUniqueColumn();
            LOG.info("Unique column is " + c.getName());
            
            try {
                constraint2.acquire();
                Assert.fail("Should already be acquired");
            }
            catch (NotUniqueException e) {
                
            }
            catch (Exception e) {
                e.printStackTrace();
                Assert.fail();
            }
            finally {
                try {
                    constraint2.release();
                }
                catch (Exception e) {
                    e.printStackTrace();
                    Assert.fail();
                }
            }
        }
        catch (Exception e) {
            e.printStackTrace();
            Assert.fail();
        }
        finally {
            try {
                constraint.release();
            }
            catch (Exception e) {
                e.printStackTrace();
                Assert.fail();
            }
        }
        
        try {
            constraint2.acquire();
            Column<UUID> c = constraint.getUniqueColumn();
            LOG.info("Unique column is " + c.getName());
        }
        catch (NotUniqueException e) {
            Assert.fail("Should already be unique");
        }
        catch (Exception e) {
            e.printStackTrace();
            Assert.fail();
        }
        finally {
            try {
                constraint2.release();
            }
            catch (Exception e) {
                e.printStackTrace();
                Assert.fail();
            }
        }
        
    }

    @Test
    public void paginateLongColumns() {
        Long column = Long.MIN_VALUE;
        ColumnList<Long> columns;
        int pageize = 10;
        try {
            RowQuery<String, Long> query = keyspace
                    .prepareQuery(CF_LONGCOLUMN)
                    .getKey("A")
                    .autoPaginate(true)
                    .withColumnRange(
                            new RangeBuilder().setStart(column)
                                    .setLimit(pageize).build());

            while (!(columns = query.execute().getResult()).isEmpty()) {
                LOG.info("-----");
                for (Column<Long> c : columns) {
                    LOG.info(Long.toString(c.getName()));
                }
                // column = Iterables.getLast(columns).getName() + "\u0000";
            }
        } catch (ConnectionException e) {
        }
    }

    @Test
    public void getAll() {
        try {
            OperationResult<Rows<String, String>> rows = keyspace
                    .prepareQuery(CF_STANDARD1).getAllRows().setRowLimit(10)
                    .withColumnRange(new RangeBuilder().setLimit(0).build())
                    .setExceptionCallback(new ExceptionCallback() {
                        @Override
                        public boolean onException(ConnectionException e) {
                            Assert.fail(e.getMessage());
                            return true;
                        }
                    }).execute();
            for (Row<String, String> row : rows.getResult()) {
                LOG.info("ROW: " + row.getKey() + " " + row.getColumns().size());
            }
        } catch (ConnectionException e) {
            Assert.fail();
        }
    }
    
    @Test
    public void testAllRowsReader() throws Exception {
        final AtomicLong counter = new AtomicLong(0);
        
        boolean result = new AllRowsReader.Builder<String, String>(keyspace, CF_STANDARD1)
                .forEachRow(new Function<Row<String, String>, Boolean>() {
                    @Override
                    public Boolean apply(@Nullable Row<String, String> row) {
                        counter.incrementAndGet();
                        LOG.info("Got a row: " + row.getKey().toString());
                        return true;
                    }
                })
                .build()
                .call();
        
        Assert.assertTrue(result);
        Assert.assertEquals(28, counter.get());
    }
    
    @Test
    public void testAllRowsReaderWithCancel() throws Exception {
        final AtomicLong counter = new AtomicLong(0);
        
        AllRowsReader<String, String> reader = new AllRowsReader.Builder<String, String>(keyspace, CF_STANDARD1)
                .withPageSize(3)
                .forEachRow(new Function<Row<String, String>, Boolean>() {
                    @Override
                    public Boolean apply(@Nullable Row<String, String> row) {
                        try {
                            Thread.sleep(200);
                        } catch (InterruptedException e) {
                            Thread.currentThread().interrupt();
                            throw new RuntimeException(e);
                        }
                        counter.incrementAndGet();
                        LOG.info("Got a row: " + row.getKey().toString());
                        return true;
                    }
                })
                .build();
        
        
        Future<Boolean> future = Executors.newSingleThreadExecutor().submit(reader);        
        
        Thread.sleep(1000);
        
        reader.cancel();
        
        try {
            boolean result = future.get();
            Assert.assertEquals(false, result);
            Assert.fail();
        }
        catch (Exception e) {
            LOG.info("Failed to execute", e);
        }
        LOG.info("Before: " + counter.get());
        Assert.assertNotSame(28, counter.get());
        Thread.sleep(2000);
        LOG.info("After: " + counter.get());
        Assert.assertNotSame(28, counter.get());
    }

    @Test
    public void testAllRowsReaderConcurrency() throws Exception {
        final AtomicLong counter = new AtomicLong(0);
        
        boolean result = new AllRowsReader.Builder<String, String>(keyspace, CF_STANDARD1)
                .withConcurrencyLevel(4)
                .forEachRow(new Function<Row<String, String>, Boolean>() {
                    @Override
                    public Boolean apply(@Nullable Row<String, String> row) {
                        counter.incrementAndGet();
                        LOG.info("Got a row: " + row.getKey().toString());
                        return true;
                    }
                })
                .build()
                .call();
        
        Assert.assertTrue(result);
        Assert.assertEquals(28, counter.get());
    }

    @Test
    public void getAllWithCallback() {
        try {
            final AtomicLong counter = new AtomicLong();

            keyspace.prepareQuery(CF_STANDARD1).getAllRows().setRowLimit(3)
                    .setRepeatLastToken(false)
                    .withColumnRange(new RangeBuilder().setLimit(2).build())
                    .executeWithCallback(new RowCallback<String, String>() {
                        @Override
                        public void success(Rows<String, String> rows) {
                            for (Row<String, String> row : rows) {
                                LOG.info("ROW: " + row.getKey() + " "
                                        + row.getColumns().size());
                                counter.incrementAndGet();
                            }
                        }

                        @Override
                        public boolean failure(ConnectionException e) {
                            LOG.error(e.getMessage(), e);
                            return false;
                        }
                    });
            LOG.info("Read " + counter.get() + " keys");
        } catch (ConnectionException e) {
            Assert.fail();
        }
    }

    static class UserInfo implements Serializable {
        private static final long serialVersionUID = 6366200973810770033L;

        private String firstName;
        private String lastName;

        public UserInfo() {

        }

        public void setFirstName(String firstName) {
            this.firstName = firstName;
        }

        public String getFirstName() {
            return this.firstName;
        }

        public void setLastName(String lastName) {
            this.lastName = lastName;
        }

        public String getLastName() {
            return this.lastName;
        }

        public boolean equals(Object other) {
            UserInfo smo = (UserInfo) other;
            return firstName.equals(smo.firstName)
                    && lastName.equals(smo.lastName);
        }
    }

    @Test
    public void testSerializedClassValue() {
        UserInfo smo = new UserInfo();
        smo.setLastName("Landau");
        smo.setFirstName("Eran");

        try {
            ByteBuffer bb = ObjectSerializer.get().toByteBuffer(smo);
            keyspace.prepareColumnMutation(CF_STANDARD1, "Key_SerializeTest",
                    "Column1").putValue(bb, null).execute();

            UserInfo smo2 = (UserInfo) keyspace.prepareQuery(CF_STANDARD1)
                    .getKey("Key_SerializeTest").getColumn("Column1").execute()
                    .getResult().getValue(ObjectSerializer.get());

            Assert.assertEquals(smo, smo2);
        } catch (ConnectionException e) {
            Assert.fail();
        }
    }    
    
    @Test
    public void testSingleOps() throws Exception {
        String key = "SingleOpsTest";
        Random prng = new Random();

        // Set a string value
        {
            String column = "StringColumn";
            String value = RandomStringUtils.randomAlphanumeric(32);
            // Set
            keyspace.prepareColumnMutation(CF_STANDARD1, key, column)
                    .putValue(value, null).execute();
            // Read
            String v = keyspace.prepareQuery(CF_STANDARD1).getKey(key)
                    .getColumn(column).execute().getResult().getStringValue();
            Assert.assertEquals(value, v);
            // Delete
            keyspace.prepareColumnMutation(CF_STANDARD1, key, column)
                    .deleteColumn().execute();
            try {
                keyspace.prepareQuery(CF_STANDARD1).getKey(key)
                        .getColumn(column).execute().getResult()
                        .getStringValue();
                Assert.fail();
            } catch (NotFoundException e) {
            } catch (ConnectionException e) {
                Assert.fail();
            }
        } 

        // Set a byte value
        {
            String column = "ByteColumn";
            byte value = (byte) prng.nextInt(Byte.MAX_VALUE);
            // Set
            keyspace.prepareColumnMutation(CF_STANDARD1, key, column)
                    .putValue(value, null).execute();
            // Read
            byte v = keyspace.prepareQuery(CF_STANDARD1).getKey(key)
                    .getColumn(column).execute().getResult().getByteValue();
            Assert.assertEquals(value, v);
            // Delete
            keyspace.prepareColumnMutation(CF_STANDARD1, key, column)
                    .deleteColumn().execute();
            // verify column gone
            try {
                keyspace.prepareQuery(CF_STANDARD1).getKey(key)
                        .getColumn(column).execute().getResult().getByteValue();
                Assert.fail();
            } catch (NotFoundException e) {
            	// expected
            }
        } 
        
        // Set a short value
        {
            String column = "ShortColumn";
            short value = (short) prng.nextInt(Short.MAX_VALUE);
            // Set
            keyspace.prepareColumnMutation(CF_STANDARD1, key, column)
                    .putValue(value, null).execute();
            // Read
            short v = keyspace.prepareQuery(CF_STANDARD1).getKey(key)
                    .getColumn(column).execute().getResult().getShortValue();
            Assert.assertEquals(value, v);
            // Delete
            keyspace.prepareColumnMutation(CF_STANDARD1, key, column)
                    .deleteColumn().execute();
            // verify column gone
            try {
                keyspace.prepareQuery(CF_STANDARD1).getKey(key)
                        .getColumn(column).execute().getResult().getShortValue();
                Assert.fail();
            } catch (NotFoundException e) {
            	// expected
            }
        } 
        
        // Set a int value
        {
            String column = "IntColumn";
            int value = prng.nextInt();
            // Set
            keyspace.prepareColumnMutation(CF_STANDARD1, key, column)
                    .putValue(value, null).execute();
            // Read
            int v = keyspace.prepareQuery(CF_STANDARD1).getKey(key)
                    .getColumn(column).execute().getResult().getIntegerValue();
            Assert.assertEquals(value, v);
            // Delete
            keyspace.prepareColumnMutation(CF_STANDARD1, key, column)
                    .deleteColumn().execute();
            // verify column gone
            try {
                keyspace.prepareQuery(CF_STANDARD1).getKey(key)
                        .getColumn(column).execute().getResult().getIntegerValue();
                Assert.fail();
            } catch (NotFoundException e) {
            	// expected
            }
        }
        
        // Set a long value
        {
            String column = "LongColumn";
            long value = prng.nextLong();
            // Set
            keyspace.prepareColumnMutation(CF_STANDARD1, key, column)
                    .putValue(value, null).execute();
            // Read
            long v = keyspace.prepareQuery(CF_STANDARD1).getKey(key)
                    .getColumn(column).execute().getResult().getLongValue();
            Assert.assertEquals(value, v);
         // get as integer should fail
            try {
                keyspace.prepareQuery(CF_STANDARD1).getKey(key)
                        .getColumn(column).execute().getResult()
                        .getIntegerValue();
                Assert.fail();
            } catch (Exception e) {
            	// expected
            }
            // Delete
            keyspace.prepareColumnMutation(CF_STANDARD1, key, column)
                    .deleteColumn().execute();
            // verify column gone
            try {
                keyspace.prepareQuery(CF_STANDARD1).getKey(key)
                        .getColumn(column).execute().getResult().getLongValue();
                Assert.fail();
            } catch (NotFoundException e) {
            	// expected
            }
        }
        
        // Set a float value
        {
            String column = "FloatColumn";
            float value = prng.nextFloat();
            // Set
            keyspace.prepareColumnMutation(CF_STANDARD1, key, column)
                    .putValue(value, null).execute();
            // Read
            float v = keyspace.prepareQuery(CF_STANDARD1).getKey(key)
                    .getColumn(column).execute().getResult().getFloatValue();
            Assert.assertEquals(value, v);
            // Delete
            keyspace.prepareColumnMutation(CF_STANDARD1, key, column)
                    .deleteColumn().execute();
            // verify column gone
            try {
                keyspace.prepareQuery(CF_STANDARD1).getKey(key)
                        .getColumn(column).execute().getResult().getFloatValue();
                Assert.fail();
            } catch (NotFoundException e) {
            	// expected
            }
        }

        // Set a double value
        {
            String column = "IntColumn";
            double value = prng.nextDouble();
            // Set
            keyspace.prepareColumnMutation(CF_STANDARD1, key, column)
                    .putValue(value, null).execute();
            // Read
            double v = keyspace.prepareQuery(CF_STANDARD1).getKey(key)
                    .getColumn(column).execute().getResult().getDoubleValue();
            Assert.assertEquals(value, v);
            // get as integer should fail
            try {
                keyspace.prepareQuery(CF_STANDARD1).getKey(key)
                        .getColumn(column).execute().getResult()
                        .getIntegerValue();
                Assert.fail();
            } catch (Exception e) {
            	// expected
            }
            // Delete
            keyspace.prepareColumnMutation(CF_STANDARD1, key, column)
                    .deleteColumn().execute();
            try {
                keyspace.prepareQuery(CF_STANDARD1).getKey(key)
                        .getColumn(column).execute().getResult()
                        .getDoubleValue();
                Assert.fail();
            } catch (NotFoundException e) {
            } catch (ConnectionException e) {
                Assert.fail();
            }
        } 
        
        // Set long column with timestamp
        {
            String column = "TimestampColumn";
            long value = prng.nextLong();

            // Set
            keyspace.prepareColumnMutation(CF_STANDARD1, key, column)
                    .withTimestamp(100)
                    .putValue(value, null)
                    .execute();

            // Read
            Column<String> c = keyspace.prepareQuery(CF_STANDARD1).getKey(key)
                    .getColumn(column).execute().getResult();
            Assert.assertEquals(100,  c.getTimestamp());
        } 
    }

    @Test
    public void testTimeUUIDUnique() {
        long now = System.currentTimeMillis();
        UUID uuid1 = TimeUUIDUtils.getTimeUUID(now);
        UUID uuid2 = TimeUUIDUtils.getTimeUUID(now);
        LOG.info(uuid1.toString());
        LOG.info(uuid2.toString());
        Assert.assertTrue(uuid1.equals(uuid2));
    }

    @Test
    public void testTimeUUID2() {
        MutationBatch m = keyspace.prepareMutationBatch();
        String rowKey = "Key2";
        m.withRow(CF_TIME_UUID, rowKey).delete();
        try {
            m.execute();
        } catch (ConnectionException e) {
            Assert.fail(e.getMessage());
        }

        long now = System.currentTimeMillis();
        long msecPerDay = 86400000;
        for (int i = 0; i < 100; i++) {
            m.withRow(CF_TIME_UUID, rowKey).putColumn(
                    TimeUUIDUtils.getTimeUUID(now - i * msecPerDay), i, null);
        }
        try {
            m.execute();
        } catch (ConnectionException e) {
            Assert.fail(e.getMessage());
        }

        try {
            OperationResult<ColumnList<UUID>> result = keyspace
                    .prepareQuery(CF_TIME_UUID)
                    .getKey(rowKey)
                    .withColumnRange(
                            new RangeBuilder()
                                    .setLimit(100)
                                    .setStart(
                                            TimeUUIDUtils.getTimeUUID(now - 20
                                                    * msecPerDay)).build())
                    .execute();
            for (Column<UUID> column : result.getResult()) {
                System.out.println((now - TimeUUIDUtils.getTimeFromUUID(column
                        .getName())) / msecPerDay);
            }
        } catch (ConnectionException e) {
            Assert.fail(e.getMessage());
        }
    }

    @Test
    public void testTimeUUID() {
        MutationBatch m = keyspace.prepareMutationBatch();

        UUID columnName = TimeUUIDUtils.getUniqueTimeUUIDinMillis();
        long columnTime = TimeUUIDUtils.getTimeFromUUID(columnName);
        String rowKey = "Key1";

        m.withRow(CF_TIME_UUID, rowKey).delete();
        try {
            m.execute();
        } catch (ConnectionException e1) {
            Assert.fail();
        }

        int startTime = 100;
        int endTime = 200;

        m.withRow(CF_TIME_UUID, rowKey).putColumn(columnName, 42, null);
        for (int i = startTime; i < endTime; i++) {
            // UUID c = TimeUUIDUtils.getTimeUUID(i);
            LOG.info(TimeUUIDUtils.getTimeUUID(columnTime + i).toString());

            m.withRow(CF_TIME_UUID, rowKey).putColumn(
                    TimeUUIDUtils.getTimeUUID(columnTime + i), i, null);
        }

        try {
            m.execute();
        } catch (ConnectionException e) {
            LOG.error(e.getMessage(), e);
            Assert.fail();
        }

        try {
            OperationResult<Column<UUID>> result = keyspace
                    .prepareQuery(CF_TIME_UUID).getKey(rowKey)
                    .getColumn(columnName).execute();

            Assert.assertEquals(columnName, result.getResult().getName());
            Assert.assertTrue(result.getResult().getIntegerValue() == 42);

            OperationResult<ColumnList<UUID>> result2 = keyspace
                    .prepareQuery(CF_TIME_UUID).getKey(rowKey).execute();

            result2 = keyspace
                    .prepareQuery(CF_TIME_UUID)
                    .getKey(rowKey)
                    .withColumnRange(
                            new RangeBuilder()
                                    .setLimit(10)
                                    .setStart(TimeUUIDUtils.getTimeUUID(0))
                                    .setEnd(TimeUUIDUtils
                                            .getTimeUUID(Long.MAX_VALUE >> 8))
                                    .build()).execute();
            Assert.assertEquals(10, result2.getResult().size());

        } catch (ConnectionException e) {
            LOG.error(e.getMessage(), e);
            Assert.fail();
        }

        UUID currentUUID = TimeUUIDUtils.getUniqueTimeUUIDinMicros();

        SerializerPackage pkg = null;
        try {
            pkg = keyspace.getSerializerPackage(CF_TIME_UUID.getName(), false);
        } catch (ConnectionException e) {
            Assert.fail();
            e.printStackTrace();
        } catch (UnknownComparatorException e) {
            Assert.fail();
            e.printStackTrace();
        }
        Serializer<UUID> serializer = (Serializer<UUID>) pkg
                .getColumnNameSerializer();

        ByteBuffer buffer = serializer.toByteBuffer(currentUUID);
        String value = serializer.getString(buffer);
        LOG.info("UUID Time = " + value);

        // Test timeUUID pagination
        RowQuery<String, UUID> query = keyspace
                .prepareQuery(CF_TIME_UUID)
                .getKey(rowKey)
                .withColumnRange(
                        new RangeBuilder()
                                .setLimit(10)
                                .setStart(
                                        TimeUUIDUtils.getTimeUUID(columnTime
                                                + startTime))
                                .setEnd(TimeUUIDUtils.getTimeUUID(columnTime
                                        + endTime)).build()).autoPaginate(true);
        OperationResult<ColumnList<UUID>> result;
        int pageCount = 0;
        int rowCount = 0;
        try {
            LOG.info("starting pagination");
            while (!(result = query.execute()).getResult().isEmpty()) {
                pageCount++;
                rowCount += result.getResult().size();
                LOG.info("==== Block ====");
                for (Column<UUID> column : result.getResult()) {
                    LOG.info("Column is " + column.getName());
                }
            }
            LOG.info("pagination complete");
        } catch (ConnectionException e) {
            Assert.fail();
            LOG.info(e.getMessage());
            e.printStackTrace();
        }

    }

    @Test
    public void testCopy() {
        String keyName = "A";

        try {
            keyspace.prepareQuery(CF_STANDARD1).getKey(keyName)
                    .copyTo(CF_STANDARD2, keyName).execute();

            ColumnList<String> list1 = keyspace.prepareQuery(CF_STANDARD1)
                    .getKey(keyName).execute().getResult();

            ColumnList<String> list2 = keyspace.prepareQuery(CF_STANDARD2)
                    .getKey(keyName).execute().getResult();

            Iterator<Column<String>> iter1 = list1.iterator();
            Iterator<Column<String>> iter2 = list2.iterator();

            while (iter1.hasNext()) {
                Column<String> column1 = iter1.next();
                Column<String> column2 = iter2.next();
                Assert.assertEquals(column1.getName(), column2.getName());
                Assert.assertEquals(column1.getByteBufferValue(),
                        column2.getByteBufferValue());
            }
            Assert.assertFalse(iter2.hasNext());

        } catch (ConnectionException e) {
            LOG.error(e.getMessage(), e);
            Assert.fail(e.getMessage());
        }
    }

    @Test
    public void testMutationBatchMultipleWithRow() throws Exception {
        MutationBatch mb = keyspace.prepareMutationBatch();
        
        Long key = 9L;
        
        mb.withRow(CF_USERS, key).delete();
        mb.withRow(CF_USERS, key).putEmptyColumn("test", null);
        
        mb.execute();
        
        ColumnList<String> result = keyspace.prepareQuery(CF_USERS).getRow(key).execute().getResult();
        
        Assert.assertEquals(1, result.size());
    }
    
    @Test
    public void testClickStream() {
        MutationBatch m = keyspace.prepareMutationBatch();
        String userId = "UserId";

        long timeCounter = 0;
        for (int i = 0; i < 10; i++) {
            String sessionId = "Session" + i;

            for (int j = 0; j < 10; j++) {
                m.withRow(CF_CLICK_STREAM, userId).putColumn(
                        new SessionEvent(sessionId,
                                TimeUUIDUtils.getTimeUUID(j)),
                        Long.toString(timeCounter), null);
                timeCounter++;
            }
        }

        try {
            m.execute();
        } catch (ConnectionException e) {
            LOG.error(e.getMessage(), e);
            Assert.fail();
        }

        try {
            OperationResult<ColumnList<SessionEvent>> result;

            result = keyspace
                    .prepareQuery(CF_CLICK_STREAM)
                    .getKey(userId)
                    .withColumnRange(
                            SE_SERIALIZER.buildRange()
                                    .greaterThanEquals("Session3")
                                    .lessThanEquals("Session5").build())
                    .execute();
            // Assert.assertEquals(10, result.getResult().size());
            // LOG.info("*********************** INCLUSIVE - INCLUSIVE");
            // for (Column<SessionEvent> column : result.getResult()) {
            // LOG.info("####### " + column.getName() + " = " +
            // column.getLongValue());
            // }

            result = keyspace
                    .prepareQuery(CF_CLICK_STREAM)
                    .getKey(userId)
                    .withColumnRange(
                            SE_SERIALIZER.buildRange()
                                    .greaterThanEquals("Session3")
                                    .lessThan("Session5").build()).execute();
            // Assert.assertEquals(10, result.getResult().size());
            // LOG.info("XXXXXXXXXXXXXXXXXXXXXXXX INCLUSIVE - NON_INCLUSIVE");
            // for (Column<SessionEvent> column : result.getResult()) {
            // LOG.info("####### " + column.getName() + " = " +
            // column.getLongValue());
            // }

            result = keyspace
                    .prepareQuery(CF_CLICK_STREAM)
                    .getKey(userId)
                    .withColumnRange(
                            SE_SERIALIZER.buildRange().greaterThan("Session3")
                                    .lessThanEquals("Session5").build())
                    .execute();
            // LOG.info("XXXXXXXXXXXXXXXXXXXXXXXX NON_INCLUSIVE - INCLUSIVE");
            // Assert.assertEquals(10, result.getResult().size());
            // for (Column<SessionEvent> column : result.getResult()) {
            // LOG.info("####### " + column.getName() + " = " +
            // column.getLongValue());
            // }

            result = keyspace
                    .prepareQuery(CF_CLICK_STREAM)
                    .getKey(userId)
                    .withColumnRange(
                            SE_SERIALIZER.buildRange().greaterThan("Session3")
                                    .lessThan("Session5").build()).execute();
            // LOG.info("XXXXXXXXXXXXXXXXXXXXXXXX NON_INCLUSIVE - NON_INCLUSIVE");
            // for (Column<SessionEvent> column : result.getResult()) {
            // LOG.info("####### " + column.getName() + " = " +
            // column.getLongValue());
            // }

            result = keyspace
                    .prepareQuery(CF_CLICK_STREAM)
                    .getKey(userId)
                    .withColumnRange(
                            SE_SERIALIZER
                                    .buildRange()
                                    .withPrefix("Session3")
                                    .greaterThanEquals(
                                            TimeUUIDUtils.getTimeUUID(2))
                                    .lessThanEquals(
                                            TimeUUIDUtils.getTimeUUID(8))
                                    .build()).execute();

            // Assert.assertEquals(10, result.getResult().size());
            // LOG.info("XXXXXXXXXXXXXXXXXXXXXXXX EQUAL - EQUAL");
            // for (Column<SessionEvent> column : result.getResult()) {
            // LOG.info("####### " + column.getName() + " = " +
            // column.getLongValue());
            // }
        } catch (ConnectionException e) {
            LOG.error(e.getMessage(), e);
            Assert.fail();
        }
    }

    @Test
    public void testChangeConsistencyLevel() {
        try {
            keyspace.prepareQuery(CF_STANDARD1)
                    .setConsistencyLevel(ConsistencyLevel.CL_ONE).getKey("A")
                    .execute();
        } catch (ConnectionException e) {
            Assert.fail(e.getMessage());
        }
    }

    @Test
    public void testCompositeKey() {
        MockCompositeType key = new MockCompositeType("A", 1, 2, true, "B");
        MutationBatch m = keyspace.prepareMutationBatch();
        m.withRow(CF_COMPOSITE_KEY, key).putColumn("Test", "Value", null);
        try {
            m.execute();
        } catch (ConnectionException e) {
            LOG.error(e.getMessage(), e);
            Assert.fail();
        }

        try {
            ColumnList<String> row = keyspace.prepareQuery(CF_COMPOSITE_KEY)
                    .getKey(key).execute().getResult();
            Assert.assertFalse(row.isEmpty());
        } catch (ConnectionException e) {
            LOG.error(e.getMessage(), e);
            Assert.fail();
        }

    }

    @Test
    public void testComposite() {
        String rowKey = "Composite1";

        boolean bool = false;
        MutationBatch m = keyspace.prepareMutationBatch();
        ColumnListMutation<MockCompositeType> mRow = m.withRow(CF_COMPOSITE,
                rowKey);
        int columnCount = 0;
        for (char part1 = 'a'; part1 <= 'b'; part1++) {
            for (int part2 = 0; part2 < 10; part2++) {
                for (int part3 = 10; part3 < 11; part3++) {
                    bool = !bool;
                    columnCount++;
                    mRow.putEmptyColumn(
                            new MockCompositeType(Character.toString(part1),
                                    part2, part3, bool, "UTF"), null);
                }
            }
        }
        LOG.info("Created " + columnCount + " columns");
        
        try {
            m.execute();
        } catch (ConnectionException e) {
            LOG.error(e.getMessage(), e);
            Assert.fail();
        }

        OperationResult<ColumnList<MockCompositeType>> result;
        try {
            result = keyspace.prepareQuery(CF_COMPOSITE).getKey(rowKey)
                    .execute();
            Assert.assertEquals(columnCount,  result.getResult().size());
            for (Column<MockCompositeType> col : result.getResult()) {
                LOG.info("COLUMN: " + col.getName().toString());
            }
        } catch (ConnectionException e) {
            LOG.error(e.getMessage(), e);
            Assert.fail();
        }

        try {
            Column<MockCompositeType> column = keyspace
                    .prepareQuery(CF_COMPOSITE).getKey(rowKey)
                    .getColumn(new MockCompositeType("a", 0, 10, true, "UTF"))
                    .execute().getResult();
            LOG.info("Got single column: " + column.getName().toString());
        } catch (ConnectionException e) {
            LOG.error(e.getMessage(), e);
            Assert.fail();
        }

        LOG.info("Range builder");
        try {
            result = keyspace
                    .prepareQuery(CF_COMPOSITE)
                    .getKey(rowKey)
                    .withColumnRange(
                            M_SERIALIZER
                                    .buildRange()
                                    .withPrefix("a")
                                    .greaterThanEquals(1)
                                    .lessThanEquals(1)
                                    .build()).execute();
            for (Column<MockCompositeType> col : result.getResult()) {
                LOG.info("COLUMN: " + col.getName().toString());
            }
        } catch (ConnectionException e) {
            LOG.error(e.getMessage(), e);
            Assert.fail();
        }

        
        /*
         * Composite c = new Composite(); c.addComponent("String1",
         * StringSerializer.get()) .addComponent(123, IntegerSerializer.get());
         * 
         * MutationBatch m = keyspace.prepareMutationBatch();
         * m.withRow(CF_COMPOSITE, "Key1") .putColumn(c, 123, null);
         * 
         * try { m.execute(); } catch (ConnectionException e) { Assert.fail(); }
         * 
         * try { OperationResult<Column<Composite>> result =
         * keyspace.prepareQuery(CF_COMPOSITE) .getKey("Key1") .getColumn(c)
         * .execute();
         * 
         * Assert.assertEquals(123, result.getResult().getIntegerValue()); }
         * catch (ConnectionException e) { Assert.fail(); }
         */
    }

    @Test
    public void testCompositeSlice() throws ConnectionException {
        AnnotatedCompositeSerializer<MockCompositeType> ser = new AnnotatedCompositeSerializer<MockCompositeType>(
                MockCompositeType.class);

        keyspace.prepareQuery(CF_COMPOSITE)
                .getKey("Key1")
                .withColumnRange(
                        ser.makeEndpoint("sessionid1", Equality.LESS_THAN)
                                .toBytes(),
                        ser.makeEndpoint("sessionid1", Equality.GREATER_THAN)
                                .toBytes(), false, 100).execute();
    }

    @Test
    public void testIndexQueryWithPagination() {
        OperationResult<Rows<String, String>> result;
        try {
            LOG.info("************************************************** testIndexQueryWithPagination: ");

            int rowCount = 0;
            int pageCount = 0;
            IndexQuery<String, String> query = keyspace
                    .prepareQuery(CF_STANDARD1).searchWithIndex()
                    .setRowLimit(10).autoPaginateRows(true).addExpression()
                    .whereColumn("Index2").equals().value(42);

            while (!(result = query.execute()).getResult().isEmpty()) {
                pageCount++;
                rowCount += result.getResult().size();
                LOG.info("==== Block ====");
                for (Row<String, String> row : result.getResult()) {
                    LOG.info("RowKey is " + row.getKey());
                }
            }

            Assert.assertEquals(pageCount, 3);
            Assert.assertEquals(rowCount, 26);
            LOG.info("************************************************** Index query: "
                    + result.getResult().size());

        } catch (ConnectionException e) {
            LOG.error(e.getMessage(), e);
            e.printStackTrace();
            Assert.fail();
        } catch (Exception e) {
            LOG.error(e.getMessage(), e);
            e.printStackTrace();
            Assert.fail();
        }
    }

    @Test
    public void testIndexQuery() {
        OperationResult<Rows<String, String>> result;
        try {
            LOG.info("************************************************** prepareGetMultiRowIndexQuery: ");

            result = keyspace.prepareQuery(CF_STANDARD1).searchWithIndex()
                    .setStartKey("").addExpression().whereColumn("Index1")
                    .equals().value(26).execute();
            Assert.assertEquals(1, result.getResult().size());
            Assert.assertEquals("Z", result.getResult().getRowByIndex(0)
                    .getKey());
            /*
             * for (Row<String, String> row : result.getResult()) {
             * LOG.info("RowKey is " + row.getKey()); for (Column<String> column
             * : row.getColumns()) { LOG.info("  Column: " + column.getName() +
             * "=" + column.getIntegerValue()); } }
             */

            LOG.info("************************************************** Index query: "
                    + result.getResult().size());

        } catch (ConnectionException e) {
            LOG.error(e.getMessage(), e);
            e.printStackTrace();
            Assert.fail();
        } catch (Exception e) {
            LOG.error(e.getMessage(), e);
            e.printStackTrace();
            Assert.fail();
        }
    }

    @Test
    public void testPreparedIndexQuery() {
        OperationResult<Rows<String, String>> result;
        try {
            LOG.info("************************************************** prepareGetMultiRowIndexQuery: ");

            PreparedIndexExpression<String, String> clause = CF_STANDARD1
                    .newIndexClause().whereColumn("Index1").equals().value(26);

            result = keyspace.prepareQuery(CF_STANDARD1).searchWithIndex()
                    .setStartKey("")
                    .addPreparedExpressions(Arrays.asList(clause)).execute();

            for (Row<String, String> row : result.getResult()) {
                LOG.info("RowKey is " + row.getKey() + " columnCount="
                        + row.getColumns().size());
                for (Column<String> column : row.getColumns()) {
                    LOG.info("  Column: " + column.getName() + "="
                            + column.getIntegerValue());
                }
            }
            Assert.assertEquals(1, result.getResult().size());
            Assert.assertEquals("Z", result.getResult().getRowByIndex(0)
                    .getKey());

            LOG.info("************************************************** Index query: "
                    + result.getResult().size());

        } catch (ConnectionException e) {
            e.printStackTrace();
            LOG.error(e.getMessage(), e);
            Assert.fail();
        } catch (Exception e) {
            e.printStackTrace();
            LOG.error(e.getMessage(), e);
            Assert.fail();
        }
    }

    @Test
    public void testIncrementCounter() {
        long baseAmount, incrAmount = 100;
        Column<String> column;

        try {
            column = getColumnValue(CF_COUNTER1, "CounterRow1", "MyCounter");
            baseAmount = column.getLongValue();
        } catch (Exception e) {
            baseAmount = 0;
        }

        MutationBatch m = keyspace.prepareMutationBatch();
        m.withRow(CF_COUNTER1, "CounterRow1").incrementCounterColumn(
                "MyCounter", incrAmount);
        try {
            m.execute();
        } catch (ConnectionException e) {
            LOG.error(e.getMessage(), e);
            Assert.fail();
        }

        column = getColumnValue(CF_COUNTER1, "CounterRow1", "MyCounter");
        Assert.assertNotNull(column);
        Assert.assertEquals(column.getLongValue(), baseAmount + incrAmount);

        m = keyspace.prepareMutationBatch();
        m.withRow(CF_COUNTER1, "CounterRow1").incrementCounterColumn(
                "MyCounter", incrAmount);
        try {
            m.execute();
        } catch (ConnectionException e) {
            LOG.error(e.getMessage(), e);
            Assert.fail();
        }

        column = getColumnValue(CF_COUNTER1, "CounterRow1", "MyCounter");
        Assert.assertNotNull(column);
        Assert.assertEquals(column.getLongValue(), baseAmount + 2 * incrAmount);
    }

    @Test
    public void testDeleteCounter() {
        Column<String> column;
        String rowKey = "CounterRowDelete1";
        String counterName = "MyCounter";

        // Increment the column
        MutationBatch m = keyspace.prepareMutationBatch();
        m.withRow(CF_COUNTER1, rowKey).incrementCounterColumn(counterName, 1);
        try {
            m.execute();
        } catch (ConnectionException e) {
            LOG.error(e.getMessage(), e);
            Assert.fail();
        }

        // Read back the value
        column = getColumnValue(CF_COUNTER1, rowKey, counterName);
        Assert.assertNotNull(column);
        Assert.assertEquals(column.getLongValue(), 1);

        // Delete the column
        try {
            // keyspace.prepareColumnMutation(CF_COUNTER1, rowKey, counterName)
            // .deleteCounterColumn().execute();
            keyspace.prepareColumnMutation(CF_COUNTER1, rowKey, counterName)
                    .deleteCounterColumn().execute();
            /*
             * m = keyspace.prepareMutationBatch(); m.withRow(CF_COUNTER1,
             * rowKey) .deleteColumn(counterName); m.execute();
             */
        } catch (ConnectionException e) {
            LOG.error(e.getMessage(), e);
            Assert.fail();
        }

        // Try to read back
        // This should be non-existent
        column = getColumnValue(CF_COUNTER1, rowKey, counterName);
        if (column != null) {
            LOG.error("Counter has value: " + column.getLongValue());
            Assert.fail();
        }
    }

    @Test
    public void testCql() {
        try {
            System.out.println("testCQL");
            LOG.info("CQL Test");
            OperationResult<CqlResult<String, String>> result = keyspace
                    .prepareQuery(CF_STANDARD1)
                    .withCql("SELECT * FROM Standard1;").execute();
            Assert.assertTrue(result.getResult().hasRows());
            Assert.assertEquals(30, result.getResult().getRows().size());
            Assert.assertFalse(result.getResult().hasNumber());
            
            Row<String, String> row;
            
            row = result.getResult().getRows().getRow("A");
            Assert.assertEquals("A", row.getKey());
            
            row = result.getResult().getRows().getRow("B");
            Assert.assertEquals("B", row.getKey());
            
            row = result.getResult().getRows().getRow("NonExistent");
            Assert.assertNull(row);
            
            row = result.getResult().getRows().getRowByIndex(10);
            Assert.assertEquals("I", row.getKey());
            
            for (Row<String, String> row1 : result.getResult().getRows()) {
              LOG.info("KEY***: " + row1.getKey()); 
            }
            
        } catch (ConnectionException e) {
            LOG.error(e.getMessage(), e);
            Assert.fail();
        }
    }

    @Test
    public void testCqlCount() {
        try {
            LOG.info("CQL Test");
            OperationResult<CqlResult<String, String>> result = keyspace
                    .prepareQuery(CF_STANDARD1)
                    .withCql("SELECT count(*) FROM Standard1 where KEY='A';")
                    .execute();

            long count = result.getResult().getRows().getRowByIndex(0).getColumns().getColumnByName("count").getLongValue();
            LOG.info("CQL Count: " + count);
        } catch (ConnectionException e) {
            LOG.error(e.getMessage(), e);
            Assert.fail();
        }
    }

    @Test
    public void testGetSingleColumn() {
        Column<String> column = getColumnValue(CF_STANDARD1, "A", "a");
        Assert.assertNotNull(column);
        Assert.assertEquals(1, column.getIntegerValue());
    }

    @Test
    public void testColumnFamilyDoesntExist() {
        ColumnFamily<String, String> cf = new ColumnFamily<String, String>(
                "DoesntExist", StringSerializer.get(), StringSerializer.get());
        OperationResult<Void> result;
        try {
            MutationBatch m = keyspace.prepareMutationBatch();
            m.withRow(cf, "Key1").putColumn("Column2", "Value2", null);
            result = m.execute();
            Assert.fail();
        } catch (ConnectionException e) {
            LOG.info(e.getMessage());
        }
    }

    @Test
    public void testKeyspaceDoesntExist() {
        AstyanaxContext<Keyspace> keyspaceContext = new AstyanaxContext.Builder()
                .forCluster(TEST_CLUSTER_NAME)
                .forKeyspace(TEST_KEYSPACE_NAME + "_DOESNT_EXIST")
                .withAstyanaxConfiguration(
                        new AstyanaxConfigurationImpl()
                                .setDiscoveryType(NodeDiscoveryType.NONE))
                .withConnectionPoolConfiguration(
                        new ConnectionPoolConfigurationImpl(TEST_CLUSTER_NAME
                                + "_" + TEST_KEYSPACE_NAME + "_DOESNT_EXIST")
                                .setMaxConnsPerHost(1).setSeeds(SEEDS))
                .buildKeyspace(ThriftFamilyFactory.getInstance());

        try {
            keyspaceContext.start();

            Keyspace ks = keyspaceContext.getEntity();

            OperationResult<Void> result = null;
            try {
                MutationBatch m = ks.prepareMutationBatch();
                m.withRow(CF_STANDARD1, "Key1").putColumn("Column2", "Value2",
                        null);
                result = m.execute();
                Assert.fail();
            } catch (ConnectionException e) {
                LOG.info(e.getMessage());
            }
        } finally {
            keyspaceContext.shutdown();
        }
    }

    @Test
    public void testGetSingleColumnNotExists() {
        Column<String> column = getColumnValue(CF_STANDARD1, "A",
                "DoesNotExist");
        Assert.assertNull(column);
    }

    @Test
    public void testGetSingleColumnNotExistsAsync() {
        Future<OperationResult<Column<String>>> future = null;
        try {
            future = keyspace.prepareQuery(CF_STANDARD1).getKey("A")
                    .getColumn("DoesNotExist").executeAsync();
            future.get(1000, TimeUnit.MILLISECONDS);
        } catch (ConnectionException e) {
            LOG.info("ConnectionException: " + e.getMessage());
            Assert.fail();
        } catch (InterruptedException e) {
            LOG.info(e.getMessage());
            Assert.fail();
        } catch (ExecutionException e) {
            if (e.getCause() instanceof NotFoundException)
                LOG.info(e.getCause().getMessage());
            else {
                Assert.fail(e.getMessage());
            }
        } catch (TimeoutException e) {
            future.cancel(true);
            LOG.info(e.getMessage());
            Assert.fail();
        }
    }

    @Test
    public void testGetSingleKeyNotExists() {
        Column<String> column = getColumnValue(CF_STANDARD1, "AA", "ab");
        Assert.assertNull(column);
    }

    @Test
    public void testFunctionalQuery() throws ConnectionException {
        OperationResult<ColumnList<String>> r1 = keyspace
                .prepareQuery(CF_STANDARD1).getKey("A").execute();
        Assert.assertEquals(28, r1.getResult().size());

        /*
         * OperationResult<Rows<String, String>> r2 = keyspace.prepareQuery()
         * .fromColumnFamily(CF_STANDARD1) .selectKeyRange("A", "Z", null, null,
         * 5) .execute();
         */
    }
    
    @Test
    public void testNullKeyInMutation() throws ConnectionException {
        try {
            keyspace.prepareMutationBatch()
                .withRow(CF_STANDARD1,  null)
                .putColumn("abc", "def");
            
            Assert.fail();
        }
        catch (NullPointerException e) {
            
        }
    }
    

    @Test
    public void testColumnSlice() throws ConnectionException {
        OperationResult<ColumnList<String>> r1 = keyspace
                .prepareQuery(CF_STANDARD1).getKey("A")
                .withColumnSlice("a", "b").execute();
        Assert.assertEquals(2, r1.getResult().size());
    }

    @Test
    public void testColumnRangeSlice() throws ConnectionException {
        OperationResult<ColumnList<String>> r1 = keyspace
                .prepareQuery(CF_STANDARD1)
                .getKey("A")
                .withColumnRange(
                        new RangeBuilder().setStart("a").setEnd("b")
                                .setLimit(5).build()).execute();
        Assert.assertEquals(2, r1.getResult().size());

        OperationResult<ColumnList<String>> r2 = keyspace
                .prepareQuery(CF_STANDARD1).getKey("A")
                .withColumnRange("a", null, false, 5).execute();
        Assert.assertEquals(5, r2.getResult().size());
        Assert.assertEquals("a", r2.getResult().getColumnByIndex(0).getName());

        ByteBuffer EMPTY_BUFFER = ByteBuffer.wrap(new byte[0]);
        OperationResult<ColumnList<String>> r3 = keyspace
                .prepareQuery(CF_STANDARD1).getKey("A")
                .withColumnRange(EMPTY_BUFFER, EMPTY_BUFFER, true, 5).execute();
        Assert.assertEquals(5, r3.getResult().size());
        Assert.assertEquals("z", r3.getResult().getColumnByIndex(0).getName());
    }

    @Test
    public void testGetColumnsWithPrefix() throws ConnectionException {
        OperationResult<ColumnList<String>> r = keyspace
                .prepareQuery(CF_STANDARD1)
                .getKey("Prefixes")
                .withColumnRange("Prefix1_\u00000", "Prefix1_\uffff", false,
                        Integer.MAX_VALUE).execute();
        Assert.assertEquals(2, r.getResult().size());
        Assert.assertEquals("Prefix1_a", r.getResult().getColumnByIndex(0)
                .getName());
        Assert.assertEquals("Prefix1_b", r.getResult().getColumnByIndex(1)
                .getName());
    }

    @Test
    public void testGetCounters() throws ConnectionException {
        LOG.info("Starting testGetCounters...");

        try {
            OperationResult<Column<String>> result = keyspace
                    .prepareQuery(CF_COUNTER1).getKey("CounterRow1")
                    .getColumn("TestCounter").execute();

            Long count = result.getResult().getLongValue();
            Assert.assertNotNull(count);
            Assert.assertTrue(count > 0);
        } catch (NotFoundException e) {

        }

        LOG.info("... testGetCounters done");
    }

    @Test
    public void testGetSingleKey() {
        try {
            for (char key = 'A'; key <= 'Z'; key++) {
                String keyName = Character.toString(key);
                OperationResult<ColumnList<String>> result = keyspace
                        .prepareQuery(CF_STANDARD1).getKey(keyName).execute();

                Assert.assertNotNull(result.getResult());

                System.out.printf("%s executed on %s in %d msec size=%d\n",
                        keyName, result.getHost(), result.getLatency(), result
                                .getResult().size());
            }
        } catch (ConnectionException e) {
            LOG.error(e.getMessage(), e);
            Assert.fail();
        }
    }

    @Test
    public void testGetSingleKeyAsync() {
        try {
            Future<OperationResult<ColumnList<String>>> result = keyspace
                    .prepareQuery(CF_STANDARD1).getKey("A").executeAsync();

            result.get(1000, TimeUnit.MILLISECONDS);

        } catch (ConnectionException e) {
            LOG.error(e.getMessage(), e);
            e.printStackTrace();
            Assert.fail();
        } catch (InterruptedException e) {
            LOG.error(e.getMessage(), e);
            e.printStackTrace();
            Assert.fail();
        } catch (ExecutionException e) {
            LOG.error(e.getMessage(), e);
            e.printStackTrace();
            Assert.fail();
        } catch (TimeoutException e) {
            LOG.error(e.getMessage(), e);
            e.printStackTrace();
            Assert.fail();
        }
    }

    @Test
    public void testGetAllKeysRoot() {
        LOG.info("Starting testGetAllKeysRoot...");

        try {
            List<String> keys = new ArrayList<String>();
            for (char key = 'A'; key <= 'Z'; key++) {
                String keyName = Character.toString(key);
                keys.add(keyName);
            }

            OperationResult<Rows<String, String>> result = keyspace
                    .prepareQuery(CF_STANDARD1)
                    .getKeySlice(keys.toArray(new String[keys.size()]))
                    .execute();

            Assert.assertEquals(26,  result.getResult().size());
            
            Row<String, String> row;
            
            row = result.getResult().getRow("A");
            Assert.assertEquals("A", row.getKey());
            
            row = result.getResult().getRow("B");
            Assert.assertEquals("B", row.getKey());
            
            row = result.getResult().getRow("NonExistent");
            Assert.assertNull(row);
            
            row = result.getResult().getRowByIndex(10);
            Assert.assertEquals("M", row.getKey());
            /*
             * LOG.info("Get " + result.getResult().size() + " keys"); for
             * (Row<String, String> row : result.getResult()) {
             * LOG.info(String.format("%s executed on %s in %d msec size=%d\n",
             * row.getKey(), result.getHost(), result.getLatency(),
             * row.getColumns().size())); for (Column<String> sc :
             * row.getColumns()) { LOG.info("  " + sc.getName());
             * ColumnList<Integer> subColumns =
             * sc.getSubColumns(IntegerSerializer.get()); for (Column<Integer>
             * sub : subColumns) { LOG.info("    " + sub.getName() + "=" +
             * sub.getStringValue()); } } }
             */

        } catch (ConnectionException e) {
            // TODO Auto-generated catch block
            e.printStackTrace();
            Assert.fail();
        }

        LOG.info("... testGetAllKeysRoot");
    }

    @Test
    public void testGetColumnSlice() {
        LOG.info("Starting testGetColumnSlice...");
        try {
            OperationResult<ColumnList<String>> result = keyspace
                    .prepareQuery(CF_STANDARD1)
                    .getKey("A")
                    .withColumnSlice(
                            new ColumnSlice<String>("c", "h").setLimit(5))
                    .execute();
            Assert.assertNotNull(result.getResult());
            Assert.assertEquals(5, result.getResult().size());
        } catch (ConnectionException e) {
            Assert.fail(e.getMessage());
        }

    }

    @Test
    public void testGetAllKeysPath() {
        LOG.info("Starting testGetAllKeysPath...");

        try {
            List<String> keys = new ArrayList<String>();
            for (char key = 'A'; key <= 'Z'; key++) {
                String keyName = Character.toString(key);
                keys.add(keyName);
            }

            OperationResult<Rows<String, String>> result = keyspace
                    .prepareQuery(CF_STANDARD1)
                    .getKeySlice(keys.toArray(new String[keys.size()]))
                    .execute();
            /*
             * System.out.printf("%s executed on %s in %d msec size=%d\n",
             * row.getKey(), result.getHost(), result.getLatency(),
             * row.getColumns().size());
             */

            // for (Row<String, String> row : result.getResult()) {
            // for (Column<Integer> column : row.getColumns()) {
            // System.out.println("  Column: " + column.getName());
            // }
            // }
            
            OperationResult<Map<String, Integer>> counts = keyspace
                .prepareQuery(CF_STANDARD1)
                .getKeySlice(keys.toArray(new String[keys.size()]))
                .getColumnCounts()
                .execute();
                        
            Assert.assertEquals(26, counts.getResult().size());
            
            for (Entry<String, Integer> count : counts.getResult().entrySet()) {
                Assert.assertEquals(new Integer(28), count.getValue());
            }
            
        } catch (ConnectionException e) {
            LOG.error(e.getMessage(), e);
            Assert.fail();
        }

        LOG.info("Starting testGetAllKeysPath...");
    }
    
    @Test
    public void testDeleteMultipleKeys() {
        LOG.info("Starting testDeleteMultipleKeys...");
        LOG.info("... testGetAllKeysPath");

    }

    @Test
    public void testMutationMerge() {
        MutationBatch m1 = keyspace.prepareMutationBatch();
        MutationBatch m2 = keyspace.prepareMutationBatch();
        MutationBatch m3 = keyspace.prepareMutationBatch();
        MutationBatch m4 = keyspace.prepareMutationBatch();
        MutationBatch m5 = keyspace.prepareMutationBatch();

        m1.withRow(CF_STANDARD1, "1").putColumn("1", "X", null);
        m2.withRow(CF_STANDARD1, "2").putColumn("2", "X", null)
                .putColumn("3", "X", null);
        m3.withRow(CF_STANDARD1, "3").putColumn("4", "X", null)
                .putColumn("5", "X", null).putColumn("6", "X", null);
        m4.withRow(CF_STANDARD1, "1").putColumn("7", "X", null)
                .putColumn("8", "X", null).putColumn("9", "X", null)
                .putColumn("10", "X", null);

        MutationBatch merged = keyspace.prepareMutationBatch();
        LOG.info(merged.toString());
        Assert.assertEquals(merged.getRowCount(), 0);

        merged.mergeShallow(m1);
        LOG.info(merged.toString());
        Assert.assertEquals(merged.getRowCount(), 1);

        merged.mergeShallow(m2);
        LOG.info(merged.toString());
        Assert.assertEquals(merged.getRowCount(), 2);

        merged.mergeShallow(m3);
        LOG.info(merged.toString());
        Assert.assertEquals(merged.getRowCount(), 3);

        merged.mergeShallow(m4);
        LOG.info(merged.toString());
        Assert.assertEquals(merged.getRowCount(), 3);

        merged.mergeShallow(m5);
        LOG.info(merged.toString());
        Assert.assertEquals(merged.getRowCount(), 3);
    }

    @Test
    public void testDelete() {
        LOG.info("Starting testDelete...");

        String rowKey = "DeleteMe_testDelete";

        MutationBatch m = keyspace.prepareMutationBatch();
        m.withRow(CF_STANDARD1, rowKey).putColumn("Column1", "X", null)
                .putColumn("Column2", "X", null);

        try {
            m.execute();
        } catch (ConnectionException e) {
            LOG.error(e.getMessage(), e);
            Assert.fail();
        }

        Assert.assertEquals(getColumnValue(CF_STANDARD1, rowKey, "Column1")
                .getStringValue(), "X");
        Assert.assertTrue(deleteColumn(CF_STANDARD1, rowKey, "Column1"));
        Assert.assertNull(getColumnValue(CF_STANDARD1, rowKey, "Column1"));

        LOG.info("... testDelete");
    }

    @Test
    public void testDeleteLotsOfColumns() {
        LOG.info("Starting testDelete...");

        String rowKey = "DeleteMe_testDeleteLotsOfColumns";

        int nColumns = 100;
        int pageSize = 25;

        // Insert a bunch of rows
        MutationBatch m = keyspace.prepareMutationBatch();
        ColumnListMutation<String> rm = m.withRow(CF_STANDARD1, rowKey);

        for (int i = 0; i < nColumns; i++) {
            rm.putEmptyColumn("" + i, null);
        }

        try {
            m.execute();
        } catch (ConnectionException e) {
            LOG.error(e.getMessage(), e);
            Assert.fail();
        }

        // Verify count
        try {
            int count = keyspace.prepareQuery(CF_STANDARD1)
                    .setConsistencyLevel(ConsistencyLevel.CL_QUORUM)
                    .getKey(rowKey).getCount().execute().getResult();
            Assert.assertEquals(nColumns, count);
        } catch (ConnectionException e) {
            Assert.fail(e.getMessage());
        }

        // Delete half of the columns
        m = keyspace.prepareMutationBatch().setConsistencyLevel(
                ConsistencyLevel.CL_QUORUM);
        rm = m.withRow(CF_STANDARD1, rowKey);

        for (int i = 0; i < nColumns / 2; i++) {
            rm.deleteColumn("" + i);
        }

        try {
            m.execute();
        } catch (ConnectionException e) {
            Assert.fail(e.getMessage());
        }

        // Verify count
        try {
            int count = getRowColumnCount(CF_STANDARD1, rowKey);
            Assert.assertEquals(nColumns / 2, count);

            count = getRowColumnCountWithPagination(CF_STANDARD1, rowKey,
                    pageSize);
            Assert.assertEquals(nColumns / 2, count);

        } catch (ConnectionException e) {
            Assert.fail(e.getMessage());
        }

        // Delete all of the columns
        m = keyspace.prepareMutationBatch().setConsistencyLevel(
                ConsistencyLevel.CL_QUORUM);
        rm = m.withRow(CF_STANDARD1, rowKey);

        for (int i = 0; i < nColumns; i++) {
            rm.deleteColumn("" + i);
        }

        try {
            m.execute();
        } catch (ConnectionException e) {
            Assert.fail(e.getMessage());
        }

        // Verify count
        try {
            int count = getRowColumnCount(CF_STANDARD1, rowKey);
            Assert.assertEquals(0, count);

            count = getRowColumnCountWithPagination(CF_STANDARD1, rowKey,
                    pageSize);
            Assert.assertEquals(0, count);
        } catch (ConnectionException e) {
            Assert.fail(e.getMessage());
        }

        LOG.info("... testDelete");
    }

    private <K, C> int getRowColumnCount(ColumnFamily<K, C> cf, K rowKey)
            throws ConnectionException {
        int count = keyspace.prepareQuery(cf)
                .setConsistencyLevel(ConsistencyLevel.CL_QUORUM).getKey(rowKey)
                .getCount().execute().getResult();

        return count;
    }

    private <K, C> int getRowColumnCountWithPagination(ColumnFamily<K, C> cf,
            K rowKey, int pageSize) throws ConnectionException {
        RowQuery<K, C> query = keyspace.prepareQuery(cf)
                .setConsistencyLevel(ConsistencyLevel.CL_QUORUM).getKey(rowKey)
                .withColumnRange(new RangeBuilder().setLimit(pageSize).build())
                .autoPaginate(true);

        ColumnList<C> result;
        int count = 0;
        while (!(result = query.execute().getResult()).isEmpty()) {
            count += result.size();
        }

        return count;
    }

    @Test
    public void testCsvLoader() {
        StringBuilder sb = new StringBuilder()
                .append("key, firstname, lastname, age, test\n")
                .append("1, eran, landau, 34, a\n")
                .append("2, netta, landau, 33, b\n")
                .append("3, arielle, landau, 6, c\n")
                .append("4, eyal, landau, 2, d\n");

        RecordReader reader = new CsvRecordReader(new StringReader(
                sb.toString()));
        RecordWriter writer = new ColumnarRecordWriter(keyspace,
                CF_USERS.getName());

        try {
            reader.start();
            writer.start();
            List<Pair<String, String>> record = null;
            while (null != (record = reader.next())) {
                writer.write(record);
            }
        } catch (IOException e) {
            LOG.error(e.getMessage(), e);
            Assert.fail();
        } catch (ConnectionException e) {
            LOG.error(e.getMessage(), e);
            Assert.fail();
        } finally {
            reader.shutdown();
            writer.shutdown();
        }

        try {
            Rows<Long, String> rows = keyspace.prepareQuery(CF_USERS)
                    .getAllRows().execute().getResult();
            new JsonRowsWriter(new PrintWriter(System.out, true),
                    keyspace.getSerializerPackage(CF_USERS.getName(), false))
                    .setRowsAsArray(false).write(rows);

            new JsonRowsWriter(new PrintWriter(System.out, true),
                    keyspace.getSerializerPackage(CF_USERS.getName(), false))
                    .setRowsAsArray(true).setCountName("_count_")
                    .setRowsName("_rows_").setNamesName("_names_").write(rows);

            new JsonRowsWriter(new PrintWriter(System.out, true),
                    keyspace.getSerializerPackage(CF_USERS.getName(), false))
                    .setRowsAsArray(true).setDynamicColumnNames(true)
                    .write(rows);

            new JsonRowsWriter(new PrintWriter(System.out, true),
                    keyspace.getSerializerPackage(CF_USERS.getName(), false))
                    .setRowsAsArray(true).setIgnoreUndefinedColumns(true)
                    .write(rows);

            new JsonRowsWriter(new PrintWriter(System.out, true),
                    keyspace.getSerializerPackage(CF_USERS.getName(), false))
                    .setRowsAsArray(true)
                    .setFixedColumnNames("firstname", "lastname")
                    .setIgnoreUndefinedColumns(true).write(rows);

            LOG.info("******* COLUMNS AS ROWS ********");
            new JsonRowsWriter(new PrintWriter(System.out, true),
                    keyspace.getSerializerPackage(CF_USERS.getName(), false))
                    .setRowsAsArray(true).setColumnsAsRows(true).write(rows);

        } catch (ConnectionException e) {
            LOG.error(e.getMessage(), e);
            Assert.fail();
        } catch (Exception e) {
            LOG.error(e.getMessage(), e);
            Assert.fail();
        }
    }

    @Test
    public void testCsvLoaderWithCustomSerializers() {
        StringBuilder sb = new StringBuilder()
                .append("key, firstname, lastname, age, test\n")
                .append("1, eran, landau, 34, a\n")
                .append("2, netta, landau, 33, b\n")
                .append("3, arielle, landau, 6, c\n")
                .append("4, eyal, landau, 2, d\n");

        SerializerPackageImpl pkg = null;
        try {
            pkg = new SerializerPackageImpl().setKeyType("LongType")
                    .setColumnNameType("UTF8Type")
                    .setDefaultValueType("UTF8Type")
                    .setValueType("firstname", "UTF8Type")
                    .setValueType("lastname", "UTF8Type")
                    .setValueType("age", "LongType");
        } catch (UnknownComparatorException e) {
            Assert.fail();
        }

        RecordReader reader = new CsvRecordReader(new StringReader(
                sb.toString()));
        RecordWriter writer = new ColumnarRecordWriter(keyspace,
                CF_USERS.getName(), pkg);

        try {
            reader.start();
            writer.start();
            List<Pair<String, String>> record = null;
            while (null != (record = reader.next())) {
                writer.write(record);
            }
        } catch (IOException e) {
            LOG.error(e.getMessage(), e);
            Assert.fail();
        } catch (ConnectionException e) {
            LOG.error(e.getMessage(), e);
            Assert.fail();
        } finally {
            reader.shutdown();
            writer.shutdown();
        }

        try {
            Rows<Long, String> rows = keyspace.prepareQuery(CF_USERS)
                    .getAllRows().execute().getResult();
            new JsonRowsWriter(new PrintWriter(System.out, true),
                    keyspace.getSerializerPackage(CF_USERS.getName(), false))
                    .setRowsAsArray(false).write(rows);
        } catch (ConnectionException e) {
            LOG.error(e.getMessage(), e);
            Assert.fail();
        } catch (Exception e) {
            LOG.error(e.getMessage(), e);
            Assert.fail();
        }
    }

    @Test
    public void testCsvLoaderComposite() {
        StringBuilder sb = new StringBuilder().append("key, column, value\n")
                .append("1, a:1, 1a1\n").append("1, b:1, 2b1\n")
                .append("2, a:1, 3a1\n").append("3, a:1, 4a1\n");

        CsvColumnReader reader = new CsvColumnReader(new StringReader(
                sb.toString()));
        RecordWriter writer = new ColumnarRecordWriter(keyspace,
                CF_COMPOSITE_CSV.getName());

        try {
            reader.start();
            writer.start();
            List<Pair<String, String>> record = null;
            while (null != (record = reader.next())) {
                writer.write(record);
            }
        } catch (IOException e) {
            LOG.error(e.getMessage(), e);
            Assert.fail();
        } catch (ConnectionException e) {
            LOG.error(e.getMessage(), e);
            Assert.fail();
        } finally {
            reader.shutdown();
            writer.shutdown();
        }

        try {
            Rows<ByteBuffer, ByteBuffer> rows = keyspace
                    .prepareQuery(CF_COMPOSITE_CSV).getAllRows().execute()
                    .getResult();
            new JsonRowsWriter(new PrintWriter(System.out, true),
                    keyspace.getSerializerPackage(CF_COMPOSITE_CSV.getName(),
                            false)).setRowsAsArray(false).write(rows);

            new JsonRowsWriter(new PrintWriter(System.out, true),
                    keyspace.getSerializerPackage(CF_COMPOSITE_CSV.getName(),
                            false)).setRowsAsArray(true)
                    .setCountName("_count_").setRowsName("_rows_")
                    .setNamesName("_names_").write(rows);

            new JsonRowsWriter(new PrintWriter(System.out, true),
                    keyspace.getSerializerPackage(CF_COMPOSITE_CSV.getName(),
                            false)).setRowsAsArray(true)
                    .setDynamicColumnNames(true).write(rows);

            new JsonRowsWriter(new PrintWriter(System.out, true),
                    keyspace.getSerializerPackage(CF_COMPOSITE_CSV.getName(),
                            false)).setRowsAsArray(true)
                    .setIgnoreUndefinedColumns(true).write(rows);

            LOG.info("******* COLUMNS AS ROWS ********");
            new JsonRowsWriter(new PrintWriter(System.out, true),
                    keyspace.getSerializerPackage(CF_COMPOSITE_CSV.getName(),
                            false)).setRowsAsArray(true).setColumnsAsRows(true)
                    .write(rows);

        } catch (ConnectionException e) {
            LOG.error(e.getMessage(), e);
            Assert.fail();
        } catch (Exception e) {
            LOG.error(e.getMessage(), e);
            Assert.fail();
        }
    }

    @Test
    public void testTtlValues() throws Exception {
        MutationBatch mb = keyspace.prepareMutationBatch();
        mb.withRow(CF_TTL, "row")
          .putColumn("TTL0", "TTL0", 0)
          .putColumn("TTLNULL", "TTLNULL", null)
          .putColumn("TTL1", "TTL1", 1);
        
        mb.execute();
        
        Thread.sleep(2000);
        
        ColumnList<String> result = keyspace.prepareQuery(CF_TTL)
            .getRow("row")
            .execute().getResult();
       
        Assert.assertEquals(2,  result.size());
        Assert.assertNotNull(result.getColumnByName("TTL0"));
        Assert.assertNotNull(result.getColumnByName("TTLNULL"));
    }
    
    @Test
    public void testCluster() {
        AstyanaxContext<Cluster> clusterContext = new AstyanaxContext.Builder()
                .forCluster(TEST_CLUSTER_NAME)
                .withAstyanaxConfiguration(new AstyanaxConfigurationImpl())
                .withConnectionPoolConfiguration(
                        new ConnectionPoolConfigurationImpl(TEST_CLUSTER_NAME)
                                .setSeeds(SEEDS).setSocketTimeout(30000)
                                .setMaxTimeoutWhenExhausted(200)
                                .setMaxConnsPerHost(1))
                .withConnectionPoolMonitor(new CountingConnectionPoolMonitor())
                .buildCluster(ThriftFamilyFactory.getInstance());

        clusterContext.start();
        Cluster cluster = clusterContext.getEntity();

        try {
            cluster.describeClusterName();
            List<KeyspaceDefinition> keyspaces = cluster.describeKeyspaces();
            LOG.info("Keyspace count:" + keyspaces.size());
            for (KeyspaceDefinition keyspace : keyspaces) {
                LOG.info("Keyspace: " + keyspace.getName());
            }
            Assert.assertNotNull(keyspaces);
            Assert.assertTrue(keyspaces.size() > 0);
        } catch (Exception e) {
            Assert.fail(e.getMessage());
        } finally {
            clusterContext.shutdown();
        }
    }

    @Test
    public void testPrefixedSerializer() {
        ColumnFamily<String, String> cf = new ColumnFamily<String, String>(
                "Standard1", StringSerializer.get(), StringSerializer.get());

        ColumnFamily<String, String> cf1 = new ColumnFamily<String, String>(
                "Standard1", new PrefixedSerializer<String, String>("Prefix1_",
                        StringSerializer.get(), StringSerializer.get()),
                StringSerializer.get());

        ColumnFamily<String, String> cf2 = new ColumnFamily<String, String>(
                "Standard1", new PrefixedSerializer<String, String>("Prefix2_",
                        StringSerializer.get(), StringSerializer.get()),
                StringSerializer.get());

        MutationBatch m = keyspace.prepareMutationBatch();
        m.withRow(cf1, "A").putColumn("Column1", "Value1", null);
        m.withRow(cf2, "A").putColumn("Column1", "Value2", null);

        try {
            m.execute();
        } catch (ConnectionException e) {
            LOG.error(e.getMessage(), e);
            Assert.fail();
        }

        try {
            OperationResult<ColumnList<String>> result = keyspace
                    .prepareQuery(cf).getKey("Prefix1_A").execute();
            Assert.assertEquals(1, result.getResult().size());
            Column<String> c = result.getResult().getColumnByName("Column1");
            Assert.assertEquals("Value1", c.getStringValue());
        } catch (ConnectionException e) {
            LOG.error(e.getMessage(), e);
            Assert.fail();
        }

        try {
            OperationResult<ColumnList<String>> result = keyspace
                    .prepareQuery(cf).getKey("Prefix2_A").execute();
            Assert.assertEquals(1, result.getResult().size());
            Column<String> c = result.getResult().getColumnByName("Column1");
            Assert.assertEquals("Value2", c.getStringValue());
        } catch (ConnectionException e) {
            LOG.error(e.getMessage(), e);
            Assert.fail();
        }

    }

    @Test
    public void testWithRetry() {
        String clusterName = TEST_CLUSTER_NAME + "_DOESNT_EXIST";
        AstyanaxContext<Keyspace> keyspaceContext = new AstyanaxContext.Builder()
                .forCluster(clusterName)
                .forKeyspace(TEST_KEYSPACE_NAME)
                .withAstyanaxConfiguration(
                        new AstyanaxConfigurationImpl()
                                .setDiscoveryType(NodeDiscoveryType.NONE))
                .withConnectionPoolConfiguration(
                        new ConnectionPoolConfigurationImpl(clusterName + "_"
                                + TEST_KEYSPACE_NAME).setMaxConnsPerHost(1)
                                .setSeeds(SEEDS))
                .withConnectionPoolMonitor(new CountingConnectionPoolMonitor())
                .buildKeyspace(ThriftFamilyFactory.getInstance());

        ColumnFamily<String, String> cf = new ColumnFamily<String, String>(
                "DoesntExist", StringSerializer.get(), StringSerializer.get());
        try {
            MutationBatch m = keyspaceContext.getEntity()
                    .prepareMutationBatch()
                    .withRetryPolicy(new ExponentialBackoff(10, 3));
            m.withRow(cf, "Key1").putColumn("Column2", "Value2", null);
            m.execute();
            Assert.fail();
        } catch (ConnectionException e) {
            LOG.info(e.getMessage());
        }
    }
    
    @Test
    public void testTtl() throws Exception {
        ColumnPrefixDistributedRowLock<String> lock = 
            new ColumnPrefixDistributedRowLock<String>(keyspace, LOCK_CF_LONG, "testTtl")
                .withTtl(2)
                .withConsistencyLevel(ConsistencyLevel.CL_ONE)
                .expireLockAfter(1,  TimeUnit.SECONDS);
        
        try {
            lock.acquire();
            Assert.assertEquals(1, lock.readLockColumns().size());
            Thread.sleep(3000);
            Assert.assertEquals(0, lock.readLockColumns().size());
        }
        catch (Exception e) {
            Assert.fail(e.getMessage());
        }
        finally {
            lock.release();
        }    
        Assert.assertEquals(0, lock.readLockColumns().size());
    }
    
    @Test
    public void testTtlWithPartialUpdate() throws Exception {
    	 MutationBatch mb = keyspace.prepareMutationBatch();
         mb.withRow(CF_TTL, "row")
           .putColumn("c1", "v1", 5)
           .putColumn("c2", "v2", 5);
         mb.execute();
         
         Thread.sleep(3000);
         
         // both columns should be alive
         ColumnList<String> result = keyspace.prepareQuery(CF_TTL)
             .getRow("row")
             .execute().getResult();
         Assert.assertEquals(2,  result.size());
         Assert.assertEquals("v1", result.getColumnByName("c1").getStringValue());
         Assert.assertEquals("v2", result.getColumnByName("c2").getStringValue());
         
    	 mb = keyspace.prepareMutationBatch();
         mb.withRow(CF_TTL, "row")
           .putColumn("c1", "v1", 5);
         mb.execute();
         
         Thread.sleep(3000);
         
         // only c1 should be alive since udpate extended its life
         result = keyspace.prepareQuery(CF_TTL)
             .getRow("row")
             .execute().getResult();
         Assert.assertEquals(1,  result.size());
         Assert.assertEquals("v1", result.getColumnByName("c1").getStringValue());
         Assert.assertNull(result.getColumnByName("c2"));
         
         Thread.sleep(3000);
         
         // both columns should be dead now
         result = keyspace.prepareQuery(CF_TTL)
                 .getRow("row")
                 .execute().getResult();
         Assert.assertEquals(0,  result.size());
    }
    
    @Test
    public void testTtlString() throws Exception {
        ColumnPrefixDistributedRowLock<String> lock = 
            new ColumnPrefixDistributedRowLock<String>(keyspace, LOCK_CF_STRING, "testTtl")
                .withTtl(2)
                .withConsistencyLevel(ConsistencyLevel.CL_ONE)
                .expireLockAfter(1,  TimeUnit.SECONDS);
        
        try {
            lock.acquire();
            Assert.assertEquals(1, lock.readLockColumns().size());
            Thread.sleep(3000);
            Assert.assertEquals(0, lock.readLockColumns().size());
        }
        catch (Exception e) {
            Assert.fail(e.getMessage());
        }
        finally {
            lock.release();
        }    
        Assert.assertEquals(0, lock.readLockColumns().size());
    }
    
    @Test
    public void testStaleLockWithFail() throws Exception {
        ColumnPrefixDistributedRowLock<String> lock1 = 
            new ColumnPrefixDistributedRowLock<String>(keyspace, LOCK_CF_LONG, "testStaleLock")
                .withTtl(TTL)
                .withConsistencyLevel(ConsistencyLevel.CL_ONE)
                .expireLockAfter(1, TimeUnit.SECONDS);
        
        ColumnPrefixDistributedRowLock<String> lock2 = 
            new ColumnPrefixDistributedRowLock<String>(keyspace, LOCK_CF_LONG, "testStaleLock")
                .withTtl(TTL)
                .withConsistencyLevel(ConsistencyLevel.CL_ONE)
                .expireLockAfter(9,  TimeUnit.SECONDS);
        
        try {
            lock1.acquire();
            Thread.sleep(5000);
            try {
                lock2.acquire();
            }
            catch (Exception e) {
                Assert.fail(e.getMessage());
            }
            finally {
                lock2.release();
            }
        }
        catch (Exception e) {
            Assert.fail(e.getMessage());
        }
        finally {
            lock1.release();
        }
    }
    
    @Test
    public void testStaleLockWithFail_String() throws Exception {
        ColumnPrefixDistributedRowLock<String> lock1 = 
            new ColumnPrefixDistributedRowLock<String>(keyspace, LOCK_CF_STRING, "testStaleLock")
                .withTtl(TTL)
                .withConsistencyLevel(ConsistencyLevel.CL_ONE)
                .expireLockAfter(1, TimeUnit.SECONDS);
        
        ColumnPrefixDistributedRowLock<String> lock2 = 
            new ColumnPrefixDistributedRowLock<String>(keyspace, LOCK_CF_STRING, "testStaleLock")
                .withTtl(TTL)
                .withConsistencyLevel(ConsistencyLevel.CL_ONE)
                .expireLockAfter(9,  TimeUnit.SECONDS);
        
        try {
            lock1.acquire();
            Thread.sleep(5000);
            try {
                lock2.acquire();
            }
            catch (Exception e) {
                Assert.fail(e.getMessage());
            }
            finally {
                lock2.release();
            }
        }
        catch (Exception e) {
            Assert.fail(e.getMessage());
        }
        finally {
            lock1.release();
        }
    }
    
    @Test
    public void testStaleLock() throws Exception {
        ColumnPrefixDistributedRowLock<String> lock1 = 
            new ColumnPrefixDistributedRowLock<String>(keyspace, LOCK_CF_LONG, "testStaleLock")
                .withTtl(TTL)
                .withConsistencyLevel(ConsistencyLevel.CL_ONE)
                .expireLockAfter(1, TimeUnit.SECONDS);
        
        ColumnPrefixDistributedRowLock<String> lock2 = 
            new ColumnPrefixDistributedRowLock<String>(keyspace, LOCK_CF_LONG, "testStaleLock")
                .failOnStaleLock(true)
                .withTtl(TTL)
                .withConsistencyLevel(ConsistencyLevel.CL_ONE)
                .expireLockAfter(9, TimeUnit.SECONDS);
        
        try {
            lock1.acquire();
            Thread.sleep(2000);
            try {
                lock2.acquire();
                Assert.fail();
            }
            catch (StaleLockException e) {
            }
            catch (Exception e) {
                Assert.fail(e.getMessage());
            }
            finally {
                lock2.release();
            }
        }
        catch (Exception e) {
            e.printStackTrace();
            Assert.fail(e.getMessage());
        }
        finally {
            lock1.release();
        }
    }
    
    @Test
    public void testStaleLock_String() throws Exception {
        ColumnPrefixDistributedRowLock<String> lock1 = 
            new ColumnPrefixDistributedRowLock<String>(keyspace, LOCK_CF_STRING, "testStaleLock")
                .withTtl(TTL)
                .withConsistencyLevel(ConsistencyLevel.CL_ONE)
                .expireLockAfter(1, TimeUnit.SECONDS);
        
        ColumnPrefixDistributedRowLock<String> lock2 = 
            new ColumnPrefixDistributedRowLock<String>(keyspace, LOCK_CF_STRING, "testStaleLock")
                .failOnStaleLock(true)
                .withTtl(TTL)
                .withConsistencyLevel(ConsistencyLevel.CL_ONE)
                .expireLockAfter(9, TimeUnit.SECONDS);
        
        try {
            lock1.acquire();
            Thread.sleep(2000);
            try {
                lock2.acquire();
                Assert.fail();
            }
            catch (StaleLockException e) {
            }
            catch (Exception e) {
                Assert.fail(e.getMessage());
            }
            finally {
                lock2.release();
            }
        }
        catch (Exception e) {
            e.printStackTrace();
            Assert.fail(e.getMessage());
        }
        finally {
            lock1.release();
        }
    }
    
    @Test
    public void testMultiLock() {
        MultiRowUniquenessConstraint unique = new MultiRowUniquenessConstraint(keyspace)
            .withConsistencyLevel(ConsistencyLevel.CL_ONE)
            .withTtl(60)
            .withLockId("abc")
            .withColumnPrefix("prefix_")
            .withRow(UNIQUE_CF, "testMultiLock_A")
            .withRow(UNIQUE_CF, "testMultiLock_B");
        
        ColumnPrefixUniquenessConstraint<String> singleUnique 
            = new ColumnPrefixUniquenessConstraint<String>(keyspace, UNIQUE_CF, "testMultiLock_A")
                .withConsistencyLevel(ConsistencyLevel.CL_ONE)
                .withPrefix("prefix_");
        try {
            unique.acquire();
            String uniqueColumn = singleUnique.readUniqueColumn();
            Assert.assertEquals("abc", uniqueColumn);
            LOG.info("UniqueColumn: " + uniqueColumn);
        }
        catch (Exception e) {
            Assert.fail(e.getMessage());
        }
        
        MultiRowUniquenessConstraint unique2 = new MultiRowUniquenessConstraint(keyspace)
            .withTtl(60)
            .withConsistencyLevel(ConsistencyLevel.CL_ONE)
            .withColumnPrefix("prefix_")
            .withRow(UNIQUE_CF, "testMultiLock_B");
        try {
            unique2.acquire();
            Assert.fail();
        }
        catch (Exception e) {
            LOG.info(e.getMessage());
        }
        
        try {
            Assert.assertEquals("abc", singleUnique.readUniqueColumn());
            unique.release();
        }
        catch (Exception e) {
            LOG.error(e.getMessage());
            Assert.fail();
        }
        
        try {
            unique2.acquire();
        }
        catch (Exception e) {
            LOG.error(e.getMessage());
            Assert.fail();
        }
        
        try {
            unique2.release();
        } catch (Exception e) {
            LOG.error(e.getMessage());
            Assert.fail();
        }
    }
    
    @Test
    public void testRowUniquenessConstraint() throws Exception {
        RowUniquenessConstraint<String, String> unique = new RowUniquenessConstraint<String, String>
                (keyspace, UNIQUE_CF, "testRowUniquenessConstraint", UUIDStringSupplier.getInstance())
                .withConsistencyLevel(ConsistencyLevel.CL_ONE)
                ;
        RowUniquenessConstraint<String, String> unique2 = new RowUniquenessConstraint<String, String>
                (keyspace, UNIQUE_CF, "testRowUniquenessConstraint", UUIDStringSupplier.getInstance())
                .withConsistencyLevel(ConsistencyLevel.CL_ONE)
                ;
        
        try {
            unique.withData("abc").acquire();
            try {
                unique2.acquire();
                Assert.fail();
            }
            catch (Exception e) {
                LOG.info(e.getMessage());
            }
            
            String data = unique.readDataAsString();
            Assert.assertNotNull(data);
        }
        catch (Exception e) {
            e.printStackTrace();
            Assert.fail(e.getMessage());
            LOG.error(e.getMessage());
        }
        finally {
            unique.release();
        }
        
        try {
            String data = unique.readDataAsString();
            Assert.fail();
        }
        catch (Exception e) {
            LOG.info("", e);
        }
    }

    @Test
    public void testPrefixUniquenessConstraint() throws Exception {
        ColumnPrefixUniquenessConstraint<String> unique = new ColumnPrefixUniquenessConstraint<String>(
                keyspace, UNIQUE_CF, "testPrefixUniquenessConstraint")
                .withConsistencyLevel(ConsistencyLevel.CL_ONE)
                ;
        ColumnPrefixUniquenessConstraint<String> unique2 = new ColumnPrefixUniquenessConstraint<String>(
                keyspace, UNIQUE_CF, "testPrefixUniquenessConstraint")
                .withConsistencyLevel(ConsistencyLevel.CL_ONE)
                ;
        
        try {
            unique.acquire();
            String column = unique.readUniqueColumn();
            LOG.info("Unique Column: " + column);
            
            try {
                unique2.acquire();
                Assert.fail();
            }
            catch (Exception e) {
                
            }
        }
        catch (Exception e) {
            Assert.fail(e.getMessage());
            LOG.error(e.getMessage());
        }
        finally {
            unique.release();
        }

        try {
            String column = unique.readUniqueColumn();
            LOG.info(column);
            Assert.fail();
        }
        catch (Exception e) {
            
        }
    }

    @Test
    public void testPrefixUniquenessConstraintWithColumn() throws Exception {
        ColumnPrefixUniquenessConstraint<String> unique = new ColumnPrefixUniquenessConstraint<String>(
                keyspace, UNIQUE_CF, "testPrefixUniquenessConstraintWithColumn")
                .withConsistencyLevel(ConsistencyLevel.CL_ONE)
                .withUniqueId("abc");
        ColumnPrefixUniquenessConstraint<String> unique2 = new ColumnPrefixUniquenessConstraint<String>(
                keyspace, UNIQUE_CF, "testPrefixUniquenessConstraintWithColumn")
                .withConsistencyLevel(ConsistencyLevel.CL_ONE)
                .withUniqueId("def");
        
        try {
            unique.acquire();
            
            String column = unique.readUniqueColumn();
            LOG.info("Unique Column: " + column);
            Assert.assertEquals("abc", column);
            
            try {
                unique2.acquire();
                Assert.fail();
            }
            catch (Exception e) {
                
            }
            
            column = unique.readUniqueColumn();
            LOG.info("Unique Column: " + column);
            Assert.assertEquals("abc", column);
            
        }
        catch (Exception e) {
            Assert.fail(e.getMessage());
            LOG.error(e.getMessage());
        }
        finally {
            unique.release();
        }
    }
    
    @Test 
    public void testAcquireAndMutate() throws Exception {
        final String row        = "testAcquireAndMutate";
        final String dataColumn = "data";
        final String value      = "test";
        
        ColumnPrefixUniquenessConstraint<String> unique = new ColumnPrefixUniquenessConstraint<String>(
                keyspace, UNIQUE_CF, row)
                .withConsistencyLevel(ConsistencyLevel.CL_ONE)
                .withUniqueId("def");
        
        try {
            unique.acquireAndApplyMutation(new Function<MutationBatch, Boolean>() {
                @Override
                public Boolean apply(@Nullable MutationBatch m) {
                    m.withRow(UNIQUE_CF, row)
                        .putColumn(dataColumn, value, null);
                    return true;
                }
            });
            String column = unique.readUniqueColumn();
            Assert.assertNotNull(column);
        }
        catch (Exception e) {
            e.printStackTrace();
            LOG.error("", e);
            Assert.fail();
        }
        finally {
        }
        
        ColumnList<String> columns = keyspace.prepareQuery(UNIQUE_CF).getKey(row).execute().getResult();
        Assert.assertEquals(2, columns.size());
        Assert.assertEquals(value, columns.getStringValue(dataColumn, null));
        
        unique.release();
        
        columns = keyspace.prepareQuery(UNIQUE_CF).getKey(row).execute().getResult();
        Assert.assertEquals(1, columns.size());
        Assert.assertEquals(value, columns.getStringValue(dataColumn, null));
    }
    
    // This test confirms the fix for https://github.com/Netflix/astyanax/issues/170
    @Test
    public void columnAutoPaginateTest() throws Exception {
        final ColumnFamily<String, UUID> CF1 = ColumnFamily.newColumnFamily("CF1", StringSerializer.get(),
                TimeUUIDSerializer.get());
        final ColumnFamily<String, String> CF2 = ColumnFamily.newColumnFamily("CF2", StringSerializer.get(),
                StringSerializer.get());
        
        keyspace.createColumnFamily(CF1, null);
        Thread.sleep(3000);
        keyspace.createColumnFamily(CF2, null);
        Thread.sleep(3000);
    
        // query on another column family with different column key type
        // does not seem to work after the first query
        keyspace.prepareQuery(CF2).getKey("anything").execute();

        MutationBatch m = keyspace.prepareMutationBatch();
        m.withRow(CF1, "test").putColumn(TimeUUIDUtils.getUniqueTimeUUIDinMillis(), "value1", null);
        m.execute();
    
        RowQuery<String, UUID> query = keyspace.prepareQuery(CF1).getKey("test").autoPaginate(true);
    
        // Adding a column range removes the problem
        // query.withColumnRange(new RangeBuilder().build());
    
        ColumnList<UUID> columns = query.execute().getResult();
        
        keyspace.prepareQuery(CF2).getKey("anything").execute();
    }
    
    private boolean deleteColumn(ColumnFamily<String, String> cf,
            String rowKey, String columnName) {
        MutationBatch m = keyspace.prepareMutationBatch();
        m.withRow(cf, rowKey).deleteColumn(columnName);

        try {
            m.execute();
            return true;
        } catch (ConnectionException e) {
            LOG.error(e.getMessage(), e);
            Assert.fail();
            return false;
        }
    }

    private Column<String> getColumnValue(ColumnFamily<String, String> cf,
            String rowKey, String columnName) {
        OperationResult<Column<String>> result;
        try {
            result = keyspace.prepareQuery(cf).getKey(rowKey)
                    .getColumn(columnName).execute();
            return result.getResult();
        } catch (NotFoundException e) {
            LOG.info(e.getMessage());
            return null;
        } catch (ConnectionException e) {
            LOG.error(e.getMessage(), e);
            Assert.fail();
            return null;
        }
    }
}=======
>>>>>>> YOURS
/home/ramdisk/experiment3/projects/astyanax/revisions/rev_2b15fd9_3de01e3/rev_2b15fd9-3de01e3/astyanax-core/src/main/java/com/netflix/astyanax/connectionpool/ConnectionPoolProxy.java;<<<<<<< MINE
=======
package com.netflix.astyanax.connectionpool;

import java.util.Collection;
import java.util.List;
import java.util.concurrent.atomic.AtomicReference;

import org.slf4j.Logger;
import org.slf4j.LoggerFactory;

import com.netflix.astyanax.connectionpool.exceptions.ConnectionException;
import com.netflix.astyanax.connectionpool.exceptions.OperationException;
import com.netflix.astyanax.connectionpool.impl.Topology;
import com.netflix.astyanax.partitioner.Partitioner;
import com.netflix.astyanax.retry.RetryPolicy;

public class ConnectionPoolProxy<T> implements ConnectionPool<T> {

	private static final Logger Logger = LoggerFactory.getLogger(ConnectionPoolProxy.class);
	
	private AtomicReference<SeedHostListener> listener = new AtomicReference<SeedHostListener>(null);
	private AtomicReference<Collection<Host>> lastHostList = new AtomicReference<Collection<Host>>(null);
	
	private final ConnectionPoolConfiguration cpConfig;
	private final ConnectionPoolMonitor monitor;
	
	public ConnectionPoolProxy(ConnectionPoolConfiguration cpConfig, ConnectionFactory<T> connectionFactory, ConnectionPoolMonitor monitor) {
		this.cpConfig = cpConfig;
		this.monitor = monitor;
	}

	public ConnectionPoolConfiguration getConnectionPoolConfiguration() {
		return cpConfig;
	}

	public ConnectionPoolMonitor getConnectionPoolMonitor() {
		return monitor;
	}

	@Override
	public void setHosts(Collection<Host> hosts) {
		
		if (hosts != null) {
			Logger.info("Setting hosts for listener here: " + listener.getClass().getName() +  "   " + hosts);
			lastHostList.set(hosts);
		}
		
		if (listener.get() != null) {
			Logger.info("Setting hosts for listener: " + listener.getClass().getName() +  "   " + hosts);
			listener.get().setHosts(hosts, cpConfig.getPort());
		}
	}
	
	public Collection<Host> getHosts() {
		return lastHostList.get();
	}

	public interface SeedHostListener {
		public void setHosts(Collection<Host> hosts, int port);
		public void shutdown();
	}
	
	public void addListener(SeedHostListener listener) {
		this.listener.set(listener);
		if (this.lastHostList.get() != null) {
			this.listener.get().setHosts(lastHostList.get(), cpConfig.getPort());
		}
	}

	@Override
	public boolean addHost(Host host, boolean refresh) {
		// TODO Auto-generated method stub
		return false;
	}

	@Override
	public boolean removeHost(Host host, boolean refresh) {
		// TODO Auto-generated method stub
		return false;
	}

	@Override
	public boolean isHostUp(Host host) {
		// TODO Auto-generated method stub
		return false;
	}

	@Override
	public boolean hasHost(Host host) {
		// TODO Auto-generated method stub
		return false;
	}

	@Override
	public List<HostConnectionPool<T>> getActivePools() {
		// TODO Auto-generated method stub
		return null;
	}

	@Override
	public List<HostConnectionPool<T>> getPools() {
		// TODO Auto-generated method stub
		return null;
	}

	@Override
	public HostConnectionPool<T> getHostPool(Host host) {
		// TODO Auto-generated method stub
		return null;
	}

	@Override
	public <R> OperationResult<R> executeWithFailover(Operation<T, R> op, RetryPolicy retry) throws ConnectionException, OperationException {
		// TODO Auto-generated method stub
		return null;
	}

	@Override
	public void shutdown() {
		// TODO Auto-generated method stub
		
	}

	@Override
	public void start() {
		// TODO Auto-generated method stub
	}

	@Override
	public Topology<T> getTopology() {
		// TODO Auto-generated method stub
		return null;
	}

	@Override
	public Partitioner getPartitioner() {
		// TODO Auto-generated method stub
		return null;
	}

}>>>>>>> YOURS
/home/ramdisk/experiment3/projects/astyanax/revisions/rev_2b15fd9_3de01e3/rev_2b15fd9-3de01e3/astyanax-core/src/main/java/com/netflix/astyanax/retry/RetryPolicy.java;<<<<<<< MINE
=======
    
    public interface RetryPolicyFactory {
    	public RetryPolicy createRetryPolicy();
    }
>>>>>>> YOURS
/home/ramdisk/experiment3/projects/astyanax/revisions/rev_2b15fd9_3de01e3/rev_2b15fd9-3de01e3/astyanax-core/src/main/java/com/netflix/astyanax/retry/RunOnceRetryPolicyFactory.java;<<<<<<< MINE
=======
package com.netflix.astyanax.retry;

import com.netflix.astyanax.retry.RetryPolicy.RetryPolicyFactory;

public class RunOnceRetryPolicyFactory implements RetryPolicyFactory {

	@Override
	public RetryPolicy createRetryPolicy() {
		return new RunOnce();
	}
}>>>>>>> YOURS
/home/ramdisk/experiment3/projects/astyanax/revisions/rev_2b15fd9_3de01e3/rev_2b15fd9-3de01e3/astyanax-cql/src/main/java/com/netflix/astyanax/cql/util/AsyncOperationResult.java;<<<<<<< MINE
=======
package com.netflix.astyanax.cql.util;

import java.util.concurrent.ExecutionException;
import java.util.concurrent.Executor;
import java.util.concurrent.TimeUnit;
import java.util.concurrent.TimeoutException;

import com.datastax.driver.core.ResultSet;
import com.datastax.driver.core.ResultSetFuture;
import com.google.common.util.concurrent.ListenableFuture;
import com.netflix.astyanax.connectionpool.OperationResult;

/**
 * Impl for istenableFuture<OperationResult<V>> that wraps the {@link ResultSetFuture} from java driver for async operations. 
 * 
 * @author poberai
 *
 * @param <V>
 */
public abstract class AsyncOperationResult<V> implements ListenableFuture<OperationResult<V>> {

	private ResultSetFuture rsFuture; 

	public AsyncOperationResult(ResultSetFuture rsFuture) {
		this.rsFuture = rsFuture;
	}

	@Override
	public boolean cancel(boolean mayInterruptIfRunning) {
		return rsFuture.cancel(mayInterruptIfRunning);
	}

	@Override
	public boolean isCancelled() {
		return rsFuture.isCancelled();
	}

	@Override
	public boolean isDone() {
		return rsFuture.isDone();
	}

	@Override
	public OperationResult<V> get() throws InterruptedException, ExecutionException {
		return getOperationResult(rsFuture.get());
	}

	public OperationResult<V> getUninterruptably() throws InterruptedException, ExecutionException {
		return getOperationResult(rsFuture.getUninterruptibly());
	}

	@Override
	public OperationResult<V> get(long timeout, TimeUnit unit) throws InterruptedException, ExecutionException, TimeoutException {
		return getOperationResult(rsFuture.get(timeout, unit));
	}

	@Override
	public void addListener(Runnable listener, Executor executor) {
		rsFuture.addListener(listener, executor);
	}

	public abstract OperationResult<V> getOperationResult(ResultSet rs);
}>>>>>>> YOURS
/home/ramdisk/experiment3/projects/astyanax/revisions/rev_2b15fd9_3de01e3/rev_2b15fd9-3de01e3/astyanax-cql/src/main/java/com/netflix/astyanax/cql/util/DataTypeMapping.java;<<<<<<< MINE
=======
package com.netflix.astyanax.cql.util;

import com.datastax.driver.core.DataType;
import com.datastax.driver.core.Row;

public class DataTypeMapping {

	public static <T> Object getDynamicColumn(Row row, String columnName, DataType dataType) {
		
		switch(dataType.getName()) {

		case ASCII:
		    return row.getString(columnName);
		case BIGINT:
		    return row.getLong(columnName);
		case BLOB:
		    return row.getBytes(columnName);
		case BOOLEAN:
		    return row.getBool(columnName);
		case COUNTER:
		    return row.getLong(columnName);
		case DECIMAL:
		    return row.getDecimal(columnName);
		case DOUBLE:
		    return row.getDouble(columnName);
		case FLOAT:
		    return row.getFloat(columnName);
		case INET:
		    return row.getInet(columnName);
		case INT:
		    return row.getInt(columnName);
		case TEXT:
		    return row.getString(columnName);
		case TIMESTAMP:
		    return row.getDate(columnName);
		case UUID:
		    return row.getUUID(columnName);
		case VARCHAR:
		    return row.getString(columnName);
		case VARINT:
		    return row.getLong(columnName);
		case TIMEUUID:
		    return row.getUUID(columnName);
		case LIST:
		    throw new UnsupportedOperationException("Collection objects not supported for column: " + columnName);
		case SET:
		    throw new UnsupportedOperationException("Collection objects not supported for column: " + columnName);
		case MAP:
			return row.getMap(columnName, Object.class, Object.class);
		    //throw new UnsupportedOperationException("Collection objects not supported for column: " + columnName);
		case CUSTOM:
		    throw new UnsupportedOperationException("Collection objects not supported for column: " + columnName);
		    
		default:
		    throw new UnsupportedOperationException("Unrecognized object for column: " + columnName);
		}
	}

}>>>>>>> YOURS
/home/ramdisk/experiment3/projects/astyanax/revisions/rev_2b15fd9_3de01e3/rev_2b15fd9-3de01e3/astyanax-cql/src/main/java/com/netflix/astyanax/cql/util/CFQueryContext.java;<<<<<<< MINE
=======
package com.netflix.astyanax.cql.util;

import java.nio.ByteBuffer;

import com.netflix.astyanax.cql.schema.CqlColumnFamilyDefinitionImpl;
import com.netflix.astyanax.model.ColumnFamily;
import com.netflix.astyanax.model.ConsistencyLevel;
import com.netflix.astyanax.retry.RetryPolicy;

public class CFQueryContext<K,C> {

	private final ColumnFamily<K,C> columnFamily;
	private final Object rowKey;
	private RetryPolicy retryPolicy;
	private ConsistencyLevel clLevel; 

	public CFQueryContext(ColumnFamily<K,C> cf) {
		this(cf, null, null, null);
	}

	public CFQueryContext(ColumnFamily<K,C> cf, K rKey) {
		this(cf, rKey, null, null);
	}

	public CFQueryContext(ColumnFamily<K,C> cf, K rKey, RetryPolicy retry, ConsistencyLevel cl) {
		this.columnFamily = cf;
		this.rowKey = checkRowKey(rKey);
		this.retryPolicy = retry;
		this.clLevel = cl;
	}

	public ColumnFamily<K, C> getColumnFamily() {
		return columnFamily;
	}

	public Object getRowKey() {
		return rowKey;
	}

	public void setRetryPolicy(RetryPolicy retry) {
		this.retryPolicy = retry;
	}

	public RetryPolicy getRetryPolicy() {
		return retryPolicy;
	}

	public void setConsistencyLevel(ConsistencyLevel cl) {
		this.clLevel = cl;
	}

	public ConsistencyLevel getConsistencyLevel() {
		return clLevel;
	}
	
	public Object checkRowKey(K rKey) {
		
		if (rKey == null) {
			return null;
		}
		
		CqlColumnFamilyDefinitionImpl cfDef = (CqlColumnFamilyDefinitionImpl) columnFamily.getColumnFamilyDefinition();
		
		if (cfDef.getKeyValidationClass().contains("BytesType")) {
			
			// Row key is of type bytes. Convert row key to bytebuffer if needed
			if (rKey instanceof ByteBuffer) {
				return rKey;
			}
			
			return columnFamily.getKeySerializer().toByteBuffer(rKey);
		}
		
		// else just return the row key as is
		return rKey;
	}

	public String toString() {
		StringBuilder sb = new StringBuilder();
		sb.append("CF=").append(columnFamily.getName());
		sb.append(" RowKey: ").append(rowKey);
		sb.append(" RetryPolicy: ").append(retryPolicy);
		sb.append(" ConsistencyLevel: ").append(clLevel);
		return sb.toString();
	}
}>>>>>>> YOURS
/home/ramdisk/experiment3/projects/astyanax/revisions/rev_2b15fd9_3de01e3/rev_2b15fd9-3de01e3/astyanax-cql/src/main/java/com/netflix/astyanax/cql/util/ConsistencyLevelTransform.java;<<<<<<< MINE
=======
package com.netflix.astyanax.cql.util;

import com.datastax.driver.core.ConsistencyLevel;

/**
 * Utility class for transforming Astyanx consistency level to java driver consistency level.
 * 
 * @author poberai
 *
 */
public class ConsistencyLevelTransform {

	public static ConsistencyLevel getConsistencyLevel(com.netflix.astyanax.model.ConsistencyLevel level) {
		
		ConsistencyLevel result = null;
		switch(level) {
		case CL_ONE:
			result = ConsistencyLevel.ONE;
			break;
		case CL_ALL:
			result = ConsistencyLevel.ALL;
			break;
		case CL_ANY:
			result = ConsistencyLevel.ANY;
			break;
		case CL_QUORUM:
			result = ConsistencyLevel.QUORUM;
			break;
		case CL_EACH_QUORUM:
			result = ConsistencyLevel.EACH_QUORUM;
			break;
		case CL_LOCAL_QUORUM:
			result = ConsistencyLevel.LOCAL_QUORUM;
			break;
		default:
			throw new RuntimeException("Consistency level not supported");
		}
		return result;
	}
}>>>>>>> YOURS
/home/ramdisk/experiment3/projects/astyanax/revisions/rev_2b15fd9_3de01e3/rev_2b15fd9-3de01e3/astyanax-cql/src/main/java/com/netflix/astyanax/cql/util/CqlTypeMapping.java;<<<<<<< MINE
=======
package com.netflix.astyanax.cql.util;

import java.util.HashMap;
import java.util.List;
import java.util.Map;
import java.util.Map.Entry;

import org.apache.cassandra.cql3.CQL3Type;

import sun.reflect.generics.reflectiveObjects.NotImplementedException;

import com.datastax.driver.core.Row;
import com.netflix.astyanax.Serializer;
import com.netflix.astyanax.cql.schema.CqlColumnFamilyDefinitionImpl;
import com.netflix.astyanax.ddl.ColumnDefinition;
import com.netflix.astyanax.model.ColumnFamily;
import com.netflix.astyanax.serializers.AnnotatedCompositeSerializer;
import com.netflix.astyanax.serializers.AnnotatedCompositeSerializer.ComponentSerializer;
import com.netflix.astyanax.serializers.ComparatorType;

/**
 * Helpful utility that maps the different data types and helps translate to and from Astyanax and java driver objects.
 * 
 * @author poberai
 *
 */
public class CqlTypeMapping {

	private static Map<String, CQL3Type> directTypeMap = new HashMap<String, CQL3Type>();
	private static Map<String, CQL3Type> reverseTypeMap = new HashMap<String, CQL3Type>();
	
	private static Map<CQL3Type, ComparatorType> cql3ToComparatorTypeMap = new HashMap<CQL3Type, ComparatorType>();
	private static Map<String, CQL3Type> reverseCql3ToComparatorTypeMap = new HashMap<String, CQL3Type>();
		
	static {
		
		for (CQL3Type.Native cqlType : CQL3Type.Native.values()) {
			if (!cqlType.name().contains("VAR")) {
				directTypeMap.put(cqlType.name().toLowerCase(), cqlType);
				reverseTypeMap.put(cqlType.getType().getClass().getSimpleName(), cqlType);
				reverseTypeMap.put(cqlType.getType().getClass().getName(), cqlType);
			}
		}
		
		cql3ToComparatorTypeMap.put(CQL3Type.Native.ASCII,     ComparatorType.ASCIITYPE);
		cql3ToComparatorTypeMap.put(CQL3Type.Native.BIGINT,    ComparatorType.INTEGERTYPE);
		cql3ToComparatorTypeMap.put(CQL3Type.Native.BLOB,      ComparatorType.BYTESTYPE);    
		cql3ToComparatorTypeMap.put(CQL3Type.Native.BOOLEAN,   ComparatorType.BOOLEANTYPE);  
		cql3ToComparatorTypeMap.put(CQL3Type.Native.COUNTER,   ComparatorType.COUNTERTYPE); 
		cql3ToComparatorTypeMap.put(CQL3Type.Native.DECIMAL,   ComparatorType.DECIMALTYPE);
		cql3ToComparatorTypeMap.put(CQL3Type.Native.DOUBLE,    ComparatorType.DOUBLETYPE);   
		cql3ToComparatorTypeMap.put(CQL3Type.Native.FLOAT,     ComparatorType.FLOATTYPE);   
		//cql3ToComparatorTypeMap.put(CQL3Type.Native.INET,      null);    
		cql3ToComparatorTypeMap.put(CQL3Type.Native.INT,       ComparatorType.INT32TYPE);
		cql3ToComparatorTypeMap.put(CQL3Type.Native.TEXT,      ComparatorType.UTF8TYPE);   
		cql3ToComparatorTypeMap.put(CQL3Type.Native.TIMESTAMP, ComparatorType.DATETYPE);
		cql3ToComparatorTypeMap.put(CQL3Type.Native.UUID,      ComparatorType.UUIDTYPE);     
		cql3ToComparatorTypeMap.put(CQL3Type.Native.VARCHAR,   ComparatorType.UTF8TYPE); 
		cql3ToComparatorTypeMap.put(CQL3Type.Native.VARINT,    ComparatorType.INTEGERTYPE);   
		cql3ToComparatorTypeMap.put(CQL3Type.Native.TIMEUUID,  ComparatorType.TIMEUUIDTYPE); 
		
		for (Entry<CQL3Type, ComparatorType> e : cql3ToComparatorTypeMap.entrySet()) {
			CQL3Type cql3Type = e.getKey();
			ComparatorType compType = e.getValue();
			reverseCql3ToComparatorTypeMap.put(compType.getClassName(), cql3Type);
			reverseCql3ToComparatorTypeMap.put(compType.getTypeName(), cql3Type);
		}
	}
		
	public static String getCqlType(String typeString) {
		
		CQL3Type type = directTypeMap.get(typeString.toLowerCase());
		type = (type == null) ? reverseTypeMap.get(typeString) : type;
		type = (type == null) ? reverseCql3ToComparatorTypeMap.get(typeString) : type;
			
		if (type == null) {
			throw new RuntimeException("Type not found: " + type);
		}
		return type.toString();
	}

	public static ComparatorType getComparatorType(CQL3Type cqlType) {
		return cql3ToComparatorTypeMap.get(cqlType);
	}
	
	private static <T> Object getDynamicColumn(Row row, Serializer<T> serializer, String columnName, ColumnFamily<?,?> cf) {
		
		ComparatorType comparatorType = serializer.getComparatorType();
		
		switch(comparatorType) {

		case ASCIITYPE:
			return row.getString(columnName);
		case BYTESTYPE:
			return row.getBytes(columnName);
		case INTEGERTYPE:
			return row.getInt(columnName);
		case INT32TYPE:
			return row.getInt(columnName);
		case DECIMALTYPE:
			return row.getFloat(columnName);
		case LEXICALUUIDTYPE:
			return row.getUUID(columnName);
		case LOCALBYPARTITIONERTYPE:
		    return row.getBytes(columnName);
		case LONGTYPE:
		    return row.getLong(columnName);
		case TIMEUUIDTYPE:
		    return row.getUUID(columnName);
		case UTF8TYPE:
		    return row.getString(columnName);
		case COMPOSITETYPE:
			return getCompositeColumn(row, (AnnotatedCompositeSerializer<?>) serializer, cf);
		case DYNAMICCOMPOSITETYPE:
			throw new NotImplementedException();
		case UUIDTYPE:
		    return row.getUUID(columnName);
		case COUNTERTYPE:
		    return row.getLong(columnName);
		case DOUBLETYPE:
		    return row.getDouble(columnName);
		case FLOATTYPE:
		    return row.getFloat(columnName);
		case BOOLEANTYPE:
		    return row.getBool(columnName);
		case DATETYPE:
		    return row.getDate(columnName);
		    
		default:
			throw new RuntimeException("Could not recognize comparator type: " + comparatorType.getTypeName());
		}
	}
	
	public static <T> Object getDynamicColumn(Row row, Serializer<T> serializer, int columnIndex, ColumnFamily<?,?> cf) {
		
		ComparatorType comparatorType = serializer.getComparatorType();
		
		switch(comparatorType) {

		case ASCIITYPE:
			return row.getString(columnIndex);
		case BYTESTYPE:
			return row.getBytes(columnIndex);
		case INTEGERTYPE:
			return row.getInt(columnIndex);
		case INT32TYPE:
			return row.getInt(columnIndex);
		case DECIMALTYPE:
			return row.getFloat(columnIndex);
		case LEXICALUUIDTYPE:
			return row.getUUID(columnIndex);
		case LOCALBYPARTITIONERTYPE:
		    return row.getBytes(columnIndex);
		case LONGTYPE:
		    return row.getLong(columnIndex);
		case TIMEUUIDTYPE:
		    return row.getUUID(columnIndex);
		case UTF8TYPE:
		    return row.getString(columnIndex);
		case COMPOSITETYPE:
			return getCompositeColumn(row, (AnnotatedCompositeSerializer<?>) serializer, cf);
		case DYNAMICCOMPOSITETYPE:
			throw new NotImplementedException();
		case UUIDTYPE:
		    return row.getUUID(columnIndex);
		case COUNTERTYPE:
		    return row.getLong(columnIndex);
		case DOUBLETYPE:
		    return row.getDouble(columnIndex);
		case FLOATTYPE:
		    return row.getFloat(columnIndex);
		case BOOLEANTYPE:
		    return row.getBool(columnIndex);
		case DATETYPE:
		    return row.getDate(columnIndex);
		    
		default:
			throw new RuntimeException("Could not recognize comparator type: " + comparatorType.getTypeName());
		}
	}
	
	
	private static Object getCompositeColumn(Row row, AnnotatedCompositeSerializer<?> compositeSerializer, ColumnFamily<?,?> cf) {
		
		Class<?> clazz = compositeSerializer.getClazz();
		
		Object obj = null;
		try {
			obj = clazz.newInstance();
		} catch (Exception e) {
			throw new RuntimeException(e);
		}
		
		CqlColumnFamilyDefinitionImpl cfDef = (CqlColumnFamilyDefinitionImpl) cf.getColumnFamilyDefinition();
		List<ColumnDefinition> cluseringKeyList = cfDef.getClusteringKeyColumnDefinitionList();
		
		int componentIndex = 0;
		for (ComponentSerializer<?> component : compositeSerializer.getComponents()) {
			
			Object value = getDynamicColumn(row, component.getSerializer(), cluseringKeyList.get(componentIndex).getName(), cf);
			try {
				component.setFieldValueDirectly(obj, value);
				componentIndex++;
			} catch (Exception e) {
				throw new RuntimeException(e);
			}
		}
		return obj;
	}
}>>>>>>> YOURS
/home/ramdisk/experiment3/projects/astyanax/revisions/rev_2b15fd9_3de01e3/rev_2b15fd9-3de01e3/astyanax-cql/src/main/java/com/netflix/astyanax/cql/reads/model/CqlRangeImpl.java;<<<<<<< MINE
=======
package com.netflix.astyanax.cql.reads.model;

import java.nio.ByteBuffer;

import com.netflix.astyanax.model.ByteBufferRange;
import com.netflix.astyanax.model.ColumnSlice;
import com.netflix.astyanax.query.RowQuery;

/**
 * Impl for {@link ByteBufferRange} that tracks the individual components of a {@link ColumnSlice} when using a column range
 * specification. 
 * 
 * Users of such queries (columns slices with column ranges) can use this class when performing using the {@link RowQuery}
 * 
 * @author poberai
 *
 * @param <T>
 */
public class CqlRangeImpl<T> implements ByteBufferRange {
	
	private final String columnName;
	private final T start;
    private final T end;
    private final int limit;
    private final boolean reversed;
    private int fetchSize = -1;

    public CqlRangeImpl(String columnName, T start, T end, int limit, boolean reversed, int fetchSize) {
    	this.columnName = columnName;
        this.start = start;
        this.end = end;
        this.limit = limit;
        this.reversed = reversed;
        this.fetchSize = fetchSize;
    }

    @Override
    public ByteBuffer getStart() {
		throw new UnsupportedOperationException("Operation not supported");
    }

    @Override
    public ByteBuffer getEnd() {
		throw new UnsupportedOperationException("Operation not supported");
    }

    public String getColumnName() {
    	return columnName;
    }
    
    public T getCqlStart() {
		return start;
    }

    public T getCqlEnd() {
		return end;
    }

    @Override
    public boolean isReversed() {
        return reversed;
    }

    @Override
    public int getLimit() {
        return limit;
    }
    
    public int getFetchSize() {
    	return fetchSize;
    }
    
    public void setFetchSize(int size) {
    	fetchSize = size;
    }
}>>>>>>> YOURS
/home/ramdisk/experiment3/projects/astyanax/revisions/rev_2b15fd9_3de01e3/rev_2b15fd9-3de01e3/astyanax-cql/src/main/java/com/netflix/astyanax/cql/reads/model/CqlRangeBuilder.java;<<<<<<< MINE
=======
package com.netflix.astyanax.cql.reads.model;

/**
 * Helpful class that tracks the state for building a {@link ColumnSlice} query using column range specification. 
 * 
 * @author poberai
 *
 * @param <T>
 */
public class CqlRangeBuilder<T> {
	
    private T start = null;
    private T end = null;
    private int limit = -1;
    private boolean reversed = false;
    private int fetchSize = -1;

    private String columnName = "column1";

    public CqlRangeBuilder<T> withRange(CqlRangeImpl<T> oldRange) {
    	if (oldRange != null) {
    		this.start = oldRange.getCqlStart();
    		this.end = oldRange.getCqlEnd();
    		this.limit = oldRange.getLimit();
    		this.reversed = oldRange.isReversed();
    		this.fetchSize = oldRange.getFetchSize();
    	}
        return this;
    }

    public CqlRangeBuilder<T> setLimit(int count) {
        this.limit = count;
        return this;
    }

    public int getLimit() {
    	return this.limit;
    }
    
    public CqlRangeBuilder<T> setReversed(boolean reversed) {
        this.reversed = reversed;
        return this;
    }
    
    public boolean getReversed() {
    	return this.reversed;
    }

    public CqlRangeBuilder<T> setStart(T value) {
        start = value;
        return this;
    }

    public T getStart() {
    	return this.start;
    }
    
    public CqlRangeBuilder<T> setEnd(T value) {
        end = value;
        return this;
    }

    public T getEnd() {
    	return this.end;
    }
    
    public CqlRangeBuilder<T> setColumn(String name) {
    	this.columnName = name;
    	return this;
    }

    public String getColumn() {
    	return this.columnName;
    }
    
    public CqlRangeBuilder<T> setFetchSize(int count) {
        this.fetchSize = count;
        return this;
    }

    public int getFetchSize() {
    	return this.fetchSize;
    }

    public CqlRangeImpl<T> build() {
    	return new CqlRangeImpl<T>(columnName, start, end, limit, reversed, fetchSize);
    }
}>>>>>>> YOURS
/home/ramdisk/experiment3/projects/astyanax/revisions/rev_2b15fd9_3de01e3/rev_2b15fd9-3de01e3/astyanax-cql/src/main/java/com/netflix/astyanax/cql/reads/model/CqlRowListImpl.java;<<<<<<< MINE
=======
package com.netflix.astyanax.cql.reads.model;

import java.util.ArrayList;
import java.util.Collection;
import java.util.HashMap;
import java.util.Iterator;
import java.util.List;
import java.util.Map;

import com.datastax.driver.core.ResultSet;
import com.google.common.base.Function;
import com.google.common.collect.Lists;
import com.netflix.astyanax.Serializer;
import com.netflix.astyanax.cql.schema.CqlColumnFamilyDefinitionImpl;
import com.netflix.astyanax.cql.util.CqlTypeMapping;
import com.netflix.astyanax.model.ColumnFamily;
import com.netflix.astyanax.model.Row;
import com.netflix.astyanax.model.Rows;

/**
 * Impl for {@link Rows} that parses the {@link ResultSet} from java driver and translates back to Astyanax Rows. 
 * Note that if your schema has a clustering key, then each individual row from the result set is a unique column, 
 * and all result set rows with the same partition key map to a unique Astyanax row. 
 * 
 * @author poberai
 *
 * @param <K>
 * @param <C>
 */
public class CqlRowListImpl<K, C> implements Rows<K, C> {

	private final List<Row<K,C>>   rows;
	private final Map<K, Row<K,C>> lookup;

	private final ColumnFamily<K,C> cf;
	private final CqlColumnFamilyDefinitionImpl cfDef; 
	
	public CqlRowListImpl() {
		this.rows = new ArrayList<Row<K, C>>();
		this.lookup = new HashMap<K, Row<K,C>>();
		
		this.cf = null;
		this.cfDef = null;
	}
	
	public CqlRowListImpl(List<Row<K,C>> newRows) {
		this.rows = new ArrayList<Row<K, C>>();
		this.rows.addAll(newRows);
		this.lookup = new HashMap<K, Row<K,C>>();
		for (Row<K,C> row : this.rows) {
			this.lookup.put(row.getKey(), row);
		}
		this.cf = null;
		this.cfDef = null;
	}

	public CqlRowListImpl(List<com.datastax.driver.core.Row> resultRows, ColumnFamily<K,C> cf) {
		
		this.rows = new ArrayList<Row<K, C>>();
		this.lookup = new HashMap<K, Row<K,C>>();
		
		this.cf = cf;
		this.cfDef = (CqlColumnFamilyDefinitionImpl) cf.getColumnFamilyDefinition();
		
		Serializer<?> keySerializer = cf.getKeySerializer();
		K prevKey = null; 
		List<com.datastax.driver.core.Row> tempList = new ArrayList<com.datastax.driver.core.Row>();
		
		for (com.datastax.driver.core.Row row : resultRows) {
			
			K rowKey = (K) CqlTypeMapping.getDynamicColumn(row, keySerializer, 0, cf);
			
			if (prevKey == null || prevKey.equals(rowKey)) {
				tempList.add(row);
			} else {
			
				// we found a set of contiguous rows that match with the same row key
				addToResultRows(tempList);
				tempList = new ArrayList<com.datastax.driver.core.Row>();
				tempList.add(row);
			}
			prevKey = rowKey;
		}
		// flush the final list
		if (tempList.size() > 0) {
			addToResultRows(tempList);
		}
		
		for (Row<K,C> row : rows) {
			this.lookup.put(row.getKey(),  row);
		}
	}

	private void addToResultRows(List<com.datastax.driver.core.Row> rowList) {

		if (cfDef.getClusteringKeyColumnDefinitionList().size() == 0 || cfDef.getRegularColumnDefinitionList().size() > 1) {
			for (com.datastax.driver.core.Row row : rowList) {
				this.rows.add(new CqlRowImpl<K, C>(row, cf));
			}
		} else {
			this.rows.add(new CqlRowImpl<K, C>(rowList, cf));
		}
	}
	
	@Override
	public Iterator<Row<K, C>> iterator() {
		return rows.iterator();
	}

	@Override
	public Row<K, C> getRow(K key) {
		return lookup.get(key);
	}

	@Override
	public int size() {
		return rows.size();
	}

	@Override
	public boolean isEmpty() {
		return rows.isEmpty();
	}

	@Override
	public Row<K, C> getRowByIndex(int index) {
		return rows.get(index);
	}

	@Override
	public Collection<K> getKeys() {
		return Lists.transform(rows, new Function<Row<K,C>, K>() {
			@Override
			public K apply(Row<K, C> row) {
				return row.getKey();
			}
		});
	}
}>>>>>>> YOURS
/home/ramdisk/experiment3/projects/astyanax/revisions/rev_2b15fd9_3de01e3/rev_2b15fd9-3de01e3/astyanax-cql/src/main/java/com/netflix/astyanax/cql/reads/model/CqlColumnListImpl.java;<<<<<<< MINE
=======
package com.netflix.astyanax.cql.reads.model;

import java.nio.ByteBuffer;
import java.util.ArrayList;
import java.util.Collection;
import java.util.Date;
import java.util.Iterator;
import java.util.LinkedHashMap;
import java.util.List;
import java.util.UUID;

import com.datastax.driver.core.ColumnDefinitions;
import com.datastax.driver.core.Row;
import com.netflix.astyanax.Serializer;
import com.netflix.astyanax.cql.schema.CqlColumnFamilyDefinitionImpl;
import com.netflix.astyanax.cql.util.CqlTypeMapping;
import com.netflix.astyanax.model.Column;
import com.netflix.astyanax.model.ColumnFamily;
import com.netflix.astyanax.model.ColumnList;

/**
 * Class that implements the {@link ColumnList} interface. Note that this class handles the case where the table schema
 * could contain a clustering key or just regular columns for a flat table. 
 * 
 * In the case of a flat table, each row has a unique set of columns. In the case of a clustering key, each row is a unique column.
 * There are 2 separate constructors to this class in order to handle each of these cases. 
 * 
 * @author poberai
 *
 * @param <C>
 */
@SuppressWarnings("unchecked")
public class CqlColumnListImpl<C> implements ColumnList<C> {

	private List<Column<C>> columnList = new ArrayList<Column<C>>();
	private LinkedHashMap<C, Column<C>> map = new LinkedHashMap<C, Column<C>>();
	
	public CqlColumnListImpl() {

	}
	
	/**
	 * This constructor is meant to be called when we have a table with standard columns i.e no composites, just plain columns
	 * @param row
	 */
	public CqlColumnListImpl(Row row, ColumnFamily<?,?> cf) {
		
		ColumnDefinitions cfDefinitions = row.getColumnDefinitions();
		
		int index = 1; // skip the key column
		while (index < cfDefinitions.size()) {
			String columnName = cfDefinitions.getName(index); 
			CqlColumnImpl<C> cqlCol = new CqlColumnImpl<C>((C) columnName, row, index);
			columnList.add(cqlCol);
			map.put((C) columnName, cqlCol);
			index+=3;  // skip past the ttl and the timestamp
		}
	}


	/**
	 * This constructor is meant to be used when we are using the CQL3 table but still in the legacy thrift mode
	 * @param rows
	 */
	public CqlColumnListImpl(List<Row> rows, ColumnFamily<?, ?> cf) {
		
		CqlColumnFamilyDefinitionImpl cfDef = (CqlColumnFamilyDefinitionImpl) cf.getColumnFamilyDefinition();
		
		int columnNameIndex = cfDef.getPartitionKeyColumnDefinitionList().size();  
		
		for (Row row : rows) {
			Object columnName = CqlTypeMapping.getDynamicColumn(row, cf.getColumnSerializer(), columnNameIndex, cf);
			int valueIndex = cfDef.getPartitionKeyColumnDefinitionList().size() + cfDef.getClusteringKeyColumnDefinitionList().size();
			
			CqlColumnImpl<C> cqlCol = new CqlColumnImpl<C>((C) columnName, row, valueIndex);
			columnList.add(cqlCol);
			map.put((C) columnName, cqlCol);
		}
	}
	
	public CqlColumnListImpl(List<CqlColumnImpl<C>> newColumnList) {
		this.columnList.clear();
		for (Column<C> column : newColumnList) {
			columnList.add(column);
			map.put(column.getName(), column);
		}
	}
	
	public void trimFirstColumn() {
		if (columnList.size() == 0) {
			return;
		}
		Column<C> firstCol = this.columnList.remove(0);
		map.remove(firstCol.getName());
	}

	@Override
	public Iterator<Column<C>> iterator() {
		return columnList.iterator();
	}

	@Override
	public Collection<C> getColumnNames() {
		return map.keySet();
	}

	@Override
	public Column<C> getColumnByName(C columnName) {
		return map.get(columnName);
	}

	@Override
	public String getStringValue(C columnName, String defaultValue) {
		
		Column<C> column = map.get(columnName);
		if (column == null) {
			return defaultValue;
		} else {
			return column.getStringValue();
		}
	}

	@Override
	public String getCompressedStringValue(C columnName, String defaultValue) {
		Column<C> column = map.get(columnName);
		if (column == null) {
			return defaultValue;
		} else {
			return column.getCompressedStringValue();
		}
	}

	@Override
	public Integer getIntegerValue(C columnName, Integer defaultValue) {
		Column<C> column = map.get(columnName);
		if (column == null) {
			return defaultValue;
		} else {
			return column.getIntegerValue();
		}
	}

	@Override
	public Double getDoubleValue(C columnName, Double defaultValue) {
		Column<C> column = map.get(columnName);
		if (column == null) {
			return defaultValue;
		} else {
			return column.getDoubleValue();
		}
	}

	@Override
	public Long getLongValue(C columnName, Long defaultValue) {
		Column<C> column = map.get(columnName);
		if (column == null) {
			return defaultValue;
		} else {
			return column.getLongValue();
		}
	}

	@Override
	public byte[] getByteArrayValue(C columnName, byte[] defaultValue) {
		Column<C> column = map.get(columnName);
		if (column == null) {
			return defaultValue;
		} else {
			return column.getByteArrayValue();
		}
	}

	@Override
	public Boolean getBooleanValue(C columnName, Boolean defaultValue) {
		Column<C> column = map.get(columnName);
		if (column == null) {
			return defaultValue;
		} else {
			return column.getBooleanValue();
		}
	}

	@Override
	public ByteBuffer getByteBufferValue(C columnName, ByteBuffer defaultValue) {
		Column<C> column = map.get(columnName);
		if (column == null) {
			return defaultValue;
		} else {
			return column.getByteBufferValue();
		}
	}

	@Override
	public <T> T getValue(C columnName, Serializer<T> serializer, T defaultValue) {
		Column<C> column = map.get(columnName);
		if (column == null) {
			return defaultValue;
		} else {
			return column.getValue(serializer);
		}
	}

	@Override
	public Date getDateValue(C columnName, Date defaultValue) {
		Column<C> column = map.get(columnName);
		if (column == null) {
			return defaultValue;
		} else {
			return column.getDateValue();
		}
	}

	@Override
	public UUID getUUIDValue(C columnName, UUID defaultValue) {
		Column<C> column = map.get(columnName);
		if (column == null) {
			return defaultValue;
		} else {
			return column.getUUIDValue();
		}
	}

	@Override
	public Column<C> getColumnByIndex(int idx) {
		return columnList.get(idx);
	}

	@Override
	public <C2> Column<C2> getSuperColumn(C columnName, Serializer<C2> colSer) {
		throw new UnsupportedOperationException("Operaiton not supported");
	}

	@Override
	public <C2> Column<C2> getSuperColumn(int idx, Serializer<C2> colSer) {
		throw new UnsupportedOperationException("Operaiton not supported");
	}

	@Override
	public boolean isEmpty() {
		return columnList.size() == 0;
	}

	@Override
	public int size() {
		return columnList.size();
	}

	@Override
	public boolean isSuperColumn() {
		throw new UnsupportedOperationException("Operaiton not supported");
	}
}>>>>>>> YOURS
/home/ramdisk/experiment3/projects/astyanax/revisions/rev_2b15fd9_3de01e3/rev_2b15fd9-3de01e3/astyanax-cql/src/main/java/com/netflix/astyanax/cql/reads/model/DirectCqlResult.java;<<<<<<< MINE
=======
package com.netflix.astyanax.cql.reads.model;

import java.util.ArrayList;
import java.util.List;

import com.datastax.driver.core.ColumnDefinitions.Definition;
import com.datastax.driver.core.ResultSet;
import com.datastax.driver.core.Row;
import com.netflix.astyanax.Serializer;
import com.netflix.astyanax.cql.util.CqlTypeMapping;
import com.netflix.astyanax.model.ColumnFamily;
import com.netflix.astyanax.model.CqlResult;
import com.netflix.astyanax.model.Rows;

/**
 * Impl for {@link CqlResult} that parses the {@link ResultSet} from java driver.
 * Note that the class does not interpret the column family definition but instead simply
 * parses each CQL column returned in the result set as an Astyanax column. 
 * Hence this class is applicable in instances where the table is considered to be a flat CQL table.  
 * 
 * @author poberai
 *
 * @param <K>
 * @param <C>
 */
public class DirectCqlResult<K, C> implements CqlResult<K, C> {

	private Long number = null;
	private CqlRowListImpl<K, C> rows; 
	
	public DirectCqlResult(List<Row> rows, ColumnFamily<K,C> cf) {
		
		List<com.netflix.astyanax.model.Row<K,C>> rowList = new ArrayList<com.netflix.astyanax.model.Row<K,C>>();
		
		for (Row row : rows) {
			rowList.add(getAstyanaxRow(row, cf));
		}
		this.rows = new CqlRowListImpl<K, C>(rowList);
	}

	public DirectCqlResult(Long number) {
		this.number = number;
	}
	
	@Override
	public Rows<K, C> getRows() {
		return this.rows;
	}

	@Override
	public int getNumber() {
		return number.intValue();
	}

	@Override
	public boolean hasRows() {
		return rows != null && rows.size() > 0;
	}

	@Override
	public boolean hasNumber() {
		return this.number != null;
	}

	private com.netflix.astyanax.model.Row<K, C> getAstyanaxRow(Row row, ColumnFamily<K,C> cf) {
		CqlRowImpl<K,C> rowImpl = new CqlRowImpl<K,C>(getAstyanaxRowKey(row, cf), getAstyanaxColumnList(row), cf);
		return rowImpl;
	}
	
	private K getAstyanaxRowKey(Row row, ColumnFamily<K,C> cf) {
		
		Serializer<K> keySerializer = cf.getKeySerializer();
		return (K) CqlTypeMapping.getDynamicColumn(row, keySerializer, 0, cf);
	}
	
	private CqlColumnListImpl<C> getAstyanaxColumnList(Row row) {
		
		List<CqlColumnImpl<C>> list = new ArrayList<CqlColumnImpl<C>>();

		List<Definition> colDefs  = row.getColumnDefinitions().asList();
		int index = 0;
		
		for (Definition colDef : colDefs) {
			C columnName = (C) colDef.getName();
			list.add(new CqlColumnImpl<C>(columnName, row, index, colDef));
			index++;
		}
		
		return new CqlColumnListImpl<C>(list);
	}
}>>>>>>> YOURS
/home/ramdisk/experiment3/projects/astyanax/revisions/rev_2b15fd9_3de01e3/rev_2b15fd9-3de01e3/astyanax-cql/src/main/java/com/netflix/astyanax/cql/reads/model/CqlRowSlice.java;<<<<<<< MINE
=======
package com.netflix.astyanax.cql.reads.model;

import java.util.Collection;

import com.netflix.astyanax.query.RowSliceQuery;

/**
 * Helper class that encapsulates a row slice for a {@link RowSliceQuery}
 * 
 * Note that there are 2 essential components for a row slice
 * 
 * 1. Collection of individual row keys
 * 2. Row range specification. 
 * 
 * The class has data structures to represent both these components and also has helpful methods to identify
 * the type of row slice query.
 * 
 * @author poberai
 *
 * @param <K>
 */
public class CqlRowSlice<K> {

	// Stuff needed for the direct query using the in() clause
	private Collection<K> keys; 
	private RowRange<K> range = new RowRange<K>();
	
	public static class RowRange<K> {
		// Stuff needed for the row range query
		private K startKey;
		private K endKey;
		private String startToken;
		private String endToken;
		int count; 
		
		public K getStartKey() {
			return startKey;
		}

		public K getEndKey() {
			return endKey;
		}

		public String getStartToken() {
			return startToken;
		}

		public String getEndToken() {
			return endToken;
		}

		public int getCount() {
			return count;
		}
	}
	
	public CqlRowSlice(Collection<K> keys) {
		this.keys = keys;
	}
	
	public CqlRowSlice(K startKey, K endKey, String startToken, String endToken, int count) {
		this.range.startKey = startKey;
		this.range.endKey = endKey;
		this.range.startToken = startToken;
		this.range.endToken = endToken;
		this.range.count = count;
	}
	
	public Collection<K> getKeys() {
		return keys;
	}
	
	public RowRange<K> getRange() {
		return range;
	}
	
	public boolean isCollectionQuery() {
		return this.keys != null && keys.size() > 0;
	}
	
	public boolean isRangeQuery() {
		return !isCollectionQuery() && range != null;
	}
}>>>>>>> YOURS
/home/ramdisk/experiment3/projects/astyanax/revisions/rev_2b15fd9_3de01e3/rev_2b15fd9-3de01e3/astyanax-cql/src/main/java/com/netflix/astyanax/cql/reads/model/CqlRowImpl.java;<<<<<<< MINE
=======
package com.netflix.astyanax.cql.reads.model;

import java.nio.ByteBuffer;
import java.util.List;

import com.datastax.driver.core.ResultSet;
import com.netflix.astyanax.cql.util.CqlTypeMapping;
import com.netflix.astyanax.model.ColumnFamily;
import com.netflix.astyanax.model.ColumnList;
import com.netflix.astyanax.model.Row;

/**
 * Impl for {@link Row} that parses the {@link ResultSet} from java driver and translates back to Astyanax Row. 
 * Note that if your schema has a clustering key, then each individual row from the result set is a unique column, 
 * and all result set rows with the same partition key map to a unique Astyanax row. 
 * 
 * @author poberai
 *
 * @param <K>
 * @param <C>
 */
@SuppressWarnings("unchecked")
public class CqlRowImpl<K, C> implements Row<K, C> {

	private final K rowKey;
	private final CqlColumnListImpl<C> cqlColumnList;
	private final ColumnFamily<K, C> cf;
	
	public CqlRowImpl(com.datastax.driver.core.Row resultRow, ColumnFamily<K, C> cf) {
		this.rowKey = (K) getRowKey(resultRow, cf);
		this.cqlColumnList = new CqlColumnListImpl<C>(resultRow, cf);
		this.cf = cf;
	}
	
	public CqlRowImpl(List<com.datastax.driver.core.Row> rows, ColumnFamily<K, C> cf) {
		this.rowKey = (K) getRowKey(rows.get(0), cf);
		this.cqlColumnList = new CqlColumnListImpl<C>(rows, cf);
		this.cf = cf;
	}
	
	public CqlRowImpl(K rKey, CqlColumnListImpl<C> colList, ColumnFamily<K, C> columnFamily) {
		this.rowKey = rKey;
		this.cqlColumnList = colList;
		this.cf = columnFamily;
	}
	
	@Override
	public K getKey() {
		return rowKey;
	}

	@Override
	public ByteBuffer getRawKey() {
		return cf.getKeySerializer().toByteBuffer(rowKey);
	}

	@Override
	public ColumnList<C> getColumns() {
		return cqlColumnList;
	}
	
	private Object getRowKey(com.datastax.driver.core.Row row, ColumnFamily<K, C> cf) {
		return CqlTypeMapping.getDynamicColumn(row, cf.getKeySerializer(), 0, cf);
	}
}>>>>>>> YOURS
/home/ramdisk/experiment3/projects/astyanax/revisions/rev_2b15fd9_3de01e3/rev_2b15fd9-3de01e3/astyanax-cql/src/main/java/com/netflix/astyanax/cql/reads/model/CqlColumnSlice.java;<<<<<<< MINE
=======
package com.netflix.astyanax.cql.reads.model;

import java.util.Collection;

import com.netflix.astyanax.model.ColumnSlice;
import com.netflix.astyanax.query.RowQuery;

/**
 * Impl for {@link ColumnSlice}. 
 * 
 * See {@link RowQuery} for where ColumnSlice can be used. There are essentially 2 components to a ColumnSLice
 * 1. Collection of Columns.
 * 2. Column range specification
 * 
 * This class encapsulates data structures for both types of ColumnSlice(s). It also maintains state that helps identify
 * the type of query being performed.
 * 
 * @author poberai
 *
 * @param <C>
 */
public class CqlColumnSlice<C> extends ColumnSlice<C> {
	
	private CqlRangeImpl<C> cqlRange;
	private Collection<C> cqlColumns;
	
	public CqlColumnSlice() {
		super(null, null);
	}

	public CqlColumnSlice(C startColumn, C endColumn) {
		super(null, null);
	}
	
	public CqlColumnSlice(CqlRangeImpl<C> cqlRange) {
		super(null, null);
		this.cqlRange = cqlRange;
	}

	public CqlColumnSlice(Collection<C> columns) {
		super(null, null);
		this.cqlColumns = columns;
	}
	
	public void setColumns(Collection<C> columns) {
		this.cqlColumns = columns;
	}
	
	public void setCqlRange(CqlRangeImpl<C> cqlRange) {
		this.cqlRange = cqlRange;
	}

	public CqlColumnSlice(ColumnSlice<C> columnSlice) {
		super(null, null);
		
		if (columnSlice instanceof CqlColumnSlice<?>) {
			initFrom(((CqlColumnSlice<C>)columnSlice));
		} else {
			
			if (columnSlice.getColumns() != null) {
				 this.cqlColumns = columnSlice.getColumns();
				 this.cqlRange = null;
			} else {
				// this is where the consumer is using the old style range query using the same code i.e no column name specified.
				// in this case we must assume the columnName = 'column1' which is the default chosen by CQL3
				this.cqlColumns = null;
				this.cqlRange = new CqlRangeBuilder<C>()
											.setColumn("column1")
											.setStart(columnSlice.getStartColumn())
											.setEnd(columnSlice.getEndColumn())
											.setReversed(columnSlice.getReversed())
											.setLimit(columnSlice.getLimit())
											.build();
			}
		}
		
	}

	public CqlColumnSlice(CqlColumnSlice<C> cqlColumnSlice) {
		super(null, null);
		initFrom(cqlColumnSlice);
	}
	
	private void initFrom(CqlColumnSlice<C> cqlColumnSlice) {
		this.cqlColumns = (Collection<C>) cqlColumnSlice.cqlColumns;
		this.cqlRange = cqlColumnSlice.cqlRange;
	}

	@Override
	public ColumnSlice<C> setLimit(int limit) {
		this.cqlRange = new CqlRangeBuilder<C>().withRange(cqlRange).setLimit(limit).build();
		return this;
	}

	@Override
	public ColumnSlice<C> setReversed(boolean value) {
		this.cqlRange = new CqlRangeBuilder<C>().withRange(cqlRange).setReversed(value).build();
		return this;
	}

	public String getColumnName() {
		return cqlRange.getColumnName();
	}

	@Override
	public Collection<C> getColumns() {
		return cqlColumns;
	}

	@Override
	public C getStartColumn() {
		return (cqlRange != null) ? (C) cqlRange.getCqlStart() : null;
	}

	@Override
	public C getEndColumn() {
		return (cqlRange != null) ? (C) cqlRange.getCqlEnd() : null;
	}

	@Override
	public boolean getReversed() {
		return (cqlRange != null ) ? cqlRange.isReversed() : false;
	}

	@Override
	public int getLimit() {
		return (cqlRange != null ) ? cqlRange.getLimit() : -1;
	}
	
	public int getFetchSize() {
		return (cqlRange != null ) ? cqlRange.getFetchSize() : -1;
	}

	public boolean isColumnSelectQuery() {
		return (this.cqlColumns != null);
	}

	public boolean isRangeQuery() {
		
		if (isColumnSelectQuery()) {
			return false;
		}
		
		if (cqlRange != null)  {
			return true;
		}
		
		return false;
	}
	
	
	public boolean isSelectAllQuery() {
		return (!isColumnSelectQuery() && !isRangeQuery());
	}
	
	public static enum QueryType {
		SELECT_ALL, COLUMN_COLLECTION, COLUMN_RANGE;
	}
	
	public QueryType getQueryType() {
		if (isSelectAllQuery()) {
			return QueryType.SELECT_ALL;
		} else if (isRangeQuery()) {
			return QueryType.COLUMN_RANGE;
		} else {
			return QueryType.COLUMN_COLLECTION;
		}
	}
}>>>>>>> YOURS
/home/ramdisk/experiment3/projects/astyanax/revisions/rev_2b15fd9_3de01e3/rev_2b15fd9-3de01e3/astyanax-cql/src/main/java/com/netflix/astyanax/cql/reads/model/CqlRowListIterator.java;<<<<<<< MINE
=======
package com.netflix.astyanax.cql.reads.model;

import java.util.ArrayList;
import java.util.Collection;
import java.util.HashMap;
import java.util.Iterator;
import java.util.List;
import java.util.Map;
import java.util.concurrent.atomic.AtomicInteger;
import java.util.concurrent.atomic.AtomicReference;

import com.datastax.driver.core.ResultSet;
import com.netflix.astyanax.Serializer;
import com.netflix.astyanax.cql.schema.CqlColumnFamilyDefinitionImpl;
import com.netflix.astyanax.cql.util.CqlTypeMapping;
import com.netflix.astyanax.model.ColumnFamily;
import com.netflix.astyanax.model.Row;
import com.netflix.astyanax.model.Rows;

/**
 * Impl for {@link Rows} that parses the {@link ResultSet} from java driver and translates back to Astyanax Rows. 
 * Note that if your schema has a clustering key, then each individual row from the result set is a unique column, 
 * and all result set rows with the same partition key map to a unique Astyanax row. 
 * 
 * Note that this class leverages the cursor support from java driver and expects the user to use the iterator based 
 * approach when reading through results which contain multiple rows. 
 * 
 * Some users may want to read all the data instead of using an iterator approach. To handle this situation,
 * the class maintains some state that indicates how the object is first accessed in order to avoid iterating twice
 * over the same result set.
 * 
 * @author poberai
 *
 * @param <K>
 * @param <C>
 */
public class CqlRowListIterator<K,C> implements Rows<K,C> {

	private enum State {
		UnSet, PreFetch, PrefetchDone, Iterator;
	}
	
	private final ResultSet resultSet; 
	private final ColumnFamily<K,C> cf; 
	private final Serializer<K> keySerializer;
	private final boolean isClusteringKey;
	
	private final AtomicReference<State> stateRef = new AtomicReference<State>(State.UnSet);
	
	private final AtomicInteger iterRowCount = new AtomicInteger(0);
	private final AtomicReference<Iterator<Row<K,C>>> iterRef = new AtomicReference<Iterator<Row<K,C>>>(null);
	
	private final List<Row<K,C>>   rows = new ArrayList<Row<K,C>>();
	private final Map<K, Row<K,C>> lookup = new HashMap<K, Row<K,C>>();

	public CqlRowListIterator(ResultSet rs, ColumnFamily<K,C> cf) {
		this.resultSet = rs;
		this.cf = cf;
		this.keySerializer = cf.getKeySerializer();
		CqlColumnFamilyDefinitionImpl cfDef = (CqlColumnFamilyDefinitionImpl) cf.getColumnFamilyDefinition();
		this.isClusteringKey = cfDef.getClusteringKeyColumnDefinitionList().size() > 0;
	}
	
	@Override
	public Iterator<Row<K, C>> iterator() {
		
		if (iterRef.get() != null) {
			return iterRef.get();
			//throw new RuntimeException("Cannot re-iterate over rows while already iterating");
		}
		
		if (stateRef.get() == State.UnSet) {
			stateRef.set(State.Iterator);
		}
		
		Iterator<Row<K,C>> rowIter =  new Iterator<Row<K,C>>() {

			private final Iterator<com.datastax.driver.core.Row> rsIter = resultSet.iterator();
			private List<com.datastax.driver.core.Row> currentList = new ArrayList<com.datastax.driver.core.Row>();
			private K currentRowKey = null; 
			
			@Override
			public boolean hasNext() {
				if (!isClusteringKey) {
					return rsIter.hasNext();
				} else {
					return rsIter.hasNext() || !currentList.isEmpty();
				}
			}

			@Override
			public Row<K, C> next() {
				
//				if (!hasNext()) {
//					throw new IllegalStateException();
//				}

				if (isClusteringKey) {
					
					// Keep reading rows till we find a new rowKey, and then return the prefecthed list as a single row
					while (rsIter.hasNext()) {
						com.datastax.driver.core.Row rsRow = rsIter.next();
						K rowKey = (K) CqlTypeMapping.getDynamicColumn(rsRow, keySerializer, 0, cf);
						if (currentRowKey == null || rowKey.equals(currentRowKey)) {
							currentList.add(rsRow);
							currentRowKey = rowKey;
						} else {
							// Ok, we have read all columns of a single row. Return the current fully formed row
							List<com.datastax.driver.core.Row> newList = new ArrayList<com.datastax.driver.core.Row>();
							newList.addAll(currentList);
							
							// reset the currentList and start with the new rowkey
							currentList = new ArrayList<com.datastax.driver.core.Row>();
							currentList.add(rsRow);
							currentRowKey = rowKey;
							iterRowCount.incrementAndGet();

							return new CqlRowImpl<K,C>(newList, cf);
						}
					}
					
					// In case we got here, then we have exhausted the rsIter and can just return the last row
					List<com.datastax.driver.core.Row> newList = new ArrayList<com.datastax.driver.core.Row>();
					newList.addAll(currentList);
					
					// reset the currentList and start with the new rowkey
					currentList = new ArrayList<com.datastax.driver.core.Row>();
					iterRowCount.incrementAndGet();
					return new CqlRowImpl<K,C>(newList, cf);
					
				} else {
					// Here each cql row corresponds to a single Astyanax row
					if (rsIter.hasNext()) {
						com.datastax.driver.core.Row rsRow = rsIter.next();
						return new CqlRowImpl<K,C>(rsRow, cf);
					} else {
						return null; // this should not happen if this is all accessed via the iterator
					}
				}
			}

			@Override
			public void remove() {
				throw new UnsupportedOperationException();
			}
		};
		
		iterRef.set(rowIter);
		return iterRef.get();
	}

	@Override
	public Collection<K> getKeys() {
		consumeAllRows();
		return lookup.keySet();
	}

	@Override
	public Row<K, C> getRow(K key) {
		consumeAllRows();
		return lookup.get(key);
	}

	@Override
	public Row<K, C> getRowByIndex(int i) {
		consumeAllRows();
		return rows.get(i);
	}

	@Override
	public int size() {
		if (stateRef.get() == State.Iterator) {
			return this.iterRowCount.get();
		} else {
			consumeAllRows();
			return rows.size();
		}
	}

	@Override
	public boolean isEmpty() {
		if (stateRef.get() == State.UnSet) {
			this.iterator(); // init the iterator
		}
		
		if (stateRef.get() == State.Iterator) {
			return !this.iterRef.get().hasNext();
		} else {
			consumeAllRows();
			return rows.size() == 0;
		}
	}
	
	private void consumeAllRows() {
		
		if (this.stateRef.get() == State.PrefetchDone) {
			return;
		}
		
		if (this.stateRef.get() == State.Iterator) {
			throw new RuntimeException("Cannot pre-fetch rows while iterating over rows");
		}
		
		this.stateRef.set(State.PreFetch);
		
		// Ok, we made it this far, we can now prefetch
		
		Iterator<Row<K,C>> rowIter = this.iterator();
		while (rowIter.hasNext()) {
			Row<K,C> row = rowIter.next();
			this.rows.add(row);
			this.lookup.put(row.getKey(), row);
		}
		
		this.iterRef.set(rows.iterator());
		stateRef.set(State.PrefetchDone);
	}

}>>>>>>> YOURS
/home/ramdisk/experiment3/projects/astyanax/revisions/rev_2b15fd9_3de01e3/rev_2b15fd9-3de01e3/astyanax-cql/src/main/java/com/netflix/astyanax/cql/reads/model/CqlColumnImpl.java;<<<<<<< MINE
=======
package com.netflix.astyanax.cql.reads.model;

import java.nio.ByteBuffer;
import java.util.Date;
import java.util.UUID;

import org.apache.cassandra.cql3.CQL3Type;

import com.datastax.driver.core.ColumnDefinitions.Definition;
import com.datastax.driver.core.DataType;
import com.datastax.driver.core.ResultSet;
import com.datastax.driver.core.Row;
import com.netflix.astyanax.Serializer;
import com.netflix.astyanax.cql.util.CqlTypeMapping;
import com.netflix.astyanax.model.Column;
import com.netflix.astyanax.model.ColumnList;
import com.netflix.astyanax.serializers.BooleanSerializer;
import com.netflix.astyanax.serializers.ComparatorType;
import com.netflix.astyanax.serializers.DateSerializer;
import com.netflix.astyanax.serializers.DoubleSerializer;
import com.netflix.astyanax.serializers.FloatSerializer;
import com.netflix.astyanax.serializers.IntegerSerializer;
import com.netflix.astyanax.serializers.LongSerializer;
import com.netflix.astyanax.serializers.ShortSerializer;
import com.netflix.astyanax.serializers.StringSerializer;
import com.netflix.astyanax.serializers.UUIDSerializer;

/**
 * Class that implements the {@link Column} interface. 
 * 
 * Note that since columns can be rows in CQL3, this class needs access to the java driver {@link Row}
 * within the java driver {@link ResultSet}
 * 
 * The index provided within the row indicates where to start parsing the Column data. 
 * Also this class handles reading the TTL and Timestamp on the Column as well. 
 * 
 * @author poberai
 *
 * @param <C>
 */
public class CqlColumnImpl<C> implements Column<C> {

	private Row row; 
	private C columnName; 
	private int index;
	
	private ComparatorType cType;
	
	private boolean isBlob = false;
	
	public CqlColumnImpl() {
	}
	
	public CqlColumnImpl(C colName, Row row, int index) {
		this.columnName = colName;
		this.row = row;
		this.index = index;

		Definition colDefinition  = row.getColumnDefinitions().asList().get(index);
		isBlob = colDefinition.getType() == DataType.blob();
	}
	
	public CqlColumnImpl(C colName, Row row, int index, Definition colDefinition) {
		this.columnName = colName;
		this.row = row;
		this.index = index;

		isBlob = colDefinition.getType() == DataType.blob();
	}

	@Override
	public C getName() {
		return columnName;
	}
	
	@Override
	public ByteBuffer getRawName() {
		return StringSerializer.get().toByteBuffer(String.valueOf(columnName));
	}

	@Override
	public long getTimestamp() {
		return row.getLong(index+2);
	}

	@Override
	public <V> V getValue(Serializer<V> valSer) {
		return valSer.fromByteBuffer(row.getBytes(index));
	}

	@Override
	public String getStringValue() {
		return (isBlob) ? StringSerializer.get().fromByteBuffer(row.getBytes(index)) : row.getString(index);
	}

	@Override
	public String getCompressedStringValue() {
		throw new UnsupportedOperationException("Operation not supported");
	}

	@Override
	public byte getByteValue() {
		return row.getBytes(index).get();
	}

	@Override
	public short getShortValue() {
		Integer i = (isBlob) ? ShortSerializer.get().fromByteBuffer(row.getBytes(index)) : row.getInt(index);
		return i.shortValue();
	}

	@Override
	public int getIntegerValue() {
		return (isBlob) ? IntegerSerializer.get().fromByteBuffer(row.getBytes(index)) : row.getInt(index);
	}

	@Override
	public float getFloatValue() {
		return (isBlob) ? FloatSerializer.get().fromByteBuffer(row.getBytes(index)) : row.getFloat(index);
	}

	@Override
	public double getDoubleValue() {
		return (isBlob) ? DoubleSerializer.get().fromByteBuffer(row.getBytes(index)) : row.getDouble(index);
	}

	@Override
	public long getLongValue() {
		return (isBlob) ? LongSerializer.get().fromByteBuffer(row.getBytes(index)) : row.getLong(index);
	}

	@Override
	public byte[] getByteArrayValue() {
		return row.getBytes(index).array();
	}

	@Override
	public boolean getBooleanValue() {
		return (isBlob) ? BooleanSerializer.get().fromByteBuffer(row.getBytes(index)) : row.getBool(index);
	}

	@Override
	public ByteBuffer getByteBufferValue() {
		return row.getBytes(index);
	}

	@Override
	public Date getDateValue() {
		return (isBlob) ? DateSerializer.get().fromByteBuffer(row.getBytes(index)) : row.getDate(index);
	}

	@Override
	public UUID getUUIDValue() {
		return (isBlob) ? UUIDSerializer.get().fromByteBuffer(row.getBytes(index)) : row.getUUID(index);
	}

	@Override
	@Deprecated
	public <C2> ColumnList<C2> getSubColumns(Serializer<C2> ser) {
		throw new UnsupportedOperationException("Operation not supported");
	}

	@Override
	@Deprecated
	public boolean isParentColumn() {
		throw new UnsupportedOperationException("Operation not supported");
	}

	@Override
	public int getTtl() {
		return row.getInt(index+1);
	}

	@Override
	public boolean hasValue() {
		return (row != null) && !(row.isNull(index));
	}
	

	public Object getGenericValue() {
		ComparatorType cType = getComparatorType();
		return CqlTypeMapping.getDynamicColumn(row, cType.getSerializer(), index, null);
	}
	
	public ComparatorType getComparatorType() {
		
		if (cType != null) {
			return cType;
		}
		
		// Lazy init
		DataType type = row.getColumnDefinitions().getType(index);
		if (type.isCollection()) {
			throw new RuntimeException("This operation does not work for collection objects");
		}
		String typeString = (type.getName().name()).toUpperCase();
		CQL3Type cql3Type = CQL3Type.Native.valueOf(typeString);
		
		cType = CqlTypeMapping.getComparatorType(cql3Type);
		
		if (cType == null) {
			throw new RuntimeException("Cannot find comparator type for CQL3Type: " + cql3Type + ", DataType: " + type.getName());
		}
		
		return cType;
	}
}>>>>>>> YOURS
/home/ramdisk/experiment3/projects/astyanax/revisions/rev_2b15fd9_3de01e3/rev_2b15fd9-3de01e3/astyanax-cql/src/main/java/com/netflix/astyanax/cql/reads/CqlRowSliceQueryImpl.java;<<<<<<< MINE
=======
package com.netflix.astyanax.cql.reads;

import java.nio.ByteBuffer;
import java.util.Arrays;
import java.util.Collection;
import java.util.List;

import com.datastax.driver.core.ResultSet;
import com.datastax.driver.core.Statement;
import com.google.common.util.concurrent.ListenableFuture;
import com.netflix.astyanax.CassandraOperationType;
import com.netflix.astyanax.Serializer;
import com.netflix.astyanax.connectionpool.OperationResult;
import com.netflix.astyanax.connectionpool.exceptions.ConnectionException;
import com.netflix.astyanax.connectionpool.exceptions.NotFoundException;
import com.netflix.astyanax.cql.CqlAbstractExecutionImpl;
import com.netflix.astyanax.cql.CqlKeyspaceImpl.KeyspaceContext;
import com.netflix.astyanax.cql.reads.model.CqlColumnSlice;
import com.netflix.astyanax.cql.reads.model.CqlRangeBuilder;
import com.netflix.astyanax.cql.reads.model.CqlRangeImpl;
import com.netflix.astyanax.cql.reads.model.CqlRowListImpl;
import com.netflix.astyanax.cql.reads.model.CqlRowListIterator;
import com.netflix.astyanax.cql.reads.model.CqlRowSlice;
import com.netflix.astyanax.cql.schema.CqlColumnFamilyDefinitionImpl;
import com.netflix.astyanax.cql.util.CFQueryContext;
import com.netflix.astyanax.model.ByteBufferRange;
import com.netflix.astyanax.model.ColumnFamily;
import com.netflix.astyanax.model.ColumnSlice;
import com.netflix.astyanax.model.Rows;
import com.netflix.astyanax.query.RowSliceColumnCountQuery;
import com.netflix.astyanax.query.RowSliceQuery;
import com.netflix.astyanax.serializers.CompositeRangeBuilder;
import com.netflix.astyanax.serializers.CompositeRangeBuilder.CompositeByteBufferRange;

/**
 * Impl for {@link RowSliceQuery} interface. 
 * 
 * Just like {@link CqlRowQueryImpl} this class only manages the context for the row slice query, but does not construct the actual
 * CQL query itself. For more details on how the actual query is constructed see classes 
 * {@link CFRowKeysQueryGen} and {@link CFRowRangeQueryGen}
 * 
 * @author poberai
 *
 * @param <K>
 * @param <C>
 */
public class CqlRowSliceQueryImpl<K, C> implements RowSliceQuery<K, C> {

	private final KeyspaceContext ksContext;
	private final CFQueryContext<K,C> cfContext;

	private final CqlRowSlice<K> rowSlice;
	private CqlColumnSlice<C> columnSlice = new CqlColumnSlice<C>();
	
	private CompositeByteBufferRange compositeRange = null;

	private final boolean isPaginating; 
	private boolean useCaching = false;
	
	public enum RowSliceQueryType {
		RowKeys, RowRange
	}

	public enum ColumnSliceQueryType {
		AllColumns, ColumnSet, ColumnRange; 
	}
	
	private final RowSliceQueryType rowQueryType;
	private ColumnSliceQueryType colQueryType = ColumnSliceQueryType.AllColumns;
	
	public CqlRowSliceQueryImpl(KeyspaceContext ksCtx, CFQueryContext<K,C> cfCtx, CqlRowSlice<K> rSlice, boolean useCaching) {
		this(ksCtx, cfCtx, rSlice, true, useCaching);
	}

	public CqlRowSliceQueryImpl(KeyspaceContext ksCtx, CFQueryContext<K,C> cfCtx, CqlRowSlice<K> rSlice, boolean condition, boolean useCaching) {
		this.ksContext = ksCtx;
		this.cfContext = cfCtx;
		this.rowSlice = rSlice;
		this.isPaginating = condition;
		this.rowQueryType = (rowSlice.isRangeQuery()) ? RowSliceQueryType.RowRange : RowSliceQueryType.RowKeys;
		this.useCaching = useCaching;
	}

	@Override
	public OperationResult<Rows<K, C>> execute() throws ConnectionException {
		return new InternalRowQueryExecutionImpl(this).execute();
	}

	@Override
	public ListenableFuture<OperationResult<Rows<K, C>>> executeAsync() throws ConnectionException {
		return new InternalRowQueryExecutionImpl(this).executeAsync();
	}

	@Override
	public RowSliceQuery<K, C> withColumnSlice(C... columns) {
		colQueryType = ColumnSliceQueryType.ColumnSet;
		return withColumnSlice(Arrays.asList(columns));
	}

	@Override
	public RowSliceQuery<K, C> withColumnSlice(Collection<C> columns) {
		colQueryType = ColumnSliceQueryType.ColumnSet;
		this.columnSlice = new CqlColumnSlice<C>(columns);
		return this;
	}

	@Override
	public RowSliceQuery<K, C> withColumnSlice(ColumnSlice<C> columns) {
		colQueryType = ColumnSliceQueryType.ColumnSet;
		this.columnSlice = new CqlColumnSlice<C>(columns);
		return this;
	}

	@Override
	public RowSliceQuery<K, C> withColumnRange(C startColumn, C endColumn, boolean reversed, int count) {
		colQueryType = ColumnSliceQueryType.ColumnRange;
		this.columnSlice = new CqlColumnSlice<C>(new CqlRangeBuilder<C>()
				.setColumn("column1")
				.setStart(startColumn)
				.setEnd(endColumn)
				.setReversed(reversed)
				.setLimit(count)
				.build());
		return this;
	}

	@Override
	public RowSliceQuery<K, C> withColumnRange(ByteBuffer startColumn, ByteBuffer endColumn, boolean reversed, int limit) {
		colQueryType = ColumnSliceQueryType.ColumnRange;
		Serializer<C> colSerializer = cfContext.getColumnFamily().getColumnSerializer();
		C start = (startColumn != null && startColumn.capacity() > 0) ? colSerializer.fromByteBuffer(startColumn) : null;
		C end = (endColumn != null && endColumn.capacity() > 0) ? colSerializer.fromByteBuffer(endColumn) : null;
		return this.withColumnRange(start, end, reversed, limit);
	}

	@SuppressWarnings("unchecked")
	@Override
	public RowSliceQuery<K, C> withColumnRange(ByteBufferRange range) {

		colQueryType = ColumnSliceQueryType.ColumnRange;

		if (range instanceof CompositeByteBufferRange) {
			this.compositeRange = (CompositeByteBufferRange) range;

		} else if (range instanceof CompositeRangeBuilder) {
			this.compositeRange = ((CompositeRangeBuilder)range).build();

		} else if (range instanceof CqlRangeImpl) {
			this.columnSlice.setCqlRange((CqlRangeImpl<C>) range);
		} else {
			return this.withColumnRange(range.getStart(), range.getEnd(), range.isReversed(), range.getLimit());
		}
		return this;
	}

	@Override
	public RowSliceColumnCountQuery<K> getColumnCounts() {
		Statement query = new InternalRowQueryExecutionImpl(this).getQuery();
		return new CqlRowSliceColumnCountQueryImpl<K>(ksContext, cfContext, query);
	}

	@SuppressWarnings("unchecked")
	private class InternalRowQueryExecutionImpl extends CqlAbstractExecutionImpl<Rows<K, C>> {

		private final CqlColumnFamilyDefinitionImpl cfDef = (CqlColumnFamilyDefinitionImpl) cf.getColumnFamilyDefinition();
		private final CqlRowSliceQueryImpl<K, C> rowSliceQuery; 
		
		public InternalRowQueryExecutionImpl(CqlRowSliceQueryImpl<K, C> rSliceQuery) {
			super(ksContext, cfContext);
			this.rowSliceQuery = rSliceQuery;
		}
		
		public InternalRowQueryExecutionImpl(KeyspaceContext ksContext, CFQueryContext<?, ?> cfContext) {
			super(ksContext, cfContext);
			this.rowSliceQuery = null;
		}

		@Override
		public Statement getQuery() {
			return cfDef.getRowQueryGenerator().getQueryStatement(rowSliceQuery, useCaching);
		}

		@Override
		public Rows<K, C> parseResultSet(ResultSet rs) throws NotFoundException {
			if (!isPaginating) {
				List<com.datastax.driver.core.Row> rows = rs.all();
				if (rows == null || rows.isEmpty()) {
					return new CqlRowListImpl<K, C>();
				}
				return new CqlRowListImpl<K, C>(rows, (ColumnFamily<K, C>) cf);
			} else {
				if (rs == null) {
					return new CqlRowListImpl<K, C>();
				}
				return new CqlRowListIterator<K, C>(rs, (ColumnFamily<K, C>) cf);
			}
		}

		@Override
		public CassandraOperationType getOperationType() {
			return CassandraOperationType.GET_ROW;
		}
	}

	public CqlRowSlice<K> getRowSlice() {
		return rowSlice;
	}
	
	public CqlColumnSlice<C> getColumnSlice() {
		return columnSlice;
	}

	public CompositeByteBufferRange getCompositeRange() {
		return compositeRange;
	}
	
	public ColumnSliceQueryType getColQueryType() {
		return colQueryType;
	}

	public RowSliceQueryType getRowQueryType() {
		return rowQueryType;
	}
}>>>>>>> YOURS
/home/ramdisk/experiment3/projects/astyanax/revisions/rev_2b15fd9_3de01e3/rev_2b15fd9-3de01e3/astyanax-cql/src/main/java/com/netflix/astyanax/cql/reads/FlatTableRowSliceQueryGen.java;<<<<<<< MINE
=======
package com.netflix.astyanax.cql.reads;

import static com.datastax.driver.core.querybuilder.QueryBuilder.gte;
import static com.datastax.driver.core.querybuilder.QueryBuilder.in;
import static com.datastax.driver.core.querybuilder.QueryBuilder.lte;

import java.math.BigInteger;
import java.util.ArrayList;
import java.util.List;
import java.util.concurrent.Callable;
import java.util.concurrent.atomic.AtomicReference;

import com.datastax.driver.core.BoundStatement;
import com.datastax.driver.core.PreparedStatement;
import com.datastax.driver.core.RegularStatement;
import com.datastax.driver.core.Session;
import com.datastax.driver.core.querybuilder.QueryBuilder;
import com.datastax.driver.core.querybuilder.Select;
import com.datastax.driver.core.querybuilder.Select.Where;
import com.netflix.astyanax.cql.reads.model.CqlRowSlice.RowRange;
import com.netflix.astyanax.cql.schema.CqlColumnFamilyDefinitionImpl;
import com.netflix.astyanax.ddl.ColumnDefinition;
import com.netflix.astyanax.query.RowSliceQuery;

/**
 * Just like {@link FlatTableRowQueryGen} this class encapsulates the functionality for row query generation for 
 * Astyanax {@link RowSliceQuery}(s). 
 * 
 * The class uses a collection of query generators to handle all sort of RowSliceQuery permutations like
 * 1. Selecting all columns for a row collection
 * 2. Selecting a column set for a row collection
 * 3. Selecting all columns for a row range
 * 4. Selecting a column set for a row range
 * 
 * Note that this class supports query generation for flat tables only. 
 * For tables with clustering keys see {@link CFRowKeysQueryGen} and {@link CFRowRangeQueryGen}.
 * 
 * Also, just like the other query generators, use this with caution when using caching of {@link PreparedStatement}
 * See {@link FlatTableRowQueryGen} for a detailed explanation of why PreparedStatement caching will not work for queries
 * that do not have the same signatures. 
 * 
 * @author poberai
 *
 */
public class FlatTableRowSliceQueryGen {

	protected AtomicReference<Session> sessionRef = new AtomicReference<Session>(null);
	protected final String keyspace; 
	protected final CqlColumnFamilyDefinitionImpl cfDef;

	protected final String partitionKeyCol;
	protected final String[] allPrimayKeyCols;
	protected final List<ColumnDefinition> regularCols;
	
	protected static final String BIND_MARKER = "?";
	
	public FlatTableRowSliceQueryGen(Session session, String keyspaceName, CqlColumnFamilyDefinitionImpl cfDefinition) {

		this.keyspace = keyspaceName;
		this.cfDef = cfDefinition;
		this.sessionRef.set(session);

		partitionKeyCol = cfDef.getPartitionKeyColumnDefinition().getName();
		allPrimayKeyCols = cfDef.getAllPkColNames();
		regularCols = cfDef.getRegularColumnDefinitionList();
	}

	/**
	 * 
	 *   SOME BASIC UTILITY METHODS USED BY ALL THE ROW SLICE QUERY GENERATORS
	 */
	
	protected Select selectAllColumnsFromKeyspaceAndCF() {

		Select.Selection select = QueryBuilder.select();
		for (int i=0; i<allPrimayKeyCols.length; i++) {
			select.column(allPrimayKeyCols[i]);
		}

		for (ColumnDefinition colDef : regularCols) {
			String colName = colDef.getName();
			select.column(colName).ttl(colName).writeTime(colName);
		}
		return select.from(keyspace, cfDef.getName());
	}

	private QueryGenCache<CqlRowSliceQueryImpl<?,?>> SelectAllColumnsForRowKeys = new QueryGenCache<CqlRowSliceQueryImpl<?,?>>(sessionRef) {

		@Override
		public Callable<RegularStatement> getQueryGen(final CqlRowSliceQueryImpl<?, ?> rowSliceQuery) {
			return new Callable<RegularStatement>() {

				@Override
				public RegularStatement call() throws Exception {
					
					Select select = selectAllColumnsFromKeyspaceAndCF();
					return select.where(in(partitionKeyCol, rowSliceQuery.getRowSlice().getKeys().toArray()));
				}
			};
		}

		@Override
		public BoundStatement bindValues(PreparedStatement pStatement, CqlRowSliceQueryImpl<?, ?> rowSliceQuery) {
			return pStatement.bind(rowSliceQuery.getRowSlice().getKeys().toArray());
		}
	};
	
	private QueryGenCache<CqlRowSliceQueryImpl<?,?>> SelectColumnSetForRowKeys = new QueryGenCache<CqlRowSliceQueryImpl<?,?>>(sessionRef) {

		@Override
		public Callable<RegularStatement> getQueryGen(final CqlRowSliceQueryImpl<?, ?> rowSliceQuery) {
			return new Callable<RegularStatement>() {

				@Override
				public RegularStatement call() throws Exception {

					Select.Selection select = QueryBuilder.select();
					select.column(partitionKeyCol);

					for (Object col : rowSliceQuery.getColumnSlice().getColumns()) {
						String columnName = (String)col; 
						select.column(columnName).ttl(columnName).writeTime(columnName);
					}

					return select.from(keyspace, cfDef.getName()).where(in(partitionKeyCol, rowSliceQuery.getRowSlice().getKeys().toArray()));
				}
			};
		}

		@Override
		public BoundStatement bindValues(PreparedStatement pStatement, CqlRowSliceQueryImpl<?, ?> rowSliceQuery) {
			
			List<Object> values = new ArrayList<Object>();
			values.addAll(rowSliceQuery.getRowSlice().getKeys());
			return pStatement.bind(values.toArray());		
		}
	};
	
	private Where addWhereClauseForRowRange(String keyAlias, Select select, RowRange<?> rowRange) {

		Where where = null;

		boolean keyIsPresent = false;
		boolean tokenIsPresent = false; 

		if (rowRange.getStartKey() != null || rowRange.getEndKey() != null) {
			keyIsPresent = true;
		}
		if (rowRange.getStartToken() != null || rowRange.getEndToken() != null) {
			tokenIsPresent = true;
		}

		if (keyIsPresent && tokenIsPresent) {
			throw new RuntimeException("Cannot provide both token and keys for range query");
		}
		
		if (keyIsPresent) {
			if (rowRange.getStartKey() != null && rowRange.getEndKey() != null) {

				where = select.where(gte(keyAlias, BIND_MARKER))
						.and(lte(keyAlias, BIND_MARKER));

			} else if (rowRange.getStartKey() != null) {				
				where = select.where(gte(keyAlias, BIND_MARKER));

			} else if (rowRange.getEndKey() != null) {
				where = select.where(lte(keyAlias, BIND_MARKER));
			}

		} else if (tokenIsPresent) {
			String tokenOfKey ="token(" + keyAlias + ")";

			if (rowRange.getStartToken() != null && rowRange.getEndToken() != null) {

				where = select.where(gte(tokenOfKey, BIND_MARKER))
						.and(lte(tokenOfKey, BIND_MARKER));

			} else if (rowRange.getStartToken() != null) {
				where = select.where(gte(tokenOfKey, BIND_MARKER));

			} else if (rowRange.getEndToken() != null) {
				where = select.where(lte(tokenOfKey, BIND_MARKER));
			}
		} else { 
			where = select.where();
		}

		if (rowRange.getCount() > 0) {
			// TODO: fix this
			//where.limit(rowRange.getCount());
		}
		return where; 
	}

	private void bindWhereClauseForRowRange(List<Object> values, RowRange<?> rowRange) {

		boolean keyIsPresent = false;
		boolean tokenIsPresent = false; 

		if (rowRange.getStartKey() != null || rowRange.getEndKey() != null) {
			keyIsPresent = true;
		}
		if (rowRange.getStartToken() != null || rowRange.getEndToken() != null) {
			tokenIsPresent = true;
		}

		if (keyIsPresent && tokenIsPresent) {
			throw new RuntimeException("Cannot provide both token and keys for range query");
		}

		if (keyIsPresent) {
			if (rowRange.getStartKey() != null) {
				values.add(rowRange.getStartKey());
			}
			if (rowRange.getEndKey() != null) {
				values.add(rowRange.getEndKey());
			}

		} else if (tokenIsPresent) {

			BigInteger startTokenB = rowRange.getStartToken() != null ? new BigInteger(rowRange.getStartToken()) : null; 
			BigInteger endTokenB = rowRange.getEndToken() != null ? new BigInteger(rowRange.getEndToken()) : null; 

			Long startToken = startTokenB.longValue();
			Long endToken = endTokenB.longValue();
			
			if (startToken != null && endToken != null) {
				if (startToken != null) {
					values.add(startToken);
				}
				if (endToken != null) {
					values.add(endToken);
				}
			}

			if (rowRange.getCount() > 0) {
				// TODO: fix this
				//where.limit(rowRange.getCount());
			}
			return; 
		}
	}


	private QueryGenCache<CqlRowSliceQueryImpl<?,?>> SelectAllColumnsForRowRange = new QueryGenCache<CqlRowSliceQueryImpl<?,?>>(sessionRef) {

		@Override
		public Callable<RegularStatement> getQueryGen(final CqlRowSliceQueryImpl<?, ?> rowSliceQuery) {
			return new Callable<RegularStatement>() {

				@Override
				public RegularStatement call() throws Exception {
					Select select = selectAllColumnsFromKeyspaceAndCF();
					return addWhereClauseForRowRange(partitionKeyCol, select, rowSliceQuery.getRowSlice().getRange());
				}
			};
		}

		@Override
		public BoundStatement bindValues(PreparedStatement pStatement, CqlRowSliceQueryImpl<?, ?> rowSliceQuery) {

			List<Object> values = new ArrayList<Object>();
			bindWhereClauseForRowRange(values, rowSliceQuery.getRowSlice().getRange());
			return pStatement.bind(values.toArray(new Object[values.size()])); 
		}
	};
	
	private QueryGenCache<CqlRowSliceQueryImpl<?,?>> SelectColumnSetForRowRange = new QueryGenCache<CqlRowSliceQueryImpl<?,?>>(sessionRef) {

		@Override
		public Callable<RegularStatement> getQueryGen(final CqlRowSliceQueryImpl<?, ?> rowSliceQuery) {
			return new Callable<RegularStatement>() {

				@Override
				public RegularStatement call() throws Exception {
					Select.Selection select = QueryBuilder.select();
					select.column(partitionKeyCol);

					for (Object col : rowSliceQuery.getColumnSlice().getColumns()) {
						String columnName = (String)col;
						select.column(columnName).ttl(columnName).writeTime(columnName);
					}

					Select selection = select.from(keyspace, cfDef.getName());
					Where where = addWhereClauseForRowRange(partitionKeyCol, selection, rowSliceQuery.getRowSlice().getRange());
					return where;
				}
			};
		}

		@Override
		public BoundStatement bindValues(PreparedStatement pStatement, CqlRowSliceQueryImpl<?, ?> rowSliceQuery) {
			
			List<Object> values = new ArrayList<Object>();
			bindWhereClauseForRowRange(values, rowSliceQuery.getRowSlice().getRange());
			return pStatement.bind(values.toArray());
		}
	};

	public BoundStatement getQueryStatement(CqlRowSliceQueryImpl<?,?> rowSliceQuery, boolean useCaching) {

		
		switch (rowSliceQuery.getRowQueryType()) {

		case RowKeys:
			return getRowKeysQueryStatement(rowSliceQuery, useCaching);
		case RowRange: 
			return getRowRangeQueryStatement(rowSliceQuery, useCaching);
		default :
			throw new RuntimeException("RowSliceQuery use case not supported.");
		}
	}
	

	
	public BoundStatement getRowKeysQueryStatement(CqlRowSliceQueryImpl<?,?> rowSliceQuery, boolean useCaching) {

		switch (rowSliceQuery.getColQueryType()) {

		case AllColumns:
			return SelectAllColumnsForRowKeys.getBoundStatement(rowSliceQuery, useCaching);
		case ColumnSet: 
			return SelectColumnSetForRowKeys.getBoundStatement(rowSliceQuery, useCaching);
		case ColumnRange:
			throw new RuntimeException("RowSliceQuery use case not supported.");
		default :
			throw new RuntimeException("RowSliceQuery use case not supported.");
		}
	}
	
	public BoundStatement getRowRangeQueryStatement(CqlRowSliceQueryImpl<?,?> rowSliceQuery, boolean useCaching) {

		switch (rowSliceQuery.getColQueryType()) {

		case AllColumns:
			return SelectAllColumnsForRowRange.getBoundStatement(rowSliceQuery, useCaching);
		case ColumnSet: 
			return SelectColumnSetForRowRange.getBoundStatement(rowSliceQuery, useCaching);
		case ColumnRange:
			throw new RuntimeException("RowSliceQuery use case not supported.");
		default :
			throw new RuntimeException("RowSliceQuery use case not supported.");
		}
	}
}>>>>>>> YOURS
/home/ramdisk/experiment3/projects/astyanax/revisions/rev_2b15fd9_3de01e3/rev_2b15fd9-3de01e3/astyanax-cql/src/main/java/com/netflix/astyanax/cql/reads/CFColumnQueryGen.java;<<<<<<< MINE
=======
package com.netflix.astyanax.cql.reads;

import static com.datastax.driver.core.querybuilder.QueryBuilder.eq;

import java.util.ArrayList;
import java.util.List;
import java.util.concurrent.Callable;
import java.util.concurrent.atomic.AtomicReference;

import com.datastax.driver.core.BoundStatement;
import com.datastax.driver.core.PreparedStatement;
import com.datastax.driver.core.RegularStatement;
import com.datastax.driver.core.Session;
import com.datastax.driver.core.querybuilder.QueryBuilder;
import com.datastax.driver.core.querybuilder.Select.Builder;
import com.datastax.driver.core.querybuilder.Select.Where;
import com.netflix.astyanax.cql.schema.CqlColumnFamilyDefinitionImpl;
import com.netflix.astyanax.ddl.ColumnDefinition;
import com.netflix.astyanax.model.ColumnFamily;
import com.netflix.astyanax.serializers.AnnotatedCompositeSerializer;
import com.netflix.astyanax.serializers.AnnotatedCompositeSerializer.ComponentSerializer;

public class CFColumnQueryGen {

	private AtomicReference<Session> sessionRef = new AtomicReference<Session>(null);
	private final String keyspace; 
	private final CqlColumnFamilyDefinitionImpl cfDef;

	private final String partitionKeyCol;
	private final List<ColumnDefinition> clusteringKeyCols;
	private final List<ColumnDefinition> regularCols;
	
	private boolean isCompositeColumn = false;
	private boolean isFlatTable = false;
	
	private static final String BIND_MARKER = "?";

	

	public CFColumnQueryGen(Session session, String keyspaceName, CqlColumnFamilyDefinitionImpl cfDefinition) {

		this.keyspace = keyspaceName;
		this.cfDef = cfDefinition;
		this.sessionRef.set(session);
		
		partitionKeyCol = cfDef.getPartitionKeyColumnDefinition().getName();
		clusteringKeyCols = cfDef.getClusteringKeyColumnDefinitionList();
		regularCols = cfDef.getRegularColumnDefinitionList();
		
		isCompositeColumn = (clusteringKeyCols.size() > 1);
		isFlatTable = (clusteringKeyCols.size() == 0);
	}
	
	private QueryGenCache<CqlColumnQueryImpl<?>> ColumnQueryWithClusteringKey = new QueryGenCache<CqlColumnQueryImpl<?>>(sessionRef) {

		@Override
		public Callable<RegularStatement> getQueryGen(final CqlColumnQueryImpl<?> columnQuery) {
			
			return new Callable<RegularStatement>() {

				@Override
				public RegularStatement call() throws Exception {
					if (clusteringKeyCols.size() != 1) {
						throw new RuntimeException("Cannot use this query for this schema, clustetingKeyCols.size: " + clusteringKeyCols.size());
					}
					
					String valueColName = regularCols.get(0).getName();

					return QueryBuilder.select()
							.column(valueColName).ttl(valueColName).writeTime(valueColName)
							.from(keyspace, cfDef.getName())
							.where(eq(partitionKeyCol, BIND_MARKER))
							.and(eq(clusteringKeyCols.get(0).getName(), BIND_MARKER));
				}
			};
		}

		@Override
		public BoundStatement bindValues(PreparedStatement pStatement, CqlColumnQueryImpl<?> columnQuery) {
			return pStatement.bind(columnQuery.getRowKey(), columnQuery.getColumnName());		
		}
	};
	
	private QueryGenCache<CqlColumnQueryImpl<?>> ColumnQueryWithCompositeColumn = new QueryGenCache<CqlColumnQueryImpl<?>>(sessionRef) {

		@Override
		public Callable<RegularStatement> getQueryGen(final CqlColumnQueryImpl<?> columnQuery) {
			
			return new Callable<RegularStatement>() {

				@Override
				public RegularStatement call() throws Exception {
					
					if (clusteringKeyCols.size() <= 1) {
						throw new RuntimeException("Cannot use this query for this schema, clustetingKeyCols.size: " + clusteringKeyCols.size());
					}
					
					String valueColName = regularCols.get(0).getName();

					ColumnFamily<?,?> cf = columnQuery.getCF();
					AnnotatedCompositeSerializer<?> compSerializer = (AnnotatedCompositeSerializer<?>) cf.getColumnSerializer();
					List<ComponentSerializer<?>> components = compSerializer.getComponents();

					// select the individual columns as dictated by the no of component serializers
					Builder select = QueryBuilder.select()
							.column(valueColName).ttl(valueColName).writeTime(valueColName);

					Where where = select.from(keyspace, cfDef.getName()).where(eq(partitionKeyCol, BIND_MARKER));

					for (int index = 0; index<components.size(); index++) {
						where.and(eq(clusteringKeyCols.get(index).getName(), BIND_MARKER));
					}

					return where;
				}
			};
		}

		@Override
		public BoundStatement bindValues(PreparedStatement pStatement, CqlColumnQueryImpl<?> columnQuery) {
		
			List<Object> values = new ArrayList<Object>();
			values.add(columnQuery.getRowKey());
			
			ColumnFamily<?,?> cf = columnQuery.getCF();
			AnnotatedCompositeSerializer<?> compSerializer = (AnnotatedCompositeSerializer<?>) cf.getColumnSerializer();
			List<ComponentSerializer<?>> components = compSerializer.getComponents();

			Object columnName = columnQuery.getColumnName();
			for (ComponentSerializer<?> component : components) {
				values.add(component.getFieldValueDirectly(columnName));
			}

			return pStatement.bind(values.toArray());
		}
	};
	
	
	private QueryGenCache<CqlColumnQueryImpl<?>> FlatTableColumnQuery = new QueryGenCache<CqlColumnQueryImpl<?>>(sessionRef) {

		@Override
		public Callable<RegularStatement> getQueryGen(final CqlColumnQueryImpl<?> columnQuery) {
			
			return new Callable<RegularStatement>() {

				@Override
				public RegularStatement call() throws Exception {
					
					if (clusteringKeyCols.size() != 0) {
						throw new RuntimeException("Cannot use this query for this schema, clustetingKeyCols.size: " + clusteringKeyCols.size());
					}

					String columnNameString = (String)columnQuery.getColumnName();
					return QueryBuilder.select()
							.column(columnNameString).ttl(columnNameString).writeTime(columnNameString)
							.from(keyspace, cfDef.getName())
							.where(eq(partitionKeyCol, BIND_MARKER));				}
				
			};
		}

		@Override
		public BoundStatement bindValues(PreparedStatement pStatement, CqlColumnQueryImpl<?> columnQuery) {
			return pStatement.bind(columnQuery.getRowKey());
		}
	};
	
	public BoundStatement getQueryStatement(CqlColumnQueryImpl<?> columnQuery, boolean useCaching) {

		if (isFlatTable) {
			return FlatTableColumnQuery.getBoundStatement(columnQuery, useCaching);
		}
		
		if (isCompositeColumn) {
			return ColumnQueryWithCompositeColumn.getBoundStatement(columnQuery, useCaching);
		} else {
			return ColumnQueryWithClusteringKey.getBoundStatement(columnQuery, useCaching);
		}
	}
}>>>>>>> YOURS
/home/ramdisk/experiment3/projects/astyanax/revisions/rev_2b15fd9_3de01e3/rev_2b15fd9-3de01e3/astyanax-cql/src/main/java/com/netflix/astyanax/cql/reads/CFRowQueryGen.java;<<<<<<< MINE
=======
package com.netflix.astyanax.cql.reads;

import static com.datastax.driver.core.querybuilder.QueryBuilder.desc;
import static com.datastax.driver.core.querybuilder.QueryBuilder.eq;
import static com.datastax.driver.core.querybuilder.QueryBuilder.gt;
import static com.datastax.driver.core.querybuilder.QueryBuilder.gte;
import static com.datastax.driver.core.querybuilder.QueryBuilder.in;
import static com.datastax.driver.core.querybuilder.QueryBuilder.lt;
import static com.datastax.driver.core.querybuilder.QueryBuilder.lte;

import java.util.ArrayList;
import java.util.List;
import java.util.concurrent.Callable;
import java.util.concurrent.atomic.AtomicReference;

import com.datastax.driver.core.BoundStatement;
import com.datastax.driver.core.PreparedStatement;
import com.datastax.driver.core.RegularStatement;
import com.datastax.driver.core.Session;
import com.datastax.driver.core.Statement;
import com.datastax.driver.core.querybuilder.QueryBuilder;
import com.datastax.driver.core.querybuilder.Select.Selection;
import com.datastax.driver.core.querybuilder.Select.Where;
import com.netflix.astyanax.cql.reads.model.CqlColumnSlice;
import com.netflix.astyanax.cql.schema.CqlColumnFamilyDefinitionImpl;
import com.netflix.astyanax.ddl.ColumnDefinition;
import com.netflix.astyanax.serializers.CompositeRangeBuilder.RangeQueryOp;
import com.netflix.astyanax.serializers.CompositeRangeBuilder.RangeQueryRecord;

public class CFRowQueryGen {

	private final AtomicReference<Session> sessionRef = new AtomicReference<Session>(null);
	private final String keyspace; 
	private final CqlColumnFamilyDefinitionImpl cfDef;

	private final String partitionKeyCol;
	private final String[] allPrimayKeyCols;
	private final List<ColumnDefinition> clusteringKeyCols;
	private final List<ColumnDefinition> regularCols;
	
	private boolean isCompositeColumn; 
	private boolean isFlatTable; 
	
	private static final String BIND_MARKER = "?";

	private final CFRowKeysQueryGen rowKeysQueryGen; 
	private final CFRowRangeQueryGen rowRangeQueryGen; 
	private final FlatTableRowQueryGen flatTableRowQueryGen; 
	private final FlatTableRowSliceQueryGen flatTableRowSliceQueryGen; 
	private final CFColumnQueryGen columnQueryGen; 
	
	public CFRowQueryGen(Session session, String keyspaceName, CqlColumnFamilyDefinitionImpl cfDefinition) {

		this.keyspace = keyspaceName;
		this.cfDef = cfDefinition;
		this.sessionRef.set(session);

		partitionKeyCol = cfDef.getPartitionKeyColumnDefinition().getName();
		allPrimayKeyCols = cfDef.getAllPkColNames();
		clusteringKeyCols = cfDef.getClusteringKeyColumnDefinitionList();
		regularCols = cfDef.getRegularColumnDefinitionList();

		isCompositeColumn = (clusteringKeyCols.size() > 1);
		isFlatTable = (clusteringKeyCols.size() == 0);
		
		rowKeysQueryGen = new CFRowKeysQueryGen(session, keyspaceName, cfDefinition);
		rowRangeQueryGen = new CFRowRangeQueryGen(session, keyspaceName, cfDefinition);
		flatTableRowQueryGen = new FlatTableRowQueryGen(session, keyspaceName, cfDefinition);
		flatTableRowSliceQueryGen = new FlatTableRowSliceQueryGen(session, keyspaceName, cfDefinition);
		columnQueryGen = new CFColumnQueryGen(session, keyspaceName, cfDefinition);
	}

	private QueryGenCache<CqlRowQueryImpl<?,?>> SelectEntireRow = new QueryGenCache<CqlRowQueryImpl<?,?>>(sessionRef) {

		@Override
		public Callable<RegularStatement> getQueryGen(CqlRowQueryImpl<?, ?> rowQuery) {

			return new Callable<RegularStatement>() {

				@Override
				public RegularStatement call() throws Exception {
					Selection select = QueryBuilder.select();

					for (int i=0; i<allPrimayKeyCols.length; i++) {
						select.column(allPrimayKeyCols[i]);
					}

					for (ColumnDefinition colDef : regularCols) {
						String colName = colDef.getName();
						select.column(colName).ttl(colName).writeTime(colName);
					}

					RegularStatement stmt = select.from(keyspace, cfDef.getName()).where(eq(partitionKeyCol, BIND_MARKER));
					return stmt; 
				}
			};
		}


		@Override
		public BoundStatement bindValues(PreparedStatement pStatement, CqlRowQueryImpl<?, ?> rowQuery) {
			return pStatement.bind(rowQuery.getRowKey());
		}
	};

	private QueryGenCache<CqlRowQueryImpl<?,?>> SelectColumnSliceWithClusteringKey = new QueryGenCache<CqlRowQueryImpl<?,?>>(sessionRef) {

		@Override
		public Callable<RegularStatement> getQueryGen(final CqlRowQueryImpl<?, ?> rowQuery) {

			return new Callable<RegularStatement>() {

				@Override
				public RegularStatement call() throws Exception {

					if (clusteringKeyCols.size() != 1) {
						throw new RuntimeException("Cannot perform column slice query with clusteringKeyCols.size: " + clusteringKeyCols.size());
					}

					// THIS IS A QUERY WHERE THE COLUMN NAME IS DYNAMIC  E.G TIME SERIES

					Selection select = QueryBuilder.select();

					for (int i=0; i<allPrimayKeyCols.length; i++) {
						select.column(allPrimayKeyCols[i]);
					}

					for (ColumnDefinition colDef : regularCols) {
						String colName = colDef.getName();
						select.column(colName).ttl(colName).writeTime(colName);
					}

					int numCols = rowQuery.getColumnSlice().getColumns().size(); 
					
					List<Object> colSelection = new ArrayList<Object>();
					for (int i=0; i<numCols; i++) {
						colSelection.add(BIND_MARKER);
					}
					
					return select
							.from(keyspace, cfDef.getName())
							.where(eq(partitionKeyCol, BIND_MARKER))
							.and(in(clusteringKeyCols.get(0).getName(), colSelection.toArray(new Object[colSelection.size()])));
				}
			};
		}

		@Override
		public BoundStatement bindValues(PreparedStatement pStatement, CqlRowQueryImpl<?, ?> rowQuery) {

			List<Object> objects = new ArrayList<Object>();
			objects.add(rowQuery.getRowKey());
			for (Object col : rowQuery.getColumnSlice().getColumns()) {
				objects.add(col);
			}
			return pStatement.bind(objects.toArray(new Object[objects.size()]));
		}
	};

	private QueryGenCache<CqlRowQueryImpl<?,?>> SelectColumnRangeWithClusteringKey = new QueryGenCache<CqlRowQueryImpl<?,?>>(sessionRef) {

		@Override
		public Callable<RegularStatement> getQueryGen(final CqlRowQueryImpl<?, ?> rowQuery) {
			return new Callable<RegularStatement>() {

				@Override
				public RegularStatement call() throws Exception {

					if (clusteringKeyCols.size() != 1) {
						throw new RuntimeException("Cannot perform col range query with current schema, missing pk cols");
					}

					Selection select = QueryBuilder.select();

					for (int i=0; i<allPrimayKeyCols.length; i++) {
						select.column(allPrimayKeyCols[i]);
					}

					for (ColumnDefinition colDef : regularCols) {
						String colName = colDef.getName();
						select.column(colName).ttl(colName).writeTime(colName);
					}

					Where where = select.from(keyspace, cfDef.getName())
							.where(eq(partitionKeyCol, BIND_MARKER));

					String clusterKeyCol = clusteringKeyCols.get(0).getName();

					CqlColumnSlice<?> columnSlice = rowQuery.getColumnSlice();
					if (columnSlice.getStartColumn() != null) {
						where.and(gte(clusterKeyCol, BIND_MARKER));
					}

					if (columnSlice.getEndColumn() != null) {
						where.and(lte(clusterKeyCol, BIND_MARKER));
					}

					if (columnSlice.getReversed()) {
						where.orderBy(desc(clusterKeyCol));
					}
					
					if (!rowQuery.isPaginating()) {
						// Column limits are applicable only when we are not paginating
						if (columnSlice.getLimit() != -1) {
							where.limit(columnSlice.getLimit());
						}
					}

					return where;
				}				
			};
		}

		@Override
		public BoundStatement bindValues(PreparedStatement pStatement, CqlRowQueryImpl<?, ?> rowQuery) {
			if (clusteringKeyCols.size() != 1) {
				throw new RuntimeException("Cannot perform col range query with current schema, missing pk cols");
			}

			List<Object> values = new ArrayList<Object>();
			values.add(rowQuery.getRowKey());

			CqlColumnSlice<?> columnSlice = rowQuery.getColumnSlice();

			if (columnSlice.getStartColumn() != null) {
				values.add(columnSlice.getStartColumn());
			}

			if (columnSlice.getEndColumn() != null) {
				values.add(columnSlice.getEndColumn());
			}

			return pStatement.bind(values.toArray(new Object[values.size()]));
		}
	};

	private QueryGenCache<CqlRowQueryImpl<?,?>> SelectWithCompositeColumn = new QueryGenCache<CqlRowQueryImpl<?,?>>(sessionRef) {

		@Override
		public Callable<RegularStatement> getQueryGen(final CqlRowQueryImpl<?, ?> rowQuery) {
			return new Callable<RegularStatement>() {

				@Override
				public RegularStatement call() throws Exception {

					Selection select = QueryBuilder.select();

					for (int i=0; i<allPrimayKeyCols.length; i++) {
						select.column(allPrimayKeyCols[i]);
					}

					for (ColumnDefinition colDef : regularCols) {
						String colName = colDef.getName();
						select.column(colName).ttl(colName).writeTime(colName);
					}

					Where stmt = select.from(keyspace, cfDef.getName())
							.where(eq(partitionKeyCol, BIND_MARKER));

					List<RangeQueryRecord> records = rowQuery.getCompositeRange().getRecords();

					int componentIndex = 0; 

					for (RangeQueryRecord record : records) {


						for (RangeQueryOp op : record.getOps()) {

							String columnName = clusteringKeyCols.get(componentIndex).getName();
							switch (op.getOperator()) {

							case EQUAL:
								stmt.and(eq(columnName, BIND_MARKER));
								componentIndex++;
								break;
							case LESS_THAN :
								stmt.and(lt(columnName, BIND_MARKER));
								break;
							case LESS_THAN_EQUALS:
								stmt.and(lte(columnName, BIND_MARKER));
								break;
							case GREATER_THAN:
								stmt.and(gt(columnName, BIND_MARKER));
								break;
							case GREATER_THAN_EQUALS:
								stmt.and(gte(columnName, BIND_MARKER));
								break;
							default:
								throw new RuntimeException("Cannot recognize operator: " + op.getOperator().name());
							}; // end of switch stmt
						} // end of inner for for ops for each range query record
					}
					return stmt;
				}
			};
		}

		@Override
		public BoundStatement bindValues(PreparedStatement pStatement, CqlRowQueryImpl<?, ?> rowQuery) {

			List<RangeQueryRecord> records = rowQuery.getCompositeRange().getRecords();

			List<Object> values = new ArrayList<Object>();
			values.add(rowQuery.getRowKey());

			for (RangeQueryRecord record : records) {

				for (RangeQueryOp op : record.getOps()) {

					switch (op.getOperator()) {

					case EQUAL:
						values.add(op.getValue());
						break;
					case LESS_THAN :
						values.add(op.getValue());
						break;
					case LESS_THAN_EQUALS:
						values.add(op.getValue());
						break;
					case GREATER_THAN:
						values.add(op.getValue());
						break;
					case GREATER_THAN_EQUALS:
						values.add(op.getValue());
						break;
					default:
						throw new RuntimeException("Cannot recognize operator: " + op.getOperator().name());
					}; // end of switch stmt
				} // end of inner for for ops for each range query record
			}
			return pStatement.bind(values.toArray(new Object[values.size()]));
		}
	};
	
	
	public Statement getQueryStatement(final CqlRowQueryImpl<?,?> rowQuery, boolean useCaching)  {
		
		if (isFlatTable) {
			return flatTableRowQueryGen.getQueryStatement(rowQuery, useCaching);
		}
		
		switch (rowQuery.getQueryType()) {
		
		case AllColumns:
			return SelectEntireRow.getBoundStatement(rowQuery, useCaching);
		case ColumnSlice:
			return SelectColumnSliceWithClusteringKey.getBoundStatement(rowQuery, useCaching);
		case ColumnRange:
			if (isCompositeColumn) {
				return SelectWithCompositeColumn.getBoundStatement(rowQuery, useCaching);
			} else {
				return SelectColumnRangeWithClusteringKey.getBoundStatement(rowQuery, useCaching);
			}
		default :
			throw new RuntimeException("RowQuery use case not supported. Fix this!!");
		}
	}
	
	public Statement getQueryStatement(final CqlRowSliceQueryImpl<?,?> rowSliceQuery, boolean useCaching)  {
		
		if (isFlatTable) {
			return flatTableRowSliceQueryGen.getQueryStatement(rowSliceQuery, useCaching);
		}

		switch (rowSliceQuery.getRowQueryType()) {
		
		case RowKeys:
			return rowKeysQueryGen.getQueryStatement(rowSliceQuery, useCaching);
		case RowRange:
			return rowRangeQueryGen.getQueryStatement(rowSliceQuery, useCaching);
		default :
			throw new RuntimeException("RowSliceQuery use case not supported. Fix this!!");
		}
	}

	public Statement getQueryStatement(final CqlColumnQueryImpl<?> columnQuery, boolean useCaching)  {
		
		return columnQueryGen.getQueryStatement(columnQuery, useCaching);
	}
}>>>>>>> YOURS
/home/ramdisk/experiment3/projects/astyanax/revisions/rev_2b15fd9_3de01e3/rev_2b15fd9-3de01e3/astyanax-cql/src/main/java/com/netflix/astyanax/cql/reads/CqlColumnFamilyQueryImpl.java;<<<<<<< MINE
=======
package com.netflix.astyanax.cql.reads;

import java.util.ArrayList;
import java.util.Arrays;
import java.util.Collection;
import java.util.List;

import com.netflix.astyanax.connectionpool.Host;
import com.netflix.astyanax.cql.CqlKeyspaceImpl.KeyspaceContext;
import com.netflix.astyanax.cql.reads.model.CqlRowSlice;
import com.netflix.astyanax.cql.util.CFQueryContext;
import com.netflix.astyanax.model.ColumnFamily;
import com.netflix.astyanax.model.ConsistencyLevel;
import com.netflix.astyanax.query.AllRowsQuery;
import com.netflix.astyanax.query.ColumnFamilyQuery;
import com.netflix.astyanax.query.CqlQuery;
import com.netflix.astyanax.query.IndexQuery;
import com.netflix.astyanax.query.RowQuery;
import com.netflix.astyanax.query.RowSliceQuery;
import com.netflix.astyanax.retry.RetryPolicy;

/**
 * Base impl for {@link ColumnFamilyQuery} interface. This class is the root for all read operations in Astyanax.
 * From this class, we can branch into either {@link RowQuery} or {@link RowSliceQuery}. 
 *  
 * The current class manages the column family context, retry policy and the consistency level for the read queries underneath.
 *  
 *  Important classes to see are
 *  {@link CqlRowQueryImpl}
 *  {@link CqlRowSliceQueryImpl}
 *  {@link CqlAllRowsQueryImpl}
 *  
 * @author poberai
 *
 * @param <K>
 * @param <C>
 */
public class CqlColumnFamilyQueryImpl<K, C> implements ColumnFamilyQuery<K, C> {

	private final KeyspaceContext ksContext;
	private final CFQueryContext<K,C> cfContext;
	
	private boolean useCaching = false;
	
	public CqlColumnFamilyQueryImpl(KeyspaceContext ksCtx, ColumnFamily<K,C> cf) {
		this.ksContext = ksCtx;
		this.cfContext = new CFQueryContext<K,C>(cf);
		this.cfContext.setConsistencyLevel(ConsistencyLevel.CL_ONE);
	}
	
	@Override
	public ColumnFamilyQuery<K, C> setConsistencyLevel(ConsistencyLevel clLevel) {
		this.cfContext.setConsistencyLevel(clLevel);
		return this;
	}

	@Override
	public ColumnFamilyQuery<K, C> withRetryPolicy(RetryPolicy retry) {
		this.cfContext.setRetryPolicy(retry.duplicate());
		return this;
	}

	@Override
	public ColumnFamilyQuery<K, C> pinToHost(Host host) {
		throw new UnsupportedOperationException("Operation not supported");
	}

	@Override
	public RowQuery<K, C> getKey(K rowKey) {
		return new CqlRowQueryImpl<K, C>(ksContext, cfContext, rowKey, useCaching);
	}

	@Override
	public RowQuery<K, C> getRow(K rowKey) {
		return new CqlRowQueryImpl<K, C>(ksContext, cfContext, rowKey, useCaching);
	}

	@Override
	public RowSliceQuery<K, C> getKeyRange(K startKey, K endKey, String startToken, String endToken, int count) {
		return getRowRange(startKey, endKey, startToken, endToken, count);
	}

	@Override
	public RowSliceQuery<K, C> getRowRange(K startKey, K endKey, String startToken, String endToken, int count) {
		CqlRowSlice<K> rowSlice = new CqlRowSlice<K>(startKey, endKey, startToken, endToken, count);
		return new CqlRowSliceQueryImpl<K, C>(ksContext, cfContext, rowSlice, useCaching);
	}

	@Override
	public RowSliceQuery<K, C> getKeySlice(K... keys) {
		return getRowSlice(keys);
	}

	@Override
	public RowSliceQuery<K, C> getRowSlice(K... keys) {
		List<K> keyList = Arrays.asList(keys);
		return getRowSlice(keyList);
	}

	@Override
	public RowSliceQuery<K, C> getKeySlice(Collection<K> keys) {
		return getRowSlice(keys);
	}

	@Override
	public RowSliceQuery<K, C> getRowSlice(Collection<K> keys) {
		CqlRowSlice<K> rowSlice = new CqlRowSlice<K>(keys);
		return new CqlRowSliceQueryImpl<K, C>(ksContext, cfContext, rowSlice, useCaching);
	}

	@Override
	public RowSliceQuery<K, C> getKeySlice(Iterable<K> keys) {
		return getRowSlice(keys);
	}

	@Override
	public RowSliceQuery<K, C> getRowSlice(Iterable<K> keys) {
		List<K> keyList = new ArrayList<K>();
		for (K key : keys) {
			keyList.add(key);
		}
		return getRowSlice(keyList);
	}

	@Override
	public AllRowsQuery<K, C> getAllRows() {
		return new CqlAllRowsQueryImpl<K, C>(ksContext.getKeyspaceContext(), cfContext.getColumnFamily());
	}

	@Override
	public CqlQuery<K, C> withCql(String cql) {
		return new DirectCqlQueryImpl<K, C>(ksContext, cfContext, cql);
	}

	@Override
	public IndexQuery<K, C> searchWithIndex() {
		throw new UnsupportedOperationException("Operation not supported");
	}

	@Override
	public ColumnFamilyQuery<K, C> withCaching(boolean condition) {
		this.useCaching = condition;
		return this;
	}
}>>>>>>> YOURS
/home/ramdisk/experiment3/projects/astyanax/revisions/rev_2b15fd9_3de01e3/rev_2b15fd9-3de01e3/astyanax-cql/src/main/java/com/netflix/astyanax/cql/reads/CFRowRangeQueryGen.java;<<<<<<< MINE
=======
package com.netflix.astyanax.cql.reads;

import static com.datastax.driver.core.querybuilder.QueryBuilder.gte;
import static com.datastax.driver.core.querybuilder.QueryBuilder.in;
import static com.datastax.driver.core.querybuilder.QueryBuilder.lte;

import java.math.BigInteger;
import java.util.ArrayList;
import java.util.Collection;
import java.util.List;
import java.util.concurrent.Callable;

import com.datastax.driver.core.BoundStatement;
import com.datastax.driver.core.PreparedStatement;
import com.datastax.driver.core.RegularStatement;
import com.datastax.driver.core.Session;
import com.datastax.driver.core.querybuilder.Select;
import com.datastax.driver.core.querybuilder.Select.Where;
import com.netflix.astyanax.cql.reads.model.CqlColumnSlice;
import com.netflix.astyanax.cql.reads.model.CqlRowSlice.RowRange;
import com.netflix.astyanax.cql.schema.CqlColumnFamilyDefinitionImpl;
import com.netflix.astyanax.serializers.CompositeRangeBuilder.CompositeByteBufferRange;

/**
 * This class encapsulates all the query generators for row range queries. There are different row query
 * generators depending on the specific query signature. 
 * 
 * e.g 
 * 1. Select all columns for all the rows in the row range
 * 2. Select row ranges with column slice
 * 3. Select row ranges with column range
 * 4. Select row ranges using a composite range builder for composite column based schema
 * 
 * Note that for simplicity and brevity, there is another class that handles similar operations for queries that 
 * specify a collection of row keys as opposed to a row range. 
 * See {@link CFRowKeysQueryGen} for that implementation. The current class is meant for row range queries only.  
 * 
 * Each of the query generators uses the {@link QueryGenCache} so that it can cache the {@link PreparedStatement} as well
 * for future use by queries with the same signatures.
 * 
 * But one must use this with care, since the subsequent query must have the exact signature, else binding values with 
 * the previously constructed prepared statement will break. 
 * 
 * Here is a simple example of a bad query that is not cacheable. 
 * 
 * Say that we want a simple query with a column range in it. 
 * 
 *     ks.prepareQuery(myCF)
 *       .getRow("1")
 *       .withColumnSlice("colStart")
 *       .execute();
 *       
 *     In most cases this query lends itself to a CQL3 representation as follows
 *     
 *     SELECT * FROM ks.mfCF WHERE KEY = ? AND COLUMN1 > ?;
 *     
 * Now say that we want  to perform a successive query (with caching turned ON), but add to the column range query
 *    
 *     ks.prepareQuery(myCF)
 *       .getRow("1")
 *       .withColumnSlice("colStart", "colEnd")
 *       .execute();
 *       
 *     NOTE THE USE OF BOTH colStart AND colEnd     <----- THIS IS A DIFFERENT QUERY SIGNATURE
 *     AND THE CQL QUERY WILL PROBABLY LOOK LIKE 
 *     
 *     SELECT * FROM ks.mfCF WHERE KEY = ? AND COLUMN1 > ?  AND COLUMN1 < ?;     <----- NOTE THE EXTRA BIND MARKER AT THE END FOR THE colEnd
 * 
 * If we re-use the previously cached prepared statement, then it will not work for the new query signature. The way out of this is to NOT
 * use caching with different query signatures. 
 * 
 * @author poberai
 *
 */
public class CFRowRangeQueryGen extends CFRowSliceQueryGen {

	/**
	 * Constructor
	 * 
	 * @param session
	 * @param keyspaceName
	 * @param cfDefinition
	 */
	public CFRowRangeQueryGen(Session session, String keyspaceName, CqlColumnFamilyDefinitionImpl cfDefinition) {
		super(session, keyspaceName, cfDefinition);
	}

	/**
	 * Private helper for constructing the where clause for row ranges
	 * @param keyAlias
	 * @param select
	 * @param rowRange
	 * @return
	 */
	private Where addWhereClauseForRowRange(String keyAlias, Select select, RowRange<?> rowRange) {

		Where where = null;

		boolean keyIsPresent = false;
		boolean tokenIsPresent = false; 

		if (rowRange.getStartKey() != null || rowRange.getEndKey() != null) {
			keyIsPresent = true;
		}
		if (rowRange.getStartToken() != null || rowRange.getEndToken() != null) {
			tokenIsPresent = true;
		}

		if (keyIsPresent && tokenIsPresent) {
			throw new RuntimeException("Cannot provide both token and keys for range query");
		}
		
		if (keyIsPresent) {
			if (rowRange.getStartKey() != null && rowRange.getEndKey() != null) {

				where = select.where(gte(keyAlias, BIND_MARKER))
						.and(lte(keyAlias, BIND_MARKER));

			} else if (rowRange.getStartKey() != null) {				
				where = select.where(gte(keyAlias, BIND_MARKER));

			} else if (rowRange.getEndKey() != null) {
				where = select.where(lte(keyAlias, BIND_MARKER));
			}

		} else if (tokenIsPresent) {
			String tokenOfKey ="token(" + keyAlias + ")";

			if (rowRange.getStartToken() != null && rowRange.getEndToken() != null) {

				where = select.where(gte(tokenOfKey, BIND_MARKER))
						.and(lte(tokenOfKey, BIND_MARKER));

			} else if (rowRange.getStartToken() != null) {
				where = select.where(gte(tokenOfKey, BIND_MARKER));

			} else if (rowRange.getEndToken() != null) {
				where = select.where(lte(tokenOfKey, BIND_MARKER));
			}
		} else { 
			where = select.where();
		}

		if (rowRange.getCount() > 0) {
			// TODO: fix this
			//where.limit(rowRange.getCount());
		}
		return where; 
	}

	/**
	 * Private helper for constructing the bind values for the given row range. Note that the assumption here is that 
	 * we have a previously constructed prepared statement that we can bind these values with. 
	 * 
	 * @param keyAlias
	 * @param select
	 * @param rowRange
	 * @return
	 */
	private void bindWhereClauseForRowRange(List<Object> values, RowRange<?> rowRange) {

		boolean keyIsPresent = false;
		boolean tokenIsPresent = false; 

		if (rowRange.getStartKey() != null || rowRange.getEndKey() != null) {
			keyIsPresent = true;
		}
		if (rowRange.getStartToken() != null || rowRange.getEndToken() != null) {
			tokenIsPresent = true;
		}

		if (keyIsPresent && tokenIsPresent) {
			throw new RuntimeException("Cannot provide both token and keys for range query");
		}

		if (keyIsPresent) {
			if (rowRange.getStartKey() != null) {
				values.add(rowRange.getStartKey());
			}
			if (rowRange.getEndKey() != null) {
				values.add(rowRange.getEndKey());
			}

		} else if (tokenIsPresent) {

			BigInteger startTokenB = rowRange.getStartToken() != null ? new BigInteger(rowRange.getStartToken()) : null; 
			BigInteger endTokenB = rowRange.getEndToken() != null ? new BigInteger(rowRange.getEndToken()) : null; 

			Long startToken = startTokenB.longValue();
			Long endToken = endTokenB.longValue();
			
			if (startToken != null && endToken != null) {
				if (startToken != null) {
					values.add(startToken);
				}
				if (endToken != null) {
					values.add(endToken);
				}
			}

			if (rowRange.getCount() > 0) {
				// TODO: fix this
				//where.limit(rowRange.getCount());
			}
			return; 
		}
	}

	/**
	 * Query generator for selecting all columns for the specified row range. 
	 * 
	 * Note that this object is an implementation of {@link QueryGenCache}
	 * and hence it maintains a cached reference to the previously constructed {@link PreparedStatement} for row range queries with the same 
	 * signature  (i.e all columns)
	 */
	private QueryGenCache<CqlRowSliceQueryImpl<?,?>> SelectAllColumnsForRowRange = new QueryGenCache<CqlRowSliceQueryImpl<?,?>>(sessionRef) {

		@Override
		public Callable<RegularStatement> getQueryGen(final CqlRowSliceQueryImpl<?, ?> rowSliceQuery) {
			return new Callable<RegularStatement>() {

				@Override
				public RegularStatement call() throws Exception {
					Select select = selectAllColumnsFromKeyspaceAndCF();
					return addWhereClauseForRowRange(partitionKeyCol, select, rowSliceQuery.getRowSlice().getRange());
				}
			};
		}

		@Override
		public BoundStatement bindValues(PreparedStatement pStatement, CqlRowSliceQueryImpl<?, ?> rowSliceQuery) {

			List<Object> values = new ArrayList<Object>();
			bindWhereClauseForRowRange(values, rowSliceQuery.getRowSlice().getRange());
			return pStatement.bind(values.toArray(new Object[values.size()])); 
		}
	};
	
	private QueryGenCache<CqlRowSliceQueryImpl<?,?>> SelectColumnSetForRowRange = new QueryGenCache<CqlRowSliceQueryImpl<?,?>>(sessionRef) {

		@Override
		public Callable<RegularStatement> getQueryGen(final CqlRowSliceQueryImpl<?, ?> rowSliceQuery) {
			return new Callable<RegularStatement>() {

				@Override
				public RegularStatement call() throws Exception {
					
					// THIS IS A QUERY WHERE THE COLUMN NAME IS DYNAMIC  E.G TIME SERIES
					RowRange<?> range = rowSliceQuery.getRowSlice().getRange();
					Collection<?> cols = rowSliceQuery.getColumnSlice().getColumns();
					Object[] columns = cols.toArray(new Object[cols.size()]); 

					Select select = selectAllColumnsFromKeyspaceAndCF();

					if (columns != null && columns.length > 0) {
						select.allowFiltering();
					}
					Where where = addWhereClauseForRowRange(partitionKeyCol, select, range);
					where.and(in(clusteringKeyCols.get(0).getName(), columns));

					return where;
				}
			};
		}

		@Override
		public BoundStatement bindValues(PreparedStatement pStatement, CqlRowSliceQueryImpl<?, ?> rowSliceQuery) {
			
			List<Object> values = new ArrayList<Object>();

			bindWhereClauseForRowRange(values, rowSliceQuery.getRowSlice().getRange());
			values.addAll(rowSliceQuery.getColumnSlice().getColumns());

			return pStatement.bind(values.toArray());
		}
	};

	
	/**
	 * Query generator for selecting a specified column range with a specified row range. 
	 * 
	 * Note that this object is an implementation of {@link QueryGenCache}
	 * and hence it maintains a cached reference to the previously constructed {@link PreparedStatement} for row range queries with the same 
	 * signature  (i.e similar column range for the row range)
	 */
	private QueryGenCache<CqlRowSliceQueryImpl<?,?>> SelectColumnRangeForRowRange = new QueryGenCache<CqlRowSliceQueryImpl<?,?>>(sessionRef) {

		@Override
		public Callable<RegularStatement> getQueryGen(final CqlRowSliceQueryImpl<?, ?> rowSliceQuery) {
			return new Callable<RegularStatement>() {

				@Override
				public RegularStatement call() throws Exception {

					Select select = selectAllColumnsFromKeyspaceAndCF();
					
					CqlColumnSlice<?> columnSlice = rowSliceQuery.getColumnSlice();
					
					if (columnSlice != null && columnSlice.isRangeQuery()) {
						select.allowFiltering();
					}

					Where where = addWhereClauseForRowRange(partitionKeyCol, select, rowSliceQuery.getRowSlice().getRange());			
					where = addWhereClauseForColumnRange(where, columnSlice);
					return where;
				}
			};
		}

		@Override
		public BoundStatement bindValues(PreparedStatement pStatement, CqlRowSliceQueryImpl<?, ?> rowSliceQuery) {

			List<Object> values = new ArrayList<Object>();

			bindWhereClauseForRowRange(values, rowSliceQuery.getRowSlice().getRange());
			bindWhereClauseForColumnRange(values, rowSliceQuery.getColumnSlice());
			
			return pStatement.bind(values.toArray());
		}
	};
	
	/**
	 * Query generator for selecting a specified composite column range with a specified row range. 
	 * 
	 * Note that this object is an implementation of {@link QueryGenCache}
	 * and hence it maintains a cached reference to the previously constructed {@link PreparedStatement} for row range queries with the same 
	 * signature  (i.e similar composite column range for the row range)
	 */
	private QueryGenCache<CqlRowSliceQueryImpl<?,?>> SelectCompositeColumnRangeForRowRange = new QueryGenCache<CqlRowSliceQueryImpl<?,?>>(sessionRef) {

		@Override
		public Callable<RegularStatement> getQueryGen(final CqlRowSliceQueryImpl<?, ?> rowSliceQuery) {
			return new Callable<RegularStatement>() {

				@Override
				public RegularStatement call() throws Exception {
					Select select = selectAllColumnsFromKeyspaceAndCF();
					CompositeByteBufferRange compositeRange = rowSliceQuery.getCompositeRange();
					if (compositeRange != null) {
						select.allowFiltering();
					}

					Where where = addWhereClauseForRowRange(partitionKeyCol, select, rowSliceQuery.getRowSlice().getRange());	
					where = addWhereClauseForCompositeColumnRange(where, compositeRange);
					return where;
				}
				
			};
		}

		@Override
		public BoundStatement bindValues(PreparedStatement pStatement, CqlRowSliceQueryImpl<?, ?> rowSliceQuery) {

			List<Object> values = new ArrayList<Object>();

			bindWhereClauseForRowRange(values, rowSliceQuery.getRowSlice().getRange());
			bindWhereClauseForCompositeColumnRange(values, rowSliceQuery.getCompositeRange());

			return pStatement.bind(values.toArray());
		}
	};
	
	/**
	 * Main method used to generate the query for the specified row slice query. 
	 * Note that depending on the query signature, the caller may choose to enable/disable caching
	 * 
	 * @param rowSliceQuery: The Astaynax query for which we need to generate a java driver query
	 * @param useCaching: boolean condition indicating whether we should use a previously cached prepared stmt or not. 
	 *                    If false, then the cache is ignored and we generate the prepared stmt for this query
	 *                    If true, then the cached prepared stmt is used. If the cache has not been inited, 
	 *                    then the prepared stmt is constructed for this query and subsequently cached
	 *                    
	 * @return BoundStatement: they statement for this Astyanax query
	 */
	public BoundStatement getQueryStatement(CqlRowSliceQueryImpl<?,?> rowSliceQuery, boolean useCaching) {

		switch (rowSliceQuery.getColQueryType()) {

		case AllColumns:
			return SelectAllColumnsForRowRange.getBoundStatement(rowSliceQuery, useCaching);
		case ColumnSet: 
			return SelectColumnSetForRowRange.getBoundStatement(rowSliceQuery, useCaching);
		case ColumnRange:
			if (isCompositeColumn) {
				return SelectCompositeColumnRangeForRowRange.getBoundStatement(rowSliceQuery, useCaching);
			} else {
				return SelectColumnRangeForRowRange.getBoundStatement(rowSliceQuery, useCaching);
			}
		default :
			throw new RuntimeException("RowSliceQuery with row range use case not supported.");
		}
	}
}>>>>>>> YOURS
/home/ramdisk/experiment3/projects/astyanax/revisions/rev_2b15fd9_3de01e3/rev_2b15fd9-3de01e3/astyanax-cql/src/main/java/com/netflix/astyanax/cql/reads/CFRowKeysQueryGen.java;<<<<<<< MINE
=======
package com.netflix.astyanax.cql.reads;

import static com.datastax.driver.core.querybuilder.QueryBuilder.in;

import java.util.ArrayList;
import java.util.Collection;
import java.util.List;
import java.util.concurrent.Callable;

import com.datastax.driver.core.BoundStatement;
import com.datastax.driver.core.PreparedStatement;
import com.datastax.driver.core.RegularStatement;
import com.datastax.driver.core.Session;
import com.datastax.driver.core.querybuilder.Select;
import com.datastax.driver.core.querybuilder.Select.Where;
import com.netflix.astyanax.cql.schema.CqlColumnFamilyDefinitionImpl;

/**
 * This class encapsulates all the query generators for row slice queries that use a collection of row keys. 
 * There are different row query generators depending on the specific query signature. 
 * 
 * e.g 
 * 1. Select all columns for all the rows in the row range
 * 2. Select rows with column slice
 * 3. Select rows with column range
 * 4. Select rows using a composite range builder for composite column based schema
 * 
 * Note that for simplicity and brevity, there is another class that handles similar operations for queries that 
 * specify a row range as opposed to a collection of row keys (as is done here). 
 * See {@link CFRowRangeQueryGen} for that implementation. The current class is meant for row slice queries using only row key collections.  
 * 
 * Each of the query generators uses the {@link QueryGenCache} so that it can cache the {@link PreparedStatement} as well
 * for future use by queries with the same signatures.
 * 
 * But one must use this with care, since the subsequent query must have the exact signature, else binding values with 
 * the previously constructed prepared statement will break. 
 * 
 * Here is a simple example of a bad query that is not cacheable. 
 * 
 * Say that we want a simple query with a column range in it. 
 * 
 *     ks.prepareQuery(myCF)
 *       .getRow("1")
 *       .withColumnSlice("colStart")
 *       .execute();
 *       
 *     In most cases this query lends itself to a CQL3 representation as follows
 *     
 *     SELECT * FROM ks.mfCF WHERE KEY = ? AND COLUMN1 > ?;
 *     
 * Now say that we want  to perform a successive query (with caching turned ON), but add to the column range query
 *    
 *     ks.prepareQuery(myCF)
 *       .getRow("1")
 *       .withColumnSlice("colStart", "colEnd")
 *       .execute();
 *       
 *     NOTE THE USE OF BOTH colStart AND colEnd     <----- THIS IS A DIFFERENT QUERY SIGNATURE
 *     AND THE CQL QUERY WILL PROBABLY LOOK LIKE 
 *     
 *     SELECT * FROM ks.mfCF WHERE KEY = ? AND COLUMN1 > ?  AND COLUMN1 < ?;     <----- NOTE THE EXTRA BIND MARKER AT THE END FOR THE colEnd
 * 
 * If we re-use the previously cached prepared statement, then it will not work for the new query signature. The way out of this is to NOT
 * use caching with different query signatures. 
 * 
 * @author poberai
 *
 */
public class CFRowKeysQueryGen extends CFRowSliceQueryGen {

	public CFRowKeysQueryGen(Session session, String keyspaceName, CqlColumnFamilyDefinitionImpl cfDefinition) {
		super(session, keyspaceName, cfDefinition);
	}

	/**
	 * Query generator for selecting all columns for the specified row keys. 
	 * 
	 * Note that this object is an implementation of {@link QueryGenCache}
	 * and hence it maintains a cached reference to the previously constructed {@link PreparedStatement} for row range queries with the same 
	 * signature  (i.e all columns for a similar set of row keys)
	 */
	private QueryGenCache<CqlRowSliceQueryImpl<?,?>> SelectAllColumnsForRowKeys = new QueryGenCache<CqlRowSliceQueryImpl<?,?>>(sessionRef) {

		@Override
		public Callable<RegularStatement> getQueryGen(final CqlRowSliceQueryImpl<?, ?> rowSliceQuery) {
			return new Callable<RegularStatement>() {

				@Override
				public RegularStatement call() throws Exception {
					
					Select select = selectAllColumnsFromKeyspaceAndCF();
					return select.where(in(partitionKeyCol, bindMarkerArray(rowSliceQuery.getRowSlice().getKeys().size())));
				}
			};
		}

		@Override
		public BoundStatement bindValues(PreparedStatement pStatement, CqlRowSliceQueryImpl<?, ?> rowSliceQuery) {
			return pStatement.bind(rowSliceQuery.getRowSlice().getKeys().toArray());
		}
	};
	
	/**
	 * Query generator for selecting a column set for the specified row keys. 
	 * 
	 * Note that this object is an implementation of {@link QueryGenCache}
	 * and hence it maintains a cached reference to the previously constructed {@link PreparedStatement} for row range queries with the same 
	 * signature  (i.e a similar set of columns for a similar set of rows )
	 */
	private QueryGenCache<CqlRowSliceQueryImpl<?,?>> SelectColumnSetForRowKeys = new QueryGenCache<CqlRowSliceQueryImpl<?,?>>(sessionRef) {
	
		@Override
		public Callable<RegularStatement> getQueryGen(final CqlRowSliceQueryImpl<?, ?> rowSliceQuery) {
			return new Callable<RegularStatement>() {

				@Override
				public RegularStatement call() throws Exception {

					if (clusteringKeyCols.size() != 1) {
						throw new RuntimeException("Cannot perform row slice with col slice query for this schema, clusteringKeyCols.size(): " 
								+ clusteringKeyCols.size());
					}
						
					Collection<?> rowKeys = rowSliceQuery.getRowSlice().getKeys();
					Collection<?> cols = rowSliceQuery.getColumnSlice().getColumns();

					// THIS IS A QUERY WHERE THE COLUMN NAME IS DYNAMIC  E.G TIME SERIES
					Object[] columns = cols.toArray(new Object[cols.size()]); 

					String clusteringCol = clusteringKeyCols.get(0).getName();

					Select select = selectAllColumnsFromKeyspaceAndCF();
					return select.where(in(partitionKeyCol, bindMarkerArray(rowKeys.size())))
							.and(in(clusteringCol, bindMarkerArray(columns.length)));
				}
			};
		}

		@Override
		public BoundStatement bindValues(PreparedStatement pStatement, CqlRowSliceQueryImpl<?, ?> rowSliceQuery) {

			if (clusteringKeyCols.size() != 1) {
				throw new RuntimeException("Cannot perform row slice with col slice query for this schema, clusteringKeyCols.size(): " 
						+ clusteringKeyCols.size());
			}
			
			List<Object> values = new ArrayList<Object>();
			values.addAll(rowSliceQuery.getRowSlice().getKeys());
			values.addAll(rowSliceQuery.getColumnSlice().getColumns());

			return pStatement.bind(values.toArray());		
		}
	};
	
	/**
	 * Query generator for selecting a column range for the specified row keys. 
	 * 
	 * Note that this object is an implementation of {@link QueryGenCache}
	 * and hence it maintains a cached reference to the previously constructed {@link PreparedStatement} for row range queries with the same 
	 * signature  (i.e a similar column range for a similar set of rows)
	 */
	private QueryGenCache<CqlRowSliceQueryImpl<?,?>> SelectColumnRangeForRowKeys = new QueryGenCache<CqlRowSliceQueryImpl<?,?>>(sessionRef) {

		@Override
		public Callable<RegularStatement> getQueryGen(final CqlRowSliceQueryImpl<?, ?> rowSliceQuery) {
			return new Callable<RegularStatement>() {

				@Override
				public RegularStatement call() throws Exception {

					if (clusteringKeyCols.size() != 1) {
						throw new RuntimeException("Cannot perform row slice with col slice query for this schema, clusteringKeyCols.size(): " 
								+ clusteringKeyCols.size());
					}
						
					Select select = selectAllColumnsFromKeyspaceAndCF();
					Where where = select.where(in(partitionKeyCol, bindMarkerArray(rowSliceQuery.getRowSlice().getKeys().size())));
					where = addWhereClauseForColumnRange(where, rowSliceQuery.getColumnSlice());
					return where;
				}
			};
		}

		@Override
		public BoundStatement bindValues(PreparedStatement pStatement, CqlRowSliceQueryImpl<?, ?> rowSliceQuery) {

			if (clusteringKeyCols.size() != 1) {
				throw new RuntimeException("Cannot perform row slice with col slice query for this schema, clusteringKeyCols.size(): " 
						+ clusteringKeyCols.size());
			}
			
			List<Object> values = new ArrayList<Object>();

			values.addAll(rowSliceQuery.getRowSlice().getKeys());
			bindWhereClauseForColumnRange(values, rowSliceQuery.getColumnSlice());

			return pStatement.bind(values.toArray());
		}
	};
	
	
	/**
	 * Query generator for selecting a composite column range for the specified row keys. 
	 * 
	 * Note that this object is an implementation of {@link QueryGenCache}
	 * and hence it maintains a cached reference to the previously constructed {@link PreparedStatement} for row range queries with the same 
	 * signature  (i.e a similar composite column range for a similar set of rows)
	 */
	private QueryGenCache<CqlRowSliceQueryImpl<?,?>> SelectCompositeColumnRangeForRowKeys = new QueryGenCache<CqlRowSliceQueryImpl<?,?>>(sessionRef) {

		@Override
		public Callable<RegularStatement> getQueryGen(final CqlRowSliceQueryImpl<?, ?> rowSliceQuery) {
			return new Callable<RegularStatement>() {

				@Override
				public RegularStatement call() throws Exception {
					Select select = selectAllColumnsFromKeyspaceAndCF();
					Where stmt = select.where(in(partitionKeyCol, bindMarkerArray(rowSliceQuery.getRowSlice().getKeys().size())));
					stmt = addWhereClauseForCompositeColumnRange(stmt, rowSliceQuery.getCompositeRange());
					return stmt;
				}
			};
		}

		@Override
		public BoundStatement bindValues(PreparedStatement pStatement, CqlRowSliceQueryImpl<?, ?> rowSliceQuery) {

			List<Object> values = new ArrayList<Object>();

			values.addAll(rowSliceQuery.getRowSlice().getKeys());
			bindWhereClauseForCompositeColumnRange(values, rowSliceQuery.getCompositeRange());

			return pStatement.bind(values.toArray());
		}
	};
	
	/**
	 * Main method that is used to generate the java driver statement from the given Astyanax row slice query. 
	 * Note that the method allows the caller to specify whether to use caching or not. 
	 * 
	 * If caching is disabled, then the PreparedStatement is generated every time
	 * If caching is enabled, then the cached PreparedStatement is used for the given Astyanax RowSliceQuery. 
	 * In this case if the PreparedStatement is missing, then it is constructed from the Astyanax query and 
	 * used to init the cached reference and hence can be used by other subsequent Astayanx RowSliceQuery
	 * operations with the same signature (that opt in for  caching)
	 * 
	 * @param rowSliceQuery
	 * @param useCaching
	 * @return
	 */
	public BoundStatement getQueryStatement(CqlRowSliceQueryImpl<?,?> rowSliceQuery, boolean useCaching) {

		switch (rowSliceQuery.getColQueryType()) {

		case AllColumns:
			return SelectAllColumnsForRowKeys.getBoundStatement(rowSliceQuery, useCaching);
		case ColumnSet: 
			return SelectColumnSetForRowKeys.getBoundStatement(rowSliceQuery, useCaching);
		case ColumnRange:
			if (isCompositeColumn) {
				return SelectCompositeColumnRangeForRowKeys.getBoundStatement(rowSliceQuery, useCaching);
			} else {
				return SelectColumnRangeForRowKeys.getBoundStatement(rowSliceQuery, useCaching);
			}
		default :
			throw new RuntimeException("RowSliceQuery with row keys use case not supported.");
		}
	}
}>>>>>>> YOURS
/home/ramdisk/experiment3/projects/astyanax/revisions/rev_2b15fd9_3de01e3/rev_2b15fd9-3de01e3/astyanax-cql/src/main/java/com/netflix/astyanax/cql/reads/CqlRowSliceColumnCountQueryImpl.java;<<<<<<< MINE
=======
package com.netflix.astyanax.cql.reads;

import java.util.HashMap;
import java.util.Map;

import com.datastax.driver.core.ResultSet;
import com.datastax.driver.core.Row;
import com.datastax.driver.core.Statement;
import com.google.common.util.concurrent.ListenableFuture;
import com.netflix.astyanax.CassandraOperationType;
import com.netflix.astyanax.connectionpool.OperationResult;
import com.netflix.astyanax.connectionpool.exceptions.ConnectionException;
import com.netflix.astyanax.cql.CqlAbstractExecutionImpl;
import com.netflix.astyanax.cql.CqlKeyspaceImpl.KeyspaceContext;
import com.netflix.astyanax.cql.util.CFQueryContext;
import com.netflix.astyanax.cql.util.CqlTypeMapping;
import com.netflix.astyanax.query.ColumnCountQuery;
import com.netflix.astyanax.query.RowSliceColumnCountQuery;

/**
 * Impl for {@link RowSliceColumnCountQuery} interface. 
 * Just like {@link ColumnCountQuery}, this class only manages the context for the query. 
 * The actual query statement is supplied from the {@link CqlRowSliceQueryImpl} class.
 * 
 * Note that CQL3 treats columns as rows for certain schemas that contain clustering keys. 
 * Hence this class collapses all {@link ResultSet} rows with the same partition key into a single row
 * when counting all unique rows. 
 *  
 * @author poberai
 *
 * @param <K>
 */
public class CqlRowSliceColumnCountQueryImpl<K> implements RowSliceColumnCountQuery<K> {

	private final KeyspaceContext ksContext;
	private final CFQueryContext<?,?> cfContext;
	private final Statement query;
	
	public CqlRowSliceColumnCountQueryImpl(KeyspaceContext ksCtx, CFQueryContext<?,?> cfCtx, Statement query) {
		this.ksContext = ksCtx;
		this.cfContext = cfCtx;
		this.query = query;
		
	}

	@Override
	public OperationResult<Map<K, Integer>> execute() throws ConnectionException {
		return new InternalQueryExecutionImpl().execute();
	}

	@Override
	public ListenableFuture<OperationResult<Map<K, Integer>>> executeAsync() throws ConnectionException {
		return new InternalQueryExecutionImpl().executeAsync();
	}
	
	private class InternalQueryExecutionImpl extends CqlAbstractExecutionImpl<Map<K, Integer>> {

		public InternalQueryExecutionImpl() {
			super(ksContext, cfContext);
		}

		@Override
		public CassandraOperationType getOperationType() {
			return CassandraOperationType.GET_ROWS_SLICE;
		}

		@Override
		public Statement getQuery() {
			return query;
		}

		@SuppressWarnings("unchecked")
		@Override
		public Map<K, Integer> parseResultSet(ResultSet resultSet) {
			
			Map<K, Integer> columnCountPerRow = new HashMap<K, Integer>();
			
			for (Row row : resultSet.all()) {
				K key = (K) CqlTypeMapping.getDynamicColumn(row, cf.getKeySerializer(), 0, cf);
				Integer colCount = columnCountPerRow.get(key);
				if (colCount == null) {
					colCount = new Integer(0);
				}	
				colCount = colCount.intValue() + 1;
				columnCountPerRow.put(key, colCount);
			}
			
			return columnCountPerRow;
		}
	}
}>>>>>>> YOURS
/home/ramdisk/experiment3/projects/astyanax/revisions/rev_2b15fd9_3de01e3/rev_2b15fd9-3de01e3/astyanax-cql/src/main/java/com/netflix/astyanax/cql/reads/DirectCqlQueryImpl.java;<<<<<<< MINE
=======
package com.netflix.astyanax.cql.reads;

import java.nio.ByteBuffer;
import java.util.ArrayList;
import java.util.List;
import java.util.UUID;

import com.datastax.driver.core.BoundStatement;
import com.datastax.driver.core.ResultSet;
import com.datastax.driver.core.SimpleStatement;
import com.datastax.driver.core.Statement;
import com.google.common.util.concurrent.ListenableFuture;
import com.netflix.astyanax.CassandraOperationType;
import com.netflix.astyanax.Serializer;
import com.netflix.astyanax.connectionpool.OperationResult;
import com.netflix.astyanax.connectionpool.exceptions.ConnectionException;
import com.netflix.astyanax.cql.CqlAbstractExecutionImpl;
import com.netflix.astyanax.cql.CqlKeyspaceImpl.KeyspaceContext;
import com.netflix.astyanax.cql.reads.model.DirectCqlResult;
import com.netflix.astyanax.cql.util.CFQueryContext;
import com.netflix.astyanax.model.ColumnFamily;
import com.netflix.astyanax.model.CqlResult;
import com.netflix.astyanax.query.CqlQuery;
import com.netflix.astyanax.query.PreparedCqlQuery;

/**
 * 
 * Impl for {@link CqlQuery} that allows users to directly send CQL3 over java driver
 * @author poberai
 *
 * @param <K>
 * @param <C>
 */
public class DirectCqlQueryImpl<K, C> implements CqlQuery<K, C> {

	private final KeyspaceContext ksContext;
	private final CFQueryContext<K,C> cfContext;
	private final String basicCqlQuery; 
	
	public DirectCqlQueryImpl(KeyspaceContext ksCtx, CFQueryContext<K,C> cfCtx, String basicCqlQuery) {
		this.ksContext = ksCtx;
		this.cfContext = cfCtx;
		this.basicCqlQuery = basicCqlQuery;
	}
	
	@Override
	public OperationResult<CqlResult<K, C>> execute() throws ConnectionException {
		return new InternalExecutionImpl(new SimpleStatement(basicCqlQuery)).execute();
	}

	@Override
	public ListenableFuture<OperationResult<CqlResult<K, C>>> executeAsync() throws ConnectionException {
		return new InternalExecutionImpl(new SimpleStatement(basicCqlQuery)).executeAsync();
	}
	
	@Override
	public CqlQuery<K, C> useCompression() {
		throw new UnsupportedOperationException("Operation not supported");
	}

	@Override
	public PreparedCqlQuery<K, C> asPreparedStatement() {
		
		final BoundStatement boundStatement = new BoundStatement(ksContext.getSession().prepare(basicCqlQuery));
		final List<Object> bindList = new ArrayList<Object>();
		
		return new PreparedCqlQuery<K, C>() {

			@Override
			public OperationResult<CqlResult<K, C>> execute() throws ConnectionException {
				boundStatement.bind(bindList.toArray());
				return new InternalExecutionImpl(boundStatement).execute();
			}

			@Override
			public ListenableFuture<OperationResult<CqlResult<K, C>>> executeAsync() throws ConnectionException {
				
				boundStatement.bind(bindList.toArray());
				return new InternalExecutionImpl(boundStatement).executeAsync();
			}

			@Override
			public <V> PreparedCqlQuery<K, C> withByteBufferValue(V value, Serializer<V> serializer) {
				bindList.add(value);
				return this;
			}

			@Override
			public PreparedCqlQuery<K, C> withValue(ByteBuffer value) {
				bindList.add(value);
				return this;
			}

			@Override
			public PreparedCqlQuery<K, C> withValues(List<ByteBuffer> value) {
				bindList.addAll(value);
				return this;
			}

			@Override
			public PreparedCqlQuery<K, C> withStringValue(String value) {
				bindList.add(value);
				return this;
			}

			@Override
			public PreparedCqlQuery<K, C> withIntegerValue(Integer value) {
				bindList.add(value);
				return this;
			}

			@Override
			public PreparedCqlQuery<K, C> withBooleanValue(Boolean value) {
				bindList.add(value);
				return this;
			}

			@Override
			public PreparedCqlQuery<K, C> withDoubleValue(Double value) {
				bindList.add(value);
				return this;
			}

			@Override
			public PreparedCqlQuery<K, C> withLongValue(Long value) {
				bindList.add(value);
				return this;
			}

			@Override
			public PreparedCqlQuery<K, C> withFloatValue(Float value) {
				bindList.add(value);
				return this;
			}

			@Override
			public PreparedCqlQuery<K, C> withShortValue(Short value) {
				bindList.add(value);
				return this;
			}

			@Override
			public PreparedCqlQuery<K, C> withUUIDValue(UUID value) {
				bindList.add(value);
				return this;
			}
		};
	}
	
	private class InternalExecutionImpl extends CqlAbstractExecutionImpl<CqlResult<K, C>> {

		private final Statement query;
		
		public InternalExecutionImpl(Statement query) {
			super(ksContext, cfContext);
			this.query = query;
		}

		@Override
		public CassandraOperationType getOperationType() {
			return CassandraOperationType.CQL;
		}

		@Override
		public Statement getQuery() {
			return query;
		}

		@Override
		public CqlResult<K, C> parseResultSet(ResultSet resultSet) {
			
			boolean isCountQuery = basicCqlQuery.contains(" count(");
			if (isCountQuery) {
				return new DirectCqlResult<K,C>(new Long(resultSet.one().getLong(0)));
			} else {
				return new DirectCqlResult<K,C>(resultSet.all(), (ColumnFamily<K, C>) cf);
			}
		}
	}
}>>>>>>> YOURS
/home/ramdisk/experiment3/projects/astyanax/revisions/rev_2b15fd9_3de01e3/rev_2b15fd9-3de01e3/astyanax-cql/src/main/java/com/netflix/astyanax/cql/reads/CqlColumnQueryImpl.java;<<<<<<< MINE
=======
package com.netflix.astyanax.cql.reads;

import com.datastax.driver.core.ResultSet;
import com.datastax.driver.core.Row;
import com.datastax.driver.core.Statement;
import com.google.common.util.concurrent.ListenableFuture;
import com.netflix.astyanax.CassandraOperationType;
import com.netflix.astyanax.connectionpool.OperationResult;
import com.netflix.astyanax.connectionpool.exceptions.ConnectionException;
import com.netflix.astyanax.connectionpool.exceptions.NotFoundException;
import com.netflix.astyanax.cql.CqlAbstractExecutionImpl;
import com.netflix.astyanax.cql.CqlKeyspaceImpl.KeyspaceContext;
import com.netflix.astyanax.cql.reads.model.CqlColumnImpl;
import com.netflix.astyanax.cql.schema.CqlColumnFamilyDefinitionImpl;
import com.netflix.astyanax.cql.util.CFQueryContext;
import com.netflix.astyanax.model.Column;
import com.netflix.astyanax.model.ColumnFamily;
import com.netflix.astyanax.query.ColumnQuery;

/**
 * Impl for the {@link ColumnQuery} interface using the java driver. 
 * This class is responsible for selecting a single column for the specified row key. 
 * 
 * Note that this class acts like a placeholder for all the query context, but does not construct the query itself. 
 * For details on how the query is actually constructed see {@link CFColumnQueryGen}
 * 
 * @author poberai
 *
 * @param <C>
 */
public class CqlColumnQueryImpl<C> implements ColumnQuery<C> {

	private final KeyspaceContext ksContext;
	private final CFQueryContext<?,C> cfContext;
	private final Object rowKey;
	private final C columnName;

	private boolean useCaching = false; 
	
	private final CqlColumnFamilyDefinitionImpl cfDef;

	CqlColumnQueryImpl(KeyspaceContext ksCtx, CFQueryContext<?,C> cfCtx, Object rowKey, C colName, boolean caching) {
		this.ksContext = ksCtx;
		this.cfContext = cfCtx;
		this.rowKey = rowKey;
		this.columnName = colName;
		this.useCaching = caching;
		
		ColumnFamily<?,?> cf = cfCtx.getColumnFamily();
		cfDef = (CqlColumnFamilyDefinitionImpl) cf.getColumnFamilyDefinition();
	}

	@Override
	public OperationResult<Column<C>> execute() throws ConnectionException {
		return new InternalColumnQueryExecutionImpl(this).execute();
	}

	@Override
	public ListenableFuture<OperationResult<Column<C>>> executeAsync() throws ConnectionException {
		return new InternalColumnQueryExecutionImpl(this).executeAsync();
	}

	private class InternalColumnQueryExecutionImpl extends CqlAbstractExecutionImpl<Column<C>> {

		private final CqlColumnQueryImpl<?> columnQuery; 
		
		public InternalColumnQueryExecutionImpl(CqlColumnQueryImpl<?> query) {
			super(ksContext, cfContext);
			this.columnQuery = query;
		}

		@Override
		public CassandraOperationType getOperationType() {
			return CassandraOperationType.GET_COLUMN;
		}

		@Override
		public Statement getQuery() {
			return cfDef.getRowQueryGenerator().getQueryStatement(columnQuery, useCaching);
		}

		@Override
		public Column<C> parseResultSet(ResultSet rs) throws NotFoundException {

			Row row = rs.one();
			if (row == null) {
				return null;
			}

			CqlColumnImpl<C> cqlCol = new CqlColumnImpl<C>((C) columnName, row, 0);
			return cqlCol;
		}
	}

	public Object getRowKey() {
		return rowKey;
	}
	
	public C getColumnName() {
		return columnName;
	}
	
	public ColumnFamily<?,C> getCF() {
		return this.cfContext.getColumnFamily();
	}
}>>>>>>> YOURS
/home/ramdisk/experiment3/projects/astyanax/revisions/rev_2b15fd9_3de01e3/rev_2b15fd9-3de01e3/astyanax-cql/src/main/java/com/netflix/astyanax/cql/reads/QueryGenCache.java;<<<<<<< MINE
=======
package com.netflix.astyanax.cql.reads;

import java.util.concurrent.Callable;
import java.util.concurrent.atomic.AtomicReference;

import org.apache.log4j.Logger;

import com.datastax.driver.core.BoundStatement;
import com.datastax.driver.core.PreparedStatement;
import com.datastax.driver.core.RegularStatement;
import com.datastax.driver.core.Session;

/**
 * Template for {@link PreparedStatement} caching for a query Q. 
 * The class provides the basic functionality to store a cached reference to the PreparedStatement 
 * that is generated by the extending class. Hence actual logic for constructing the PrepatedStatement
 * and binding values to that statement is not defined here. That must be provided by the extending classes.  
 * 
 * @author poberai
 *
 * @param <Q>
 */
public abstract class QueryGenCache<Q> {

	private static final Logger LOG = Logger.getLogger(QueryGenCache.class);

	// reference to the session object. This is required for "preparing" a statement
	private AtomicReference<Session> sessionRef = new AtomicReference<Session>(null); 
	// The cached reference to the query constructed by extending classes
	private final AtomicReference<PreparedStatement> cachedStatement = new AtomicReference<PreparedStatement>(null);

	/**
	 * Constructor
	 * @param sessionR
	 */
	public QueryGenCache(AtomicReference<Session> sessionR) {
		this.sessionRef = sessionR;
	}

	/**
	 * Get the bound statement from the prepared statement
	 * @param query
	 * @param useCaching
	 * @return BoundStatement
	 */
	public BoundStatement getBoundStatement(Q query, boolean useCaching) {

		PreparedStatement pStatement = getPreparedStatement(query, useCaching);
		return bindValues(pStatement, query);
	}

	/**
	 * Get the bound statemnent by either constructing the query or using the cached statement underneath.
	 * Note that the caller can provide useCaching as a knob to turn caching ON/OFF. 
	 * If false, then the query is just constructed using the extending class and returned. 
	 * If true, then the cached reference is consulted. If the cache is empty, then the query is constructed
	 * and used to seed the cache. 
	 * 
	 * @param query
	 * @param useCaching
	 * @return PreparedStatement
	 */
	public PreparedStatement getPreparedStatement(Q query, boolean useCaching) {

		PreparedStatement pStatement = null;

		if (useCaching) {
			pStatement = cachedStatement.get();
		}

		if (pStatement == null) {
			try {
				RegularStatement stmt = getQueryGen(query).call();
				if (LOG.isDebugEnabled()) {
					LOG.debug("Query: " + stmt.getQueryString());
				}
				pStatement = sessionRef.get().prepare(stmt.getQueryString());
			} catch (Exception e) {
				throw new RuntimeException(e);
			}
		}

		if (useCaching && cachedStatement.get() == null) {
			cachedStatement.set(pStatement);
		}
		return pStatement;
	}
	
	/**
	 * Extending classes must implement this with logic for constructing the java driver query from the given Astyanax query
	 * @param query
	 * @return Callable<RegularStatement>
	 */
	public abstract Callable<RegularStatement> getQueryGen(Q query);

	/**
	 * Extending classes must implement this with logic for binding the right Astyanax query data with the pre-constructed
	 * prepared statement in the right order.
	 * @param pStatement
	 * @param query
	 * @return BoundStatement
	 */ 
	public abstract BoundStatement bindValues(PreparedStatement pStatement, Q query);
}>>>>>>> YOURS
/home/ramdisk/experiment3/projects/astyanax/revisions/rev_2b15fd9_3de01e3/rev_2b15fd9-3de01e3/astyanax-cql/src/main/java/com/netflix/astyanax/cql/reads/CqlRowCopier.java;<<<<<<< MINE
=======
package com.netflix.astyanax.cql.reads;

import java.util.Iterator;

import com.google.common.util.concurrent.ListenableFuture;
import com.netflix.astyanax.MutationBatch;
import com.netflix.astyanax.RowCopier;
import com.netflix.astyanax.connectionpool.OperationResult;
import com.netflix.astyanax.connectionpool.exceptions.ConnectionException;
import com.netflix.astyanax.cql.CqlKeyspaceImpl;
import com.netflix.astyanax.cql.CqlKeyspaceImpl.KeyspaceContext;
import com.netflix.astyanax.cql.reads.model.CqlColumnImpl;
import com.netflix.astyanax.cql.writes.CqlColumnListMutationImpl;
import com.netflix.astyanax.model.Column;
import com.netflix.astyanax.model.ColumnFamily;
import com.netflix.astyanax.model.ColumnList;
import com.netflix.astyanax.query.RowQuery;

/**
 * Impl for {@link RowCopier}
 * 
 * @author poberai
 *
 * @param <K>
 * @param <C>
 */
public class CqlRowCopier<K,C> implements RowCopier<K,C> {

	private boolean useOriginalTimestamp = false;
	
	private final RowQuery<K,C> rowQuery;
	private final ColumnFamily<K,C> cf;
	private final K rowKey; 
	private final KeyspaceContext ksContext;
	
	public CqlRowCopier(ColumnFamily<K,C> cf, K rowKey, RowQuery<K,C> query, KeyspaceContext ksContext) {
		this.cf = cf;
		this.rowKey = rowKey;
		this.rowQuery = query;
		this.ksContext = ksContext;
	}
	
	@Override
	public OperationResult<Void> execute() throws ConnectionException {
		return getMutationBatch().execute();
	}

	@Override
	public ListenableFuture<OperationResult<Void>> executeAsync() throws ConnectionException {
		return getMutationBatch().executeAsync();
	}

	@Override
	public RowCopier<K, C> withOriginalTimestamp(boolean useTimestamp) {
		this.useOriginalTimestamp = useTimestamp;
		return this;
	}
	
	private MutationBatch getMutationBatch() throws ConnectionException {
		
		ColumnList<C> columnList = rowQuery.execute().getResult();
		
		CqlKeyspaceImpl ksImpl = new CqlKeyspaceImpl(ksContext);
		
		MutationBatch mBatch = ksImpl.prepareMutationBatch();
		CqlColumnListMutationImpl<K,C> colListMutation = (CqlColumnListMutationImpl<K, C>)mBatch.withRow(cf, rowKey);
		
		Iterator<Column<C>> iter = columnList.iterator();

		boolean first = true;
		
		while(iter.hasNext()) {
			
			CqlColumnImpl<C> col = (CqlColumnImpl<C>) iter.next();
			
			if (first && useOriginalTimestamp) {
				colListMutation.setTimestamp(col.getTimestamp());
				first = false;
			}
			
			colListMutation.putColumnWithGenericValue(col.getName(), col.getGenericValue(), null);
		}
		
		return mBatch;
	}
}>>>>>>> YOURS
/home/ramdisk/experiment3/projects/astyanax/revisions/rev_2b15fd9_3de01e3/rev_2b15fd9-3de01e3/astyanax-cql/src/main/java/com/netflix/astyanax/cql/reads/CqlRowQueryImpl.java;<<<<<<< MINE
=======
package com.netflix.astyanax.cql.reads;

import java.nio.ByteBuffer;
import java.util.ArrayList;
import java.util.Arrays;
import java.util.Collection;
import java.util.Iterator;
import java.util.List;

import com.datastax.driver.core.ResultSet;
import com.datastax.driver.core.Row;
import com.datastax.driver.core.Statement;
import com.google.common.util.concurrent.ListenableFuture;
import com.netflix.astyanax.CassandraOperationType;
import com.netflix.astyanax.RowCopier;
import com.netflix.astyanax.Serializer;
import com.netflix.astyanax.connectionpool.OperationResult;
import com.netflix.astyanax.connectionpool.exceptions.ConnectionException;
import com.netflix.astyanax.connectionpool.exceptions.NotFoundException;
import com.netflix.astyanax.cql.CqlAbstractExecutionImpl;
import com.netflix.astyanax.cql.CqlKeyspaceImpl.KeyspaceContext;
import com.netflix.astyanax.cql.CqlOperationResultImpl;
import com.netflix.astyanax.cql.reads.model.CqlColumnListImpl;
import com.netflix.astyanax.cql.reads.model.CqlColumnSlice;
import com.netflix.astyanax.cql.reads.model.CqlRangeBuilder;
import com.netflix.astyanax.cql.reads.model.CqlRangeImpl;
import com.netflix.astyanax.cql.schema.CqlColumnFamilyDefinitionImpl;
import com.netflix.astyanax.cql.util.CFQueryContext;
import com.netflix.astyanax.ddl.ColumnDefinition;
import com.netflix.astyanax.model.ByteBufferRange;
import com.netflix.astyanax.model.ColumnFamily;
import com.netflix.astyanax.model.ColumnList;
import com.netflix.astyanax.model.ColumnSlice;
import com.netflix.astyanax.query.ColumnCountQuery;
import com.netflix.astyanax.query.ColumnQuery;
import com.netflix.astyanax.query.RowQuery;
import com.netflix.astyanax.serializers.CompositeRangeBuilder;
import com.netflix.astyanax.serializers.CompositeRangeBuilder.CompositeByteBufferRange;

/**
 * Impl for {@link RowQuery} that uses java driver. It manages all single row queries and also has support for pagination. 
 * All {@link ColumnQuery} and {@link ColumnCountQuery}(s) originate from this class. 
 * 
 * Note that the class acts more like a placeholder for the structure query context. The actual query construction 
 * is done by other classes like {@link CFRowQueryGen} and {@link CFColumnQueryGen}
 * 
 * @author poberai
 *
 * @param <K>
 * @param <C>
 */
public class CqlRowQueryImpl<K, C> implements RowQuery<K, C> {

	private final KeyspaceContext ksContext;
	private final CFQueryContext<K,C> cfContext;

	private final Object rowKey;
	private final CqlColumnSlice<C> columnSlice = new CqlColumnSlice<C>();
	private CompositeByteBufferRange compositeRange;
	private final PaginationContext paginationContext = new PaginationContext();

	public enum RowQueryType {
		AllColumns, ColumnSlice, ColumnRange, SingleColumn; 
	}
	
	private RowQueryType queryType = RowQueryType.AllColumns;  // The default
	private boolean useCaching = false;
	
	public CqlRowQueryImpl(KeyspaceContext ksCtx, CFQueryContext<K,C> cfCtx, K rKey, boolean useCaching) {
		this.ksContext = ksCtx;
		this.cfContext = cfCtx;
		this.rowKey = cfCtx.checkRowKey(rKey);
		this.useCaching = useCaching;
	}

	@Override
	public OperationResult<ColumnList<C>> execute() throws ConnectionException {

		if (paginationContext.isPaginating()) {
			if (!paginationContext.isFirstPage()) {
				return new CqlOperationResultImpl<ColumnList<C>>(paginationContext.getResultSet(), paginationContext.getNextColumns());
			}
			// Note that if we are paginating, and if this is the first time / page, 
			// then we will just execute the query normally, and then init the pagination context
		}

		return new InternalRowQueryExecutionImpl(this).execute();
	}

	@Override
	public ListenableFuture<OperationResult<ColumnList<C>>> executeAsync() throws ConnectionException {
		return new InternalRowQueryExecutionImpl(this).executeAsync();
	}

	@Override
	public ColumnQuery<C> getColumn(C column) {
		queryType = RowQueryType.SingleColumn;
		return new CqlColumnQueryImpl<C>(ksContext, cfContext, rowKey, column, useCaching);
	}

	@Override
	public RowQuery<K, C> withColumnSlice(Collection<C> columns) {
		queryType = RowQueryType.ColumnSlice;
		this.columnSlice.setColumns(columns);
		return this;
	}

	@Override
	public RowQuery<K, C> withColumnSlice(C... columns) {
		queryType = RowQueryType.ColumnSlice;
		return withColumnSlice(Arrays.asList(columns));
	}

	@Override
	public RowQuery<K, C> withColumnSlice(ColumnSlice<C> colSlice) {
		if (colSlice.getColumns() != null) {
			return withColumnSlice(colSlice.getColumns());
		} else {
			return withColumnRange(colSlice.getStartColumn(), colSlice.getEndColumn(), colSlice.getReversed(), colSlice.getLimit());
		}
	}

	@Override
	public RowQuery<K, C> withColumnRange(C startColumn, C endColumn, boolean reversed, int count) {
		
		queryType = RowQueryType.ColumnRange;

		this.columnSlice.setCqlRange(new CqlRangeBuilder<C>()
				.setColumn("column1")
				.setStart(startColumn)
				.setEnd(endColumn)
				.setReversed(reversed)
				.setLimit(count)
				.build());
		return this;
	}

	@Override
	public RowQuery<K, C> withColumnRange(ByteBuffer startColumn, ByteBuffer endColumn, boolean reversed, int limit) {

		queryType = RowQueryType.ColumnRange;

		Serializer<C> colSerializer = cfContext.getColumnFamily().getColumnSerializer();
		C start = (startColumn != null && startColumn.capacity() > 0) ? colSerializer.fromByteBuffer(startColumn) : null;
		C end = (endColumn != null && endColumn.capacity() > 0) ? colSerializer.fromByteBuffer(endColumn) : null;
		return this.withColumnRange(start, end, reversed, limit);
	}

	@SuppressWarnings("unchecked")
	@Override
	public RowQuery<K, C> withColumnRange(ByteBufferRange range) {

		queryType = RowQueryType.ColumnRange;

		if (range instanceof CompositeByteBufferRange) {
			this.compositeRange = (CompositeByteBufferRange) range;

		} else if (range instanceof CompositeRangeBuilder) {
			this.compositeRange = ((CompositeRangeBuilder)range).build();

		} else if (range instanceof CqlRangeImpl) {
			this.columnSlice.setCqlRange((CqlRangeImpl<C>) range);

		} else {
			return this.withColumnRange(range.getStart(), range.getEnd(), range.isReversed(), range.getLimit());
		}
		return this;
	}

	@Override
	@Deprecated
	public RowQuery<K, C> setIsPaginating() {
		return autoPaginate(true);
	}

	@Override
	public RowQuery<K, C> autoPaginate(boolean enabled) {
		paginationContext.setPaginating(enabled);
		return this;
	}

	@Override
	public RowCopier<K, C> copyTo(ColumnFamily<K, C> columnFamily, K rowKey) {
		return new CqlRowCopier<K,C>(columnFamily, rowKey, this, ksContext);
	}

	@Override
	public ColumnCountQuery getCount() {
		return new CqlColumnCountQueryImpl(ksContext, cfContext, new InternalRowQueryExecutionImpl(this).getQuery());
	}

	private class InternalRowQueryExecutionImpl extends CqlAbstractExecutionImpl<ColumnList<C>> {

		private final CqlColumnFamilyDefinitionImpl cfDef = (CqlColumnFamilyDefinitionImpl) cf.getColumnFamilyDefinition();

		private final String[] allPkColumnNames = cfDef.getAllPkColNames();
		private final List<ColumnDefinition> regularCols = cfDef.getRegularColumnDefinitionList();

		private final CqlRowQueryImpl<?,?> rowQuery; 
		
		public InternalRowQueryExecutionImpl(CqlRowQueryImpl<?,?> rQuery) {
			super(ksContext, cfContext);
			this.rowQuery = rQuery;
		}

		@Override
		public CassandraOperationType getOperationType() {
			return CassandraOperationType.GET_ROW;
		}

		@Override
		public Statement getQuery() {
			
			Statement stmt = cfDef.getRowQueryGenerator().getQueryStatement(rowQuery, useCaching);
			// Translate the column limit to the fetch size. This is useful for pagination
			if (paginationContext.isPaginating() && columnSlice.isRangeQuery()) {
//				if (columnSlice.getFetchSize() > 0) {
//					stmt.setFetchSize(columnSlice.getFetchSize() + 1);
//				}
			}
			return stmt;
		}

		@Override
		public ColumnList<C> parseResultSet(ResultSet resultSet) throws NotFoundException {

			// Use case when the schema is just a flat table. Note that there is no pagination support here.
			if (allPkColumnNames.length == 1 || regularCols.size() > 1) {
				List<Row> rows = resultSet.all(); 
				if (rows == null || rows.isEmpty()) {
					return new CqlColumnListImpl<C>();
				} else {
					return new CqlColumnListImpl<C>(rows.get(0), cf);
				}
			}
			
			// There is a clustering key for this schema. Check whether we are paginating for this row query
			if (paginationContext.isPaginating()) {
				
				paginationContext.init(resultSet, columnSlice.getFetchSize());
				return paginationContext.getNextColumns();
				
			} else {
				
				List<Row> rows = resultSet.all(); 
				if (rows == null || rows.isEmpty()) {
					return new CqlColumnListImpl<C>();
				} else {
					return new CqlColumnListImpl<C>(rows, cf);
				}
			}
			
			
//			List<Row> rows = resultSet.all(); 
//
//			if (rows == null || rows.isEmpty()) {
//				if (paginationContext.isPaginating()) {
//					paginationContext.lastPageConsumed = true;
//				}
//				return new CqlColumnListImpl<C>();
//			}
//			
//			if (allPkColumnNames.length == 1 || regularCols.size() > 1) {
//				CqlColumnListImpl<C> columnList = new CqlColumnListImpl<C>(rows.get(0), cf);
//				return columnList;
//			} else {
//				CqlColumnListImpl<C> columnList = new CqlColumnListImpl<C>(rows, cf);
//				paginationContext.trackLastColumn(columnList);
//				return columnList;
//			}
		}


	}
	
	
	private class PaginationContext {
		
		// How many rows to fetch at a time
		private int fetchSize = Integer.MAX_VALUE; 
		
		// Turn pagination ON/OFF
		private boolean paginate = false;
		// Indicate whether the first page has been consumed. 
		private boolean isFirstPage = true;
		// Track the result set
		private ResultSet resultSet = null;
		// State for all rows
		private Iterator<Row> rowIter = null;
		
		private PaginationContext() {
		}
		
		private void setPaginating(boolean condition) {
			paginate = condition;
		}
		
		private boolean isPaginating() {
			return paginate;
		}
		
		private boolean isFirstPage() {
			return isFirstPage;
		}
		
		private void firstPageConsumed() {
			isFirstPage = false;
		}

		private CqlColumnListImpl<C> getNextColumns() {
			
			try { 
				int count = 0;
				List<Row> rows = new ArrayList<Row>();
				while ((count < fetchSize) && rowIter.hasNext()) {
					rows.add(rowIter.next());
					count++;
				}
				return new CqlColumnListImpl<C>(rows, cfContext.getColumnFamily());
			} finally {
				firstPageConsumed();
			}
		}
		
		private void init(ResultSet rs, int size) {
			this.resultSet = rs;
			this.rowIter = resultSet.iterator();
			if (size > 0) {
				fetchSize = size;
			}

		}
		
		private ResultSet getResultSet() {
			return this.resultSet;
		}
	}
	
	public Object getRowKey() {
		return rowKey;
	}
	
	public CqlColumnSlice<C> getColumnSlice() {
		return columnSlice;
	}

	public CompositeByteBufferRange getCompositeRange() {
		return compositeRange;
	}
	
	public RowQueryType getQueryType() {
		return queryType;
	}
	
	public boolean isPaginating() {
		return paginationContext.isPaginating();
	}
 }>>>>>>> YOURS
/home/ramdisk/experiment3/projects/astyanax/revisions/rev_2b15fd9_3de01e3/rev_2b15fd9-3de01e3/astyanax-cql/src/main/java/com/netflix/astyanax/cql/reads/CqlAllRowsQueryImpl.java;<<<<<<< MINE
=======
package com.netflix.astyanax.cql.reads;

import java.math.BigInteger;
import java.nio.ByteBuffer;
import java.util.Arrays;
import java.util.Collection;
import java.util.Collections;
import java.util.LinkedList;
import java.util.List;
import java.util.concurrent.Callable;
import java.util.concurrent.ExecutorService;
import java.util.concurrent.Executors;
import java.util.concurrent.Future;
import java.util.concurrent.atomic.AtomicBoolean;
import java.util.concurrent.atomic.AtomicReference;

import org.slf4j.Logger;
import org.slf4j.LoggerFactory;

import com.google.common.collect.Lists;
import com.google.common.util.concurrent.ListenableFuture;
import com.google.common.util.concurrent.ThreadFactoryBuilder;
import com.netflix.astyanax.ExceptionCallback;
import com.netflix.astyanax.Keyspace;
import com.netflix.astyanax.RowCallback;
import com.netflix.astyanax.Serializer;
import com.netflix.astyanax.connectionpool.OperationResult;
import com.netflix.astyanax.connectionpool.TokenRange;
import com.netflix.astyanax.connectionpool.exceptions.ConnectionException;
import com.netflix.astyanax.cql.CqlOperationResultImpl;
import com.netflix.astyanax.cql.reads.model.CqlColumnSlice;
import com.netflix.astyanax.cql.reads.model.CqlRangeBuilder;
import com.netflix.astyanax.cql.reads.model.CqlRangeImpl;
import com.netflix.astyanax.cql.reads.model.CqlRowListImpl;
import com.netflix.astyanax.cql.schema.CqlColumnFamilyDefinitionImpl;
import com.netflix.astyanax.model.ByteBufferRange;
import com.netflix.astyanax.model.ColumnFamily;
import com.netflix.astyanax.model.ColumnSlice;
import com.netflix.astyanax.model.ConsistencyLevel;
import com.netflix.astyanax.model.Row;
import com.netflix.astyanax.model.Rows;
import com.netflix.astyanax.partitioner.Murmur3Partitioner;
import com.netflix.astyanax.partitioner.Partitioner;
import com.netflix.astyanax.query.AllRowsQuery;
import com.netflix.astyanax.query.CheckpointManager;
import com.netflix.astyanax.query.ColumnFamilyQuery;
import com.netflix.astyanax.query.RowSliceQuery;
import com.netflix.astyanax.shallows.EmptyCheckpointManager;

/**
 * Impl for {@link AllRowsQuery} that uses the java driver underneath. 
 * Note that it is easier and more intuitive to just use the AllRowsReader recipe instead. 
 * See https://github.com/Netflix/astyanax/wiki/AllRowsReader-All-rows-query for details on how to use the recipe. 
 * 
 * @author poberai
 *
 * @param <K>
 * @param <C>
 */
public class CqlAllRowsQueryImpl<K,C> implements AllRowsQuery<K,C> {

    private static final Logger LOG = LoggerFactory.getLogger(CqlAllRowsQueryImpl.class);
    
    private static final Partitioner DEFAULT_PARTITIONER = Murmur3Partitioner.get();
    private final static int DEFAULT_PAGE_SIZE = 100;
    
    private final Keyspace      keyspace;
    private final ColumnFamily<K, C> columnFamily;
    
    private    Integer                 rowLimit = DEFAULT_PAGE_SIZE;
    private    Integer             concurrencyLevel;   // Default to null will force ring describe
    private    ExecutorService     executor;
    private    CheckpointManager   checkpointManager = new EmptyCheckpointManager();
    private    RowCallback<K, C>   rowCallback;
    private    boolean             repeatLastToken;
    private    ColumnSlice<C>      columnSlice;
    private    String              startToken;
    private    String              endToken;
    private    Boolean             includeEmptyRows;  // Default to null will discard tombstones
    private    List<Future<Boolean>> futures = Lists.newArrayList();
    private    AtomicBoolean       cancelling = new AtomicBoolean(false);
    private    Partitioner         partitioner = DEFAULT_PARTITIONER;
    private    ConsistencyLevel	consistencyLevel;
    private    ExceptionCallback   exceptionCallback;
    private AtomicReference<Exception>  error = new AtomicReference<Exception>();

    public CqlAllRowsQueryImpl(Keyspace ks, ColumnFamily<K,C> cf) {
    	this.keyspace = ks;
    	this.columnFamily = cf;
    	
    }
	@Override
	public AllRowsQuery<K, C> setBlockSize(int blockSize) {
		setRowLimit(blockSize);
		return this;
	}

	@Override
	public AllRowsQuery<K, C> setRowLimit(int rowLimit) {
		this.rowLimit = rowLimit;
		return this;
	}

	@Override
	public AllRowsQuery<K, C> setExceptionCallback(ExceptionCallback cb) {
		this.exceptionCallback = cb;
		return this;
	}

	@Override
	public AllRowsQuery<K, C> setCheckpointManager(CheckpointManager manager) {
		this.checkpointManager = manager;
		return this;
	}

	@Override
	public AllRowsQuery<K, C> setRepeatLastToken(boolean condition) {
		this.repeatLastToken = condition;
		return this;
	}

	@Override
	public AllRowsQuery<K, C> setIncludeEmptyRows(boolean flag) {
		this.includeEmptyRows = flag;
		return this;
	}
	@Override
	public AllRowsQuery<K, C>  withColumnSlice(C... columns) {
		return withColumnSlice(Arrays.asList(columns));
	}

	@Override
	public AllRowsQuery<K, C>  withColumnSlice(Collection<C> columns) {
		this.columnSlice = new CqlColumnSlice<C>(columns);
		return this;
	}

	@Override
	public AllRowsQuery<K, C>  withColumnSlice(ColumnSlice<C> columns) {
		this.columnSlice = new CqlColumnSlice<C>(columns);
		return this;
	}

	@Override
	public AllRowsQuery<K, C>  withColumnRange(C startColumn, C endColumn, boolean reversed, int count) {
		
		CqlColumnFamilyDefinitionImpl cfDef = (CqlColumnFamilyDefinitionImpl) columnFamily.getColumnFamilyDefinition();
		String pkColName = cfDef.getPartitionKeyColumnDefinitionList().get(1).getName();
		
		this.columnSlice = new CqlColumnSlice<C>(new CqlRangeBuilder<C>()
				.setColumn(pkColName)
				.setStart(startColumn)
				.setEnd(endColumn)
				.setReversed(reversed)
				.setLimit(count)
				.build());
		return this;
	}

	@Override
	public AllRowsQuery<K, C>  withColumnRange(ByteBuffer startColumn, ByteBuffer endColumn, boolean reversed, int limit) {
		Serializer<C> colSerializer = columnFamily.getColumnSerializer();
		C start = (startColumn != null && startColumn.capacity() > 0) ? colSerializer.fromByteBuffer(startColumn) : null;
		C end = (endColumn != null && endColumn.capacity() > 0) ? colSerializer.fromByteBuffer(endColumn) : null;
		return this.withColumnRange(start, end, reversed, limit);
	}

	@Override
	public AllRowsQuery<K, C> withColumnRange(ByteBufferRange range) {
		if (range instanceof CqlRangeImpl) {
			this.columnSlice = new CqlColumnSlice<C>();
			((CqlColumnSlice<C>) this.columnSlice).setCqlRange((CqlRangeImpl<C>) range);
			return this;
		} else {
			return this.withColumnRange(range.getStart(), range.getEnd(), range.isReversed(), range.getLimit());
		}
	}

	@Override
	public AllRowsQuery<K, C> setConcurrencyLevel(int numberOfThreads) {
		this.concurrencyLevel = numberOfThreads;
		return this;
	}

	@Override
	@Deprecated
	public AllRowsQuery<K, C> setThreadCount(int numberOfThreads) {
		this.concurrencyLevel = numberOfThreads;
		return this;
	}

	@Override
	public void executeWithCallback(RowCallback<K, C> callback) throws ConnectionException {
		this.rowCallback = callback;
		executeTasks();
	}

	@Override
	public AllRowsQuery<K, C> forTokenRange(BigInteger start, BigInteger end) {
		return forTokenRange(start.toString(), end.toString());
	}

	@Override
	public AllRowsQuery<K, C> forTokenRange(String start, String end) {
		this.startToken = start;
		this.endToken = end;
		return this;
	}
	
	@Override
	public OperationResult<Rows<K, C>> execute() throws ConnectionException {
		
		final AtomicReference<ConnectionException> reference = new AtomicReference<ConnectionException>(null);
		
		final List<Row<K,C>> list = Collections.synchronizedList(new LinkedList<Row<K,C>>());
		
		RowCallback<K,C> rowCallback = new RowCallback<K,C>() {

			@Override
			public void success(Rows<K,C> rows) {
				if (rows != null && !rows.isEmpty()) {
					for (Row<K,C> row : rows) {
						list.add(row);
					}
				}
			}

			@Override
			public boolean failure(ConnectionException e) {
				reference.set(e);
				return false;
			}
		};
		
		executeWithCallback(rowCallback);
		
		if (reference.get() != null) {
			throw reference.get();
		}
		
		CqlRowListImpl<K,C> allRows = new CqlRowListImpl<K,C>(list);
		return new CqlOperationResultImpl<Rows<K,C>>(null, allRows);
	}

	@Override
	public ListenableFuture<OperationResult<Rows<K, C>>> executeAsync() throws ConnectionException {
		throw new UnsupportedOperationException();
	}


	private Boolean executeTasks() throws ConnectionException {
        error.set(null);
        
        List<Callable<Boolean>> subtasks = Lists.newArrayList();
        
        // We are iterating the entire ring using an arbitrary number of threads
        if (this.concurrencyLevel != null || startToken != null || endToken != null) {

    		List<TokenRange> tokens = partitioner.splitTokenRange(
                    startToken == null ? partitioner.getMinToken() : startToken, 
                    endToken == null   ? partitioner.getMinToken() : endToken, 
                    this.concurrencyLevel == null ? 1 : this.concurrencyLevel);
            
            for (TokenRange range : tokens) {
                subtasks.add(makeTokenRangeTask(range.getStartToken(), range.getEndToken()));
            }
        }
        // We are iterating through each token range
        else {
            List<TokenRange> ranges = keyspace.describeRing(null, null);
            for (TokenRange range : ranges) {
                if (range.getStartToken().equals(range.getEndToken())) {
                    subtasks.add(makeTokenRangeTask(range.getStartToken(), range.getEndToken()));
                } else {
                    subtasks.add(makeTokenRangeTask(partitioner.getTokenMinusOne(range.getStartToken()), range.getEndToken()));
                }
            }
        }
        
        try {
            // Use a local executor
            if (executor == null) {
                ExecutorService localExecutor = Executors
                        .newFixedThreadPool(subtasks.size(),
                            new ThreadFactoryBuilder().setDaemon(true)
                                .setNameFormat("AstyanaxAllRowsQuery-%d")
                                .build());
                
                try {
                	futures.addAll(startTasks(localExecutor, subtasks));
                    return waitForTasksToFinish();
                }
                finally {
                    localExecutor.shutdownNow();
                }
            }
            // Use an externally provided executor
            else {
                futures.addAll(startTasks(executor, subtasks));
                return waitForTasksToFinish();
            }
        }
        catch (Exception e) {
            error.compareAndSet(null, e);
            LOG.warn("AllRowsReader terminated. " + e.getMessage(), e);
            cancel();
            
            throw new RuntimeException(error.get());
        }
    }


    private Callable<Boolean> makeTokenRangeTask(final String startToken, final String endToken) {
        return new Callable<Boolean>() {
            @Override
            public Boolean call() {
                try {
                    String currentToken;
                    try {
                        currentToken = checkpointManager.getCheckpoint(startToken);
                        if (currentToken == null) {
                            currentToken = startToken;
                        }
                        else if (currentToken.equals(endToken)) {
                            return true;
                        }
                    } catch (Exception e) {
                        error.compareAndSet(null, e);
                        LOG.error("Failed to get checkpoint for startToken " + startToken, e);
                        cancel();
                        throw new RuntimeException("Failed to get checkpoint for startToken " + startToken, e);
                    }
                    
                    int localPageSize = rowLimit;
                    int rowsToSkip = 0;
                    while (!cancelling.get()) {
                        RowSliceQuery<K, C> query = prepareQuery().getKeyRange(null, null, currentToken, endToken, -1);
                        
                        if (columnSlice != null)
                            query.withColumnSlice(columnSlice);
                        
                        Rows<K, C> rows = query.execute().getResult();
                        if (!rows.isEmpty()) {
                           try {
                                if (rowCallback != null) {
                                    try { 
                                    	rowCallback.success(rows);
                                    } catch (Exception e) {
                                    	LOG.error("Failed to process rows", e);
                                        cancel();
                                        return false;
                                    }
                                } else {
                                	LOG.error("Row function is empty");
                                }
                            } catch (Exception e) {
                                error.compareAndSet(null, e);
                                LOG.warn(e.getMessage(), e);
                                cancel();
                                throw new RuntimeException("Error processing row", e);
                            }
                                
                            // Get the next block
                            if (rows.size() == rowLimit) {
                                Row<K, C> lastRow = rows.getRowByIndex(rows.size() - 1);
                                String lastToken = partitioner.getTokenForKey(lastRow.getRawKey());
                                checkpointManager.trackCheckpoint(startToken, currentToken);
                                if (repeatLastToken) {
                                    // Start token is non-inclusive
                                    currentToken = partitioner.getTokenMinusOne(lastToken);
                                    
                                 // Determine the number of rows to skip in the response.  Since we are repeating the
                                 // last token it's possible (although unlikely) that there is more than one key mapping to the
                                 // token.  We therefore count backwards the number of keys that have the same token and skip 
                                 // that number in the next iteration of the loop.  If, for example, 3 keys matched but only 2 were
                                    // returned in this iteration then the first 2 keys will be skipped from the next response.
                                    rowsToSkip = 1;
                                    for (int i = rows.size() - 2; i >= 0; i--, rowsToSkip++) {
                                        if (!lastToken.equals(partitioner.getTokenForKey(rows.getRowByIndex(i).getRawKey()))) {
                                            break;
                                        }
                                    }

                                    if (rowsToSkip == localPageSize) {
                                        localPageSize++;
                                    }
                                } else {
                                    currentToken = lastToken;
                                }
                                
                                continue;
                            }
                        }
                        
                        // We're done!
                        checkpointManager.trackCheckpoint(startToken, endToken);
                        return true;
                    }
                    cancel();
                    return false;
                } catch (Exception e) {
                    error.compareAndSet(null, e);
                    LOG.error("Error process token/key range", e);
                    cancel();
                    throw new RuntimeException("Error process token/key range", e);
                }
            }
        };
    }


    /**
     * Submit all the callables to the executor by synchronize their execution so they all start
     * AFTER the have all been submitted.
     * @param executor
     * @param callables
     * @return
     */
    private List<Future<Boolean>> startTasks(ExecutorService executor, List<Callable<Boolean>> callables) {
        List<Future<Boolean>> tasks = Lists.newArrayList();
        for (Callable<Boolean> callable : callables) {
            tasks.add(executor.submit(callable));
        }
        return tasks;
    }
    
    /**
     * Wait for all tasks to finish.
     * 
     * @param futures
     * @return true if all tasks returned true or false otherwise.  
     */
    private boolean waitForTasksToFinish() throws Exception {
        for (Future<Boolean> future : futures) {
            try {
                if (!future.get()) {
                    cancel();
                    return false;
                }
            } catch (Exception e) {
                error.compareAndSet(null, e);
                cancel();
                throw e;
            }
        }
        return true;
    }
    

    
    private ColumnFamilyQuery<K, C> prepareQuery() {
    	ColumnFamilyQuery<K, C> query = keyspace.prepareQuery(columnFamily);
    	if (consistencyLevel != null)
    		query.setConsistencyLevel(consistencyLevel);
    	return query;
    }
    
    
    /**
     * Cancel all pending range iteration tasks.  This will cause all internal threads to exit and
     * call() to return false.
     */
    public synchronized void cancel() {
        cancelling.compareAndSet(false, true);
    }
}>>>>>>> YOURS
/home/ramdisk/experiment3/projects/astyanax/revisions/rev_2b15fd9_3de01e3/rev_2b15fd9-3de01e3/astyanax-cql/src/main/java/com/netflix/astyanax/cql/reads/CFRowSliceQueryGen.java;<<<<<<< MINE
=======
package com.netflix.astyanax.cql.reads;

import static com.datastax.driver.core.querybuilder.QueryBuilder.desc;
import static com.datastax.driver.core.querybuilder.QueryBuilder.eq;
import static com.datastax.driver.core.querybuilder.QueryBuilder.gt;
import static com.datastax.driver.core.querybuilder.QueryBuilder.gte;
import static com.datastax.driver.core.querybuilder.QueryBuilder.lt;
import static com.datastax.driver.core.querybuilder.QueryBuilder.lte;

import java.util.List;
import java.util.concurrent.atomic.AtomicReference;

import com.datastax.driver.core.Session;
import com.datastax.driver.core.querybuilder.QueryBuilder;
import com.datastax.driver.core.querybuilder.Select;
import com.datastax.driver.core.querybuilder.Select.Where;
import com.netflix.astyanax.cql.reads.model.CqlColumnSlice;
import com.netflix.astyanax.cql.schema.CqlColumnFamilyDefinitionImpl;
import com.netflix.astyanax.ddl.ColumnDefinition;
import com.netflix.astyanax.query.RowSliceQuery;
import com.netflix.astyanax.serializers.CompositeRangeBuilder.CompositeByteBufferRange;
import com.netflix.astyanax.serializers.CompositeRangeBuilder.RangeQueryOp;
import com.netflix.astyanax.serializers.CompositeRangeBuilder.RangeQueryRecord;

/**
 * Base class that contains the utilities for generating queries for read operations via the 
 * {@link RowSliceQuery} class. 
 * 
 * Note that this class is just a place holder for some useful generic utilities. 
 * See {@link CFRowKeysQueryGen} and {@link CFRowRangeQueryGen} which are the 2 extending classes 
 * for functionality that actually supports the queries. 
 * 
 * @author poberai
 */
public class CFRowSliceQueryGen {

	// Thread safe reference to the underlying session object. We need the session object to be able to "prepare" query statements
	protected final AtomicReference<Session> sessionRef = new AtomicReference<Session>(null);
	// the keyspace being queried. Used for all the underlying queries being generated
	protected final String keyspace; 
	// the cf definition which helps extending classes construct the right query as per the schema
	protected final CqlColumnFamilyDefinitionImpl cfDef;

	// Other useful derivatives of the cf definition that are frequently used by query generators
	protected final String partitionKeyCol;
	protected final String[] allPrimayKeyCols;
	protected final List<ColumnDefinition> clusteringKeyCols;
	protected final List<ColumnDefinition> regularCols;
	
	// Condition tracking whether the underlying schema uses composite columns. This is imp since it influences how 
	// a single Column (composite column) can be decomposed into it's individual components that form different parts of the query.
	protected boolean isCompositeColumn; 
	
	// bind marker for generating the prepared statements
	protected static final String BIND_MARKER = "?";
	
	public CFRowSliceQueryGen(Session session, String keyspaceName, CqlColumnFamilyDefinitionImpl cfDefinition) {

		this.keyspace = keyspaceName;
		this.cfDef = cfDefinition;
		this.sessionRef.set(session);

		partitionKeyCol = cfDef.getPartitionKeyColumnDefinition().getName();
		allPrimayKeyCols = cfDef.getAllPkColNames();
		clusteringKeyCols = cfDef.getClusteringKeyColumnDefinitionList();
		regularCols = cfDef.getRegularColumnDefinitionList();

		isCompositeColumn = (clusteringKeyCols.size() > 1);
	}
	
	/**
	 * 
	 *   SOME BASIC UTILITY METHODS USED BY ALL THE ROW SLICE QUERY GENERATORS
	 */
	
	protected Select selectAllColumnsFromKeyspaceAndCF() {

		Select.Selection select = QueryBuilder.select();
		for (int i=0; i<allPrimayKeyCols.length; i++) {
			select.column(allPrimayKeyCols[i]);
		}

		for (ColumnDefinition colDef : regularCols) {
			String colName = colDef.getName();
			select.column(colName).ttl(colName).writeTime(colName);
		}
		return select.from(keyspace, cfDef.getName());
	}
	
	protected Where addWhereClauseForColumnRange(Where where, CqlColumnSlice<?> columnSlice) {

		String clusteringKeyCol = clusteringKeyCols.get(0).getName();

		if (!columnSlice.isRangeQuery()) {
			return where;
		}
		if (columnSlice.getStartColumn() != null) {
			where.and(gte(clusteringKeyCol, columnSlice.getStartColumn()));
		}
		if (columnSlice.getEndColumn() != null) {
			where.and(lte(clusteringKeyCol, columnSlice.getEndColumn()));
		}

		if (columnSlice.getReversed()) {
			where.orderBy(desc(clusteringKeyCol));
		}

		if (columnSlice.getLimit() != -1) {
			where.limit(columnSlice.getLimit());
		}

		return where;
	}
	
	protected void bindWhereClauseForColumnRange(List<Object> values, CqlColumnSlice<?> columnSlice) {

		if (!columnSlice.isRangeQuery()) {
			return;
		}
		if (columnSlice.getStartColumn() != null) {
			values.add(columnSlice.getStartColumn());
		}
		if (columnSlice.getEndColumn() != null) {
			values.add(columnSlice.getEndColumn());
		}

		if (columnSlice.getLimit() != -1) {
			values.add(columnSlice.getLimit());
		}

		return;
	}
	

	protected Where addWhereClauseForCompositeColumnRange(Where stmt, CompositeByteBufferRange compositeRange) {

		List<RangeQueryRecord> records = compositeRange.getRecords();
		int componentIndex = 0; 

		for (RangeQueryRecord record : records) {

			for (RangeQueryOp op : record.getOps()) {

				String columnName = clusteringKeyCols.get(componentIndex).getName();

				switch (op.getOperator()) {

				case EQUAL:
					stmt.and(eq(columnName, BIND_MARKER));
					componentIndex++;
					break;
				case LESS_THAN :
					stmt.and(lt(columnName, BIND_MARKER));
					break;
				case LESS_THAN_EQUALS:
					stmt.and(lte(columnName, BIND_MARKER));
					break;
				case GREATER_THAN:
					stmt.and(gt(columnName, BIND_MARKER));
					break;
				case GREATER_THAN_EQUALS:
					stmt.and(gte(columnName, BIND_MARKER));
					break;
				default:
					throw new RuntimeException("Cannot recognize operator: " + op.getOperator().name());
				}; // end of switch stmt
			} // end of inner for for ops for each range query record
		}
		return stmt;
	}

	protected void bindWhereClauseForCompositeColumnRange(List<Object> values, CompositeByteBufferRange compositeRange) {

		List<RangeQueryRecord> records = compositeRange.getRecords();

		for (RangeQueryRecord record : records) {
			for (RangeQueryOp op : record.getOps()) {
				values.add(op.getValue());
			}
		}
		return;
	}
	
	protected Object[] bindMarkerArray(int n) {
		
		Object[] arr = new Object[n];
		for (int i=0; i<n; i++) {
			arr[i] = BIND_MARKER;
		}
		return arr;
	}
}>>>>>>> YOURS
/home/ramdisk/experiment3/projects/astyanax/revisions/rev_2b15fd9_3de01e3/rev_2b15fd9-3de01e3/astyanax-cql/src/main/java/com/netflix/astyanax/cql/reads/FlatTableRowQueryGen.java;<<<<<<< MINE
=======
package com.netflix.astyanax.cql.reads;

import static com.datastax.driver.core.querybuilder.QueryBuilder.eq;

import java.util.List;
import java.util.concurrent.Callable;
import java.util.concurrent.atomic.AtomicReference;

import com.datastax.driver.core.BoundStatement;
import com.datastax.driver.core.PreparedStatement;
import com.datastax.driver.core.RegularStatement;
import com.datastax.driver.core.Session;
import com.datastax.driver.core.Statement;
import com.datastax.driver.core.querybuilder.QueryBuilder;
import com.datastax.driver.core.querybuilder.Select;
import com.datastax.driver.core.querybuilder.Select.Selection;
import com.netflix.astyanax.cql.schema.CqlColumnFamilyDefinitionImpl;
import com.netflix.astyanax.ddl.ColumnDefinition;

/**
 * Read query generator for queries on flat tables i.e tables with no clustering keys.
 * 
 * The class lives along other implementations like {@link CFRowQueryGen}, {@link CFRowRangeQueryGen} and {@link CFRowKeysQueryGen}
 * The structure of queries for flat tables was different enough that they warranted their own class. If your schema contains clustering keys
 * then see {@link CFRowQueryGen}, {@link CFRowRangeQueryGen} and {@link CFRowKeysQueryGen} for implementation details. 
 * 
 * Note that the class manages several individual query generators for different use cases like 
 * 1. Selecting the entire row
 * 2. Performing a column slice operation i.e column collection
 *  
 * Each of these query generators uses the {@link QueryGenCache} to maintain a cached reference to the {@link PreparedStatement}
 * that it creates, which can then be leveraged by subsequent flat table queries that have the same signature. 
 * 
 * Note the one must use caching for flat table queries with EXTREME CAUTION. The cacheability of a query depends on the actual 
 * signature of a query. If you use different queries with different signatures for the same column slice operations, then caching will 
 * not work. Here is an example where caching will break queries.
 * 
 *  Consider a query where you want to perform a column slice operation i.e cherry pick some column for a given row. 
 *  The Astyanax query for that will look somewhat like this 
 *  
 *         ks.prepareQuery( myCF )
 *           .getRow( 1 )
 *           .getColumn( first_name )
 *           .execute(); 
 *           
 *  Now if the table is a flat table, then the query for this will look something like 
 *  
 *       SELECT first_name FROM ks.myCF WHERE key = ? ;
 *       
 *  Note the bind marker for the row key. That is the only parameter here which is dynamic and the column name here i.e "first_name" is not
 *  and hence is part of the signature of this query. 
 *  
 *  Now if we were to attempt to re-use the same prepared statement for a query like this 
 *  
 *         ks.prepareQuery( myCF )
 *           .getRow( 1 )
 *           .getColumn( last_name )  <------ NOTE THAT WE ARE CHANGING OUR COLUMN SLICE AND HENCE VIOLATING THE QUERY SIGNATURE
 *           .execute(); 
 *  
 *  Then this will break since the CQL query required for this is 
 *  
 *       SELECT first_name FROM ks.myCF WHERE key = ? ;
 *       
 *   In cases like this, DO NOT use statement caching. 
 *  
 * @author poberai
 *
 */
public class FlatTableRowQueryGen {
	
	// Reference to the session that is needed for "preparing" the statements
	private AtomicReference<Session> sessionRef = new AtomicReference<Session>(null);
	private final String keyspace; 
	private final CqlColumnFamilyDefinitionImpl cfDef;

	private final String partitionKeyCol;
	private final String[] allPrimayKeyCols;
	private final List<ColumnDefinition> regularCols;
	
	private static final String BIND_MARKER = "?";

	/**
	 * Constructor
	 * @param session
	 * @param keyspaceName
	 * @param cfDefinition
	 */
	public FlatTableRowQueryGen(Session session, String keyspaceName, CqlColumnFamilyDefinitionImpl cfDefinition) {

		this.keyspace = keyspaceName;
		this.cfDef = cfDefinition;
		this.sessionRef.set(session);
		
		partitionKeyCol = cfDef.getPartitionKeyColumnDefinition().getName();
		allPrimayKeyCols = cfDef.getAllPkColNames();
		regularCols = cfDef.getRegularColumnDefinitionList();
	}
	
	/**
	 * Query generator that generates a query to read the entire row, i.e all the columns. 
	 * Note that since it implements the {@link QueryGenCache} it also maintains an inner cached reference 
	 * to the {@link PreparedStatement} that it creates which can then be re-used by subsequent queries that 
	 * have the same signature (i.e read all columns)
	 */
	private QueryGenCache<CqlRowQueryImpl<?,?>> SelectEntireRow = new QueryGenCache<CqlRowQueryImpl<?,?>>(sessionRef) {

		@Override
		public Callable<RegularStatement> getQueryGen(CqlRowQueryImpl<?, ?> rowQuery) {

			return new Callable<RegularStatement>() {

				@Override
				public RegularStatement call() throws Exception {
					Selection select = QueryBuilder.select();

					for (int i=0; i<allPrimayKeyCols.length; i++) {
						select.column(allPrimayKeyCols[i]);
					}

					for (ColumnDefinition colDef : regularCols) {
						String colName = colDef.getName();
						select.column(colName).ttl(colName).writeTime(colName);
					}

					RegularStatement stmt = select.from(keyspace, cfDef.getName()).where(eq(partitionKeyCol, BIND_MARKER));
					return stmt; 
				}
			};
		}

		@Override
		public BoundStatement bindValues(PreparedStatement pStatement, CqlRowQueryImpl<?, ?> rowQuery) {
			return pStatement.bind(rowQuery.getRowKey());
		}
	};

	/**
	 * Query generator that generates a query to peform a column slice operation on the specified row. 
	 * Note that performing column slice operations on flat tables is dangerous since the query signature is not the same,
	 * hence use this with caution. See above for an explanation on query signatures and query cacheability. 
	 * 
	 * Note that since it implements the {@link QueryGenCache} it also maintains an inner cached reference 
	 * to the {@link PreparedStatement} that it creates which can then be re-used by subsequent queries that 
	 * have the same signature (i.e read the same column slice for a given row)
	 */
	private QueryGenCache<CqlRowQueryImpl<?,?>> SelectColumnSlice = new QueryGenCache<CqlRowQueryImpl<?,?>>(sessionRef) {

		@Override
		public Callable<RegularStatement> getQueryGen(final CqlRowQueryImpl<?, ?> rowQuery) {

			return new Callable<RegularStatement>() {

				@Override
				public RegularStatement call() throws Exception {

					Select.Selection select = QueryBuilder.select();
					select.column(partitionKeyCol);

					for (Object col : rowQuery.getColumnSlice().getColumns()) {
						String columnName = (String)col;
						select.column(columnName).ttl(columnName).writeTime(columnName);
					}

					return select.from(keyspace, cfDef.getName()).where(eq(partitionKeyCol, BIND_MARKER));
				}
			};
		}

		@Override
		public BoundStatement bindValues(PreparedStatement pStatement, CqlRowQueryImpl<?, ?> rowQuery) {
			return pStatement.bind(rowQuery.getRowKey());
		}
	};
	
	public Statement getQueryStatement(final CqlRowQueryImpl<?,?> rowQuery, boolean useCaching)  {
		
		switch (rowQuery.getQueryType()) {
		
		case AllColumns:
			return SelectEntireRow.getBoundStatement(rowQuery, useCaching);
		case ColumnSlice:
			return SelectColumnSlice.getBoundStatement(rowQuery, useCaching);
		case ColumnRange:
			throw new RuntimeException("Cannot perform col range query with current schema, missing pk cols");
		default :
			throw new RuntimeException("Flat table RowQuery use case not supported. Fix this!!");
		}
	}
}>>>>>>> YOURS
/home/ramdisk/experiment3/projects/astyanax/revisions/rev_2b15fd9_3de01e3/rev_2b15fd9-3de01e3/astyanax-cql/src/main/java/com/netflix/astyanax/cql/reads/CqlColumnCountQueryImpl.java;<<<<<<< MINE
=======
package com.netflix.astyanax.cql.reads;

import java.util.List;

import com.datastax.driver.core.ResultSet;
import com.datastax.driver.core.Row;
import com.datastax.driver.core.Statement;
import com.google.common.util.concurrent.ListenableFuture;
import com.netflix.astyanax.CassandraOperationType;
import com.netflix.astyanax.connectionpool.OperationResult;
import com.netflix.astyanax.connectionpool.exceptions.ConnectionException;
import com.netflix.astyanax.cql.CqlAbstractExecutionImpl;
import com.netflix.astyanax.cql.CqlKeyspaceImpl.KeyspaceContext;
import com.netflix.astyanax.cql.util.CFQueryContext;
import com.netflix.astyanax.query.ColumnCountQuery;
import com.netflix.astyanax.query.RowQuery;

/**
 * Impl for {@link ColumnCountQuery}
 * 
 * Note that since this query essentially derives itself from the {@link RowQuery} interface, it also uses the statement
 * constructed by the {@link CqlRowQueryImpl} class. The difference in functionality is in how the records form the result set 
 * are parsed. Here we look at the number of rows returned for the same row key. 
 * 
 * Note that since CQL3 can treat columns as rows
 * (depending on the schema), we look for multiple rows with the same row keys. If there are multiple rows, then we count the number 
 * of rows for each unique row key. If there is just one row and the schema definition is like a flat table, then we just count the actual no of data columns returned 
 * in the result set.
 * 
 * See {@link CqlRowQueryImpl} for more details on how the query is actually constructed
 * 
 * @author poberai
 *
 */
public class CqlColumnCountQueryImpl implements ColumnCountQuery {

	private final KeyspaceContext ksContext;
	private final CFQueryContext<?, ?> cfContext;
	private final Statement query;
	
	public CqlColumnCountQueryImpl(KeyspaceContext ksCtx, CFQueryContext<?,?> cfCtx, Statement query) {
		this.ksContext = ksCtx;
		this.cfContext = cfCtx;
		this.query = query;
	}
	
	@Override
	public OperationResult<Integer> execute() throws ConnectionException {
		return new InternalColumnCountExecutionImpl(query).execute();
	}

	@Override
	public ListenableFuture<OperationResult<Integer>> executeAsync() throws ConnectionException {
		return new InternalColumnCountExecutionImpl(query).executeAsync();
	}

	private class InternalColumnCountExecutionImpl extends CqlAbstractExecutionImpl<Integer> {

		public InternalColumnCountExecutionImpl(Statement query) {
			super(ksContext, cfContext);
		}

		@Override
		public CassandraOperationType getOperationType() {
			return CassandraOperationType.GET_COLUMN_COUNT;
		}

		@Override
		public Statement getQuery() {
			return query;
		}

		@Override
		public Integer parseResultSet(ResultSet resultSet) {
			List<Row> rows = resultSet.all();
			if (rows != null) {
				return rows.size();
			} else {
				return 0;
			}
		}
	}
}>>>>>>> YOURS
/home/ramdisk/experiment3/projects/astyanax/revisions/rev_2b15fd9_3de01e3/rev_2b15fd9-3de01e3/astyanax-cql/src/main/java/com/netflix/astyanax/cql/schema/CqlColumnDefinitionImpl.java;<<<<<<< MINE
=======
package com.netflix.astyanax.cql.schema;

import java.nio.ByteBuffer;
import java.util.ArrayList;
import java.util.Collection;
import java.util.HashMap;
import java.util.List;
import java.util.Map;

import org.apache.cassandra.db.marshal.UTF8Type;

import com.datastax.driver.core.ResultSet;
import com.datastax.driver.core.Row;
import com.netflix.astyanax.cql.util.CqlTypeMapping;
import com.netflix.astyanax.ddl.ColumnDefinition;
import com.netflix.astyanax.ddl.FieldMetadata;

/**
 * Impl for {@link ColumnDefinition} interface that constructs the state from a {@link Row} object 
 * retrieved from the java driver {@link ResultSet}. 
 * 
 * @author poberai
 *
 */
public class CqlColumnDefinitionImpl implements ColumnDefinition, Comparable<CqlColumnDefinitionImpl> {

	Map<String, Object> options = new HashMap<String, Object>();

	private CqlColumnType colType;
	private Integer componentIndex; 
	
	public enum CqlColumnType {
		partition_key, clustering_key, regular, compact_value;
	}
	
	public CqlColumnDefinitionImpl() {
		
	}
	
	public CqlColumnDefinitionImpl(Row row) {
		this.setName(row.getString("column_name"));
		
		String validationClass = row.getString("validator");
		if (validationClass.contains("(")) {
			int start = validationClass.indexOf("(");
			int end = validationClass.indexOf(")");
			validationClass = validationClass.substring(start+1, end);
		}
		this.setValidationClass(validationClass);
		
		colType = CqlColumnType.valueOf(row.getString("type"));
		if (colType == CqlColumnType.clustering_key) {
			componentIndex = row.getInt("component_index");
		}
	}
	
	@Override
	public ColumnDefinition setName(String name) {
		options.put("column_name", name);
		return this;
	}

	@Override
	public ColumnDefinition setName(byte[] name) {
		return setName(ByteBuffer.wrap(name));
	}

	@Override
	public ColumnDefinition setName(ByteBuffer name) {
		return setName(UTF8Type.instance.compose(name));
	}

	@Override
	public ColumnDefinition setValidationClass(String value) {
		options.put("validator", value);
		return this;
	}

	@Override
	public ColumnDefinition setIndex(String name, String type) {
		throw new UnsupportedOperationException("Operation not supported");
	}

	@Override
	public ColumnDefinition setKeysIndex(String name) {
		throw new UnsupportedOperationException("Operation not supported");
	}

	@Override
	public ColumnDefinition setKeysIndex() {
		throw new UnsupportedOperationException("Operation not supported");
	}

	@Override
	public ColumnDefinition setIndexWithType(String type) {
		throw new UnsupportedOperationException("Operation not supported");
	}

	@Override
	public String getName() {
		return (String) options.get("column_name");
	}

	@Override
	public ByteBuffer getRawName() {
		return UTF8Type.instance.decompose(getName());
	}

	@Override
	public String getValidationClass() {
		return (String) options.get("validator");
	}

	@Override
	public String getIndexName() {
		throw new UnsupportedOperationException("Operation not supported");
	}

	@Override
	public String getIndexType() {
		throw new UnsupportedOperationException("Operation not supported");
	}

	@Override
	public boolean hasIndex() {
		return getIndexName() != null;
	}

	@Override
	public Map<String, String> getOptions() {
		Map<String, String> result = new HashMap<String, String>();
		for (String key : options.keySet()) {
			result.put(key, options.get(key).toString());
		}
		return result;
	}

	@Override
	public String getOption(String name, String defaultValue) {
		String value = (String) options.get(name);
		if (value == null) {
			return defaultValue;
		} else {
			return value;
		}
	}

	@Override
	public ColumnDefinition setOptions(Map<String, String> setOptions) {
		this.options.putAll(setOptions);
		return this;
	}

	@Override
	public String setOption(String name, String value) {
		this.options.put(name, value);
		return options.get(name).toString();
	}

	@Override
	public Collection<String> getFieldNames() {
		return options.keySet();
	}

	@Override
	public Collection<FieldMetadata> getFieldsMetadata() {
		
		List<FieldMetadata> list = new ArrayList<FieldMetadata>();
		
		for (String key : options.keySet()) {
			Object value = options.get(key);
			Class<?> clazz = value.getClass();
			
			String name = key.toUpperCase();
			String type = clazz.getSimpleName().toUpperCase();
			boolean isContainer = Collection.class.isAssignableFrom(clazz) || Map.class.isAssignableFrom(clazz);
			list.add(new FieldMetadata(name, type, isContainer));
		}
		return list;
	}

	@Override
	public Object getFieldValue(String name) {
		return options.get(name);
	}

	@Override
	public ColumnDefinition setFieldValue(String name, Object value) {
		options.put(name, String.valueOf(value));
		return this;
	}

	@Override
	public ColumnDefinition setFields(Map<String, Object> fields) {
		options.putAll(fields);
		return this;
	}

	public String getCqlType() {
		return CqlTypeMapping.getCqlType(getValidationClass());
	}
	
	public CqlColumnType getColumnType() {
		return this.colType;
	}
	
	public int getComponentIndex() {
		return this.componentIndex;
	}

	@Override
	public int compareTo(CqlColumnDefinitionImpl o) {
		return this.componentIndex.compareTo(o.componentIndex);
	}

}>>>>>>> YOURS
/home/ramdisk/experiment3/projects/astyanax/revisions/rev_2b15fd9_3de01e3/rev_2b15fd9-3de01e3/astyanax-cql/src/main/java/com/netflix/astyanax/cql/schema/CqlKeyspaceDefinitionImpl.java;<<<<<<< MINE
=======
package com.netflix.astyanax.cql.schema;

import static com.datastax.driver.core.querybuilder.QueryBuilder.eq;

import java.util.ArrayList;
import java.util.Collection;
import java.util.HashMap;
import java.util.Iterator;
import java.util.LinkedHashMap;
import java.util.List;
import java.util.Map;
import java.util.Map.Entry;
import java.util.Properties;

import org.apache.commons.lang.StringUtils;
import org.codehaus.jettison.json.JSONException;
import org.codehaus.jettison.json.JSONObject;
import org.slf4j.Logger;
import org.slf4j.LoggerFactory;

import com.datastax.driver.core.ResultSet;
import com.datastax.driver.core.Row;
import com.datastax.driver.core.Session;
import com.datastax.driver.core.Statement;
import com.datastax.driver.core.querybuilder.QueryBuilder;
import com.google.common.base.Preconditions;
import com.google.common.collect.Maps;
import com.netflix.astyanax.connectionpool.OperationResult;
import com.netflix.astyanax.cql.CqlOperationResultImpl;
import com.netflix.astyanax.ddl.ColumnFamilyDefinition;
import com.netflix.astyanax.ddl.FieldMetadata;
import com.netflix.astyanax.ddl.KeyspaceDefinition;
import com.netflix.astyanax.ddl.SchemaChangeResult;

/**
 * Impl for {@link KeyspaceDefinition} using the java driver. 
 * 
 * @author poberai
 *
 */
public class CqlKeyspaceDefinitionImpl implements KeyspaceDefinition {

	private static final Logger Log = LoggerFactory.getLogger(CqlKeyspaceDefinitionImpl.class);
	
	private final Session session; 
	private boolean alterKeyspace; 
	private final Map<String, Object> options = new HashMap<String, Object>();
	private final List<CqlColumnFamilyDefinitionImpl> cfDefList = new ArrayList<CqlColumnFamilyDefinitionImpl>();
	
	public CqlKeyspaceDefinitionImpl(Session session) {
		this.session = session;
	}

	public CqlKeyspaceDefinitionImpl(Session session, Map<String, Object> input) {
		this.session = session;
		checkOptionsMap(input);
	}

	public CqlKeyspaceDefinitionImpl(Session session, Properties props) {
		this.session = session;
		checkOptionsMap(propertiesToMap(props));
	}

	public CqlKeyspaceDefinitionImpl(Session session, Row row) {
		
		this.session = session; 
		this.setName(row.getString("keyspace_name"));
		this.setStrategyClass(row.getString("strategy_class"));
		this.setStrategyOptionsMap(parseStrategyOptions(row.getString("strategy_options")));
		this.options.put("durable_writes", row.getBool("durable_writes"));
	}
	
	public CqlKeyspaceDefinitionImpl alterKeyspace() {
		alterKeyspace = true;
		return this;
	}


	@Override
	public CqlKeyspaceDefinitionImpl setName(String name) {
		this.options.put("name", name.toLowerCase());
		return this;
	}

	@Override
	public String getName() {
		return (String) options.get("name");
	}

	@Override
	public CqlKeyspaceDefinitionImpl setStrategyClass(String strategyClass) {
		getOrCreateReplicationMap().put("class", strategyClass);
		return this;
	}

	@Override
	public String getStrategyClass() {
		return (String) getOrCreateReplicationMap().get("class");
	}

	@Override
	public CqlKeyspaceDefinitionImpl setStrategyOptions(Map<String, String> strategyOptions) {
		getOrCreateReplicationMap().putAll(strategyOptions);
		return this;
	}
	
	public CqlKeyspaceDefinitionImpl setStrategyOptionsMap(Map<String, Object> strategyOptions) {
		getOrCreateReplicationMap().putAll(strategyOptions);
		return this;
	}

	
	@Override
	public CqlKeyspaceDefinitionImpl addStrategyOption(String name, String value) {
		this.getOrCreateReplicationMap().put(name, value);
		return this;
	}

	@Override
	public Map<String, String> getStrategyOptions() {
		Map<String, String> map = new HashMap<String, String>();
		Map<String, Object> repMap = getOrCreateReplicationMap();
		for (String key : repMap.keySet()) {
			map.put(key, (String) repMap.get(key));
		}
		return map;
	}

	@Override
	public List<ColumnFamilyDefinition> getColumnFamilyList() {
		Statement query = QueryBuilder.select().all()
				.from("system", "schema_columnfamilies")
				.where(eq("keyspace_name", getName()));
				
		ResultSet rs = session.execute(query);
		List<ColumnFamilyDefinition> cfDefs = new ArrayList<ColumnFamilyDefinition>();
		List<Row> rows = rs.all();
		if (rows != null) {
			for (Row row : rows) {
				cfDefs.add(new CqlColumnFamilyDefinitionImpl(session, row));
			}
		}
		return cfDefs;
	}

	@Override
	public ColumnFamilyDefinition getColumnFamily(String columnFamilyName) {
		
		Statement query = QueryBuilder.select().all()
				.from("system", "schema_columnfamilies")
				.where(eq("keyspace_name", getName()))
				.and(eq("columnfamily_name", columnFamilyName.toLowerCase()));

		Row row = session.execute(query).one();
		
		if (row == null) {
			throw new RuntimeException("CF not found: " + columnFamilyName);
		}
		return new CqlColumnFamilyDefinitionImpl(session, row);
	}

	@Override
	public KeyspaceDefinition addColumnFamily(ColumnFamilyDefinition cfDef) {
		CqlColumnFamilyDefinitionImpl cqlCfDef = (CqlColumnFamilyDefinitionImpl) cfDef; 
		cqlCfDef.execute();
		return this;
	}

	@Override
	public Collection<String> getFieldNames() {
		return options.keySet();
	}

	@Override
	public Object getFieldValue(String name) {
		return options.get(name);
	}

	@Override
	public KeyspaceDefinition setFieldValue(String name, Object value) {
		this.options.put(name, value);
		return this;
	}

	@Override
	public Collection<FieldMetadata> getFieldsMetadata() {
		
		List<FieldMetadata> list = new ArrayList<FieldMetadata>();
		
		for (String key : options.keySet()) {
			Object value = options.get(key);
			
			Class<?> clazz = value.getClass();
			
			String name = key.toUpperCase();
			String type = clazz.getSimpleName().toUpperCase();
			boolean isContainer = Collection.class.isAssignableFrom(clazz) || Map.class.isAssignableFrom(clazz);
			list.add(new FieldMetadata(name, type, isContainer));
		}
		return list;
	}

	@Override
	public void setFields(Map<String, Object> optionsMap) {
		checkOptionsMap(optionsMap);
	}

	@Override
	public Properties getProperties() throws Exception {
		return mapToProperties(options);
	}

	@Override
	public void setProperties(Properties props) throws Exception {
		options.clear();
		options.putAll(propertiesToMap(props));
	}
	
	
	public OperationResult<SchemaChangeResult> execute() {
		
		String query = getQuery();

		if (Log.isDebugEnabled()) {
			Log.debug("Query : " + query);
		}
		
		CqlOperationResultImpl<SchemaChangeResult> result = new CqlOperationResultImpl<SchemaChangeResult>(session.execute(query), null);
		
		for (CqlColumnFamilyDefinitionImpl cfDef : cfDefList) {
			cfDef.execute();
		}
		
		return result;
	}

	
	private String getQuery() {
		
		String cmd = (alterKeyspace) ? "ALTER" : "CREATE";
		
		StringBuilder sb = new StringBuilder(cmd); 
		sb.append(" KEYSPACE ");
		sb.append(getName());
		
		Map<String, Object> replicationOptions = (Map<String, Object>) options.get("replication");
		appendReplicationOptions(sb, replicationOptions);
		
		Object durableWrites = options.get("durable_writes");
		if (durableWrites != null) {
			sb.append(" AND durable_writes = ").append(durableWrites);
		}
		return sb.toString();
	}
	
	private void appendReplicationOptions(StringBuilder sb, Map<String, Object> replicationOptions) {

		if (replicationOptions == null || replicationOptions.size() == 0) {
			throw new RuntimeException("Missing properties for 'replication'");
		}

		sb.append(" WITH replication = {" );

		Iterator<Entry<String, Object>> iter = replicationOptions.entrySet().iterator();
		
		while (iter.hasNext()) {
			
			Entry<String, Object> entry = iter.next();
			sb.append("'").append(entry.getKey()).append("' : '").append(entry.getValue()).append("'");
			if (iter.hasNext()) {
				sb.append(", ");
			}
		}
		
		sb.append("}");
	}
	
	
	private void checkOptionsMap(Map<String, Object> input) {
		
		Object strategyOptions = input.get("strategy_options");
		
		if (strategyOptions == null) {
			Preconditions.checkArgument(input.get("replication") != null, "Invalid CREATE KEYSPACE properties");
			options.clear();
			options.putAll(input);
			
		} else {
			
			// this is an old style map. Convert to the new spec of CREATE KEYSPACE
			options.clear();
			
			Map<String, Object> replicationOptions = new HashMap<String, Object>();
			options.put("replication", replicationOptions);
			
			Map<String, Object> oldStrategyOptions = (Map<String, Object>) input.get("strategy_options");
			replicationOptions.putAll(oldStrategyOptions);

			String strategyClass = (String) input.get("strategy_class");
			replicationOptions.put("class", strategyClass);
		}
	}
	
	private Map<String, Object> getOrCreateReplicationMap() {
		Map<String, Object> replicationMap = (Map<String, Object>) options.get("replication");
		if (replicationMap == null) {
			replicationMap = new HashMap<String, Object>();
			options.put("replication", replicationMap);
		}
		return replicationMap;
	}
	
	private static Map<String, Object> propertiesToMap(Properties props) {
		Map<String, Object> root = Maps.newTreeMap();
		for (Entry<Object, Object> prop : props.entrySet()) {
			String[] parts = StringUtils.split((String)prop.getKey(), ".");
			Map<String, Object> node = root;
			for (int i = 0; i < parts.length - 1; i++) {
				if (!node.containsKey(parts[i])) {
					node.put(parts[i], new LinkedHashMap<String, Object>());
				}
				node = (Map<String, Object>)node.get(parts[i]);
			}
			node.put(parts[parts.length-1], (String)prop.getValue());
		}
		return root;
	}


	private static Properties mapToProperties(Map<String, Object> map) {
		
		Properties props = new Properties();
		addProperties(props, null, map);
		return props;
	}
	
	private static void addProperties(Properties props, String prefix, Map<String, Object> subMap) {
		
		for (Entry<String, Object> entry : subMap.entrySet()) {
			
			String key = (prefix != null) ? prefix + "." + entry.getKey() : entry.getKey();
			if (entry.getValue() instanceof Map) {
				addProperties(props, key, (Map<String, Object>) entry.getValue());
			} else {
				props.put(key, entry.getValue().toString());
			}
		}
	}
	
	private Map<String, Object> parseStrategyOptions(String jsonString) {
		
		if (jsonString == null || jsonString.isEmpty()) {
			return null;
		}
		
		Map<String, Object> map = new HashMap<String, Object>();
		try {
			JSONObject json = new JSONObject(jsonString);
			Iterator<String> iter = json.keys();
			while (iter.hasNext()) {
				String key = iter.next();
				Object obj = json.get(key);
				map.put(key, obj);
			}
			return map;
		} catch (JSONException e) {
			throw new RuntimeException(e);
		}
	}
	

	public String toString() {
		return "CqlKeyspaceDefinition=[ " + options.toString() + " ]";
	}

}>>>>>>> YOURS
/home/ramdisk/experiment3/projects/astyanax/revisions/rev_2b15fd9_3de01e3/rev_2b15fd9-3de01e3/astyanax-cql/src/main/java/com/netflix/astyanax/cql/schema/CqlColumnFamilyDefinitionImpl.java;<<<<<<< MINE
=======
package com.netflix.astyanax.cql.schema;

import static com.datastax.driver.core.querybuilder.QueryBuilder.eq;

import java.nio.ByteBuffer;
import java.util.ArrayList;
import java.util.Collection;
import java.util.Collections;
import java.util.HashMap;
import java.util.Iterator;
import java.util.LinkedHashMap;
import java.util.List;
import java.util.Map;
import java.util.Map.Entry;
import java.util.Properties;
import java.util.regex.Pattern;

import org.apache.commons.lang.StringUtils;
import org.codehaus.jettison.json.JSONException;
import org.codehaus.jettison.json.JSONObject;

import com.datastax.driver.core.ColumnDefinitions.Definition;
import com.datastax.driver.core.DataType;
import com.datastax.driver.core.ResultSet;
import com.datastax.driver.core.Row;
import com.datastax.driver.core.Session;
import com.datastax.driver.core.Statement;
import com.datastax.driver.core.querybuilder.QueryBuilder;
import com.google.common.base.Preconditions;
import com.google.common.collect.Maps;
import com.netflix.astyanax.connectionpool.OperationResult;
import com.netflix.astyanax.cql.CqlOperationResultImpl;
import com.netflix.astyanax.cql.reads.CFRowQueryGen;
import com.netflix.astyanax.cql.util.CqlTypeMapping;
import com.netflix.astyanax.cql.util.DataTypeMapping;
import com.netflix.astyanax.cql.writes.CFMutationQueryGen;
import com.netflix.astyanax.ddl.ColumnDefinition;
import com.netflix.astyanax.ddl.ColumnFamilyDefinition;
import com.netflix.astyanax.ddl.FieldMetadata;
import com.netflix.astyanax.ddl.SchemaChangeResult;
import com.netflix.astyanax.model.ColumnFamily;
import com.netflix.astyanax.serializers.AnnotatedCompositeSerializer;
import com.netflix.astyanax.serializers.AnnotatedCompositeSerializer.ComponentSerializer;

/**
 * Impl for {@link ColumnFamilyDefinition} interface that constructs it's state from the java driver {@link ResultSet}
 * 
 * @author poberai
 *
 */
public class CqlColumnFamilyDefinitionImpl implements ColumnFamilyDefinition {

	private Session session; 
	
	private String cfName; 
	private String keyspaceName;
	
	private Map<String, Object> optionsMap = new HashMap<String, Object>();
	
	private List<ColumnDefinition> partitionKeyList = new ArrayList<ColumnDefinition>();
	private List<ColumnDefinition> clusteringKeyList = new ArrayList<ColumnDefinition>();
	private List<ColumnDefinition> regularColumnList = new ArrayList<ColumnDefinition>();
	private List<ColumnDefinition> allColumnsDefinitionList = new ArrayList<ColumnDefinition>();
	
	private String[] allPkColNames;
	
	private AnnotatedCompositeSerializer<?> compositeSerializer = null; 
	
	private boolean alterTable = false;

	private CFMutationQueryGen mutationQueryGen = null;
	private CFRowQueryGen rowQueryGen = null;
	
	public CqlColumnFamilyDefinitionImpl(Session session) {
		this.session = session;
	}

	public CqlColumnFamilyDefinitionImpl(Session session, String keyspace, Properties props) {
		this(session, keyspace, propertiesToMap(props));
	}

	public CqlColumnFamilyDefinitionImpl(Session session, String keyspace, Map<String, Object> options) {
		this.session = session;
		this.keyspaceName = keyspace;
		
		if (options == null) {
			options = new HashMap<String, Object>();
		}
		initFromMap(options);
	}
	
	public CqlColumnFamilyDefinitionImpl(Session session, Row row) {
		initFromResultSet(session, row);
		mutationQueryGen = new CFMutationQueryGen(session, keyspaceName, this);
		rowQueryGen = new CFRowQueryGen(session, keyspaceName, this);
	}

	public CqlColumnFamilyDefinitionImpl(Session session, String keyspace, ColumnFamily<?, ?> columnFamily, Map<String, Object> options) {
		this.session = session;
		
		Preconditions.checkArgument(columnFamily != null, "ColumnFamily cannot be null");

		if (options == null) {
			options = new HashMap<String, Object>();
		}
		
		keyspaceName = keyspace;
		cfName = columnFamily.getName();

		optionsMap.put("key_validator", columnFamily.getKeySerializer().getComparatorType().getClassName());
		optionsMap.put("comparator", columnFamily.getColumnSerializer().getComparatorType().getClassName());
		optionsMap.put("default_validator", columnFamily.getDefaultValueSerializer().getComparatorType().getClassName());
		
		if (columnFamily.getColumnSerializer() instanceof AnnotatedCompositeSerializer) {
			compositeSerializer = (AnnotatedCompositeSerializer<?>) columnFamily.getColumnSerializer();
		}
		initFromMap(options);
	}
	
	
	private void initFromMap(Map<String, Object> options) {
		
		String cName = (String) options.get("name");
		if (cName != null) {
			cfName = cName;
			options.remove("name");
		}
		
		String kName = (String) options.get("keyspace");
		if (kName != null) {
			keyspaceName = kName;
		}
		
		this.optionsMap.putAll(options);
		
		if (optionsMap.containsKey("key_validation_class")) {
			optionsMap.put("key_validator", optionsMap.remove("key_validation_class"));
		}
		if (optionsMap.containsKey("comparator_type")) {
			optionsMap.put("comparator", optionsMap.remove("comparator_type"));
		}
		if (optionsMap.containsKey("default_validation_class")) {
			optionsMap.put("default_validator", optionsMap.remove("default_validation_class"));
		}
	}
	

	
	private void initFromResultSet(Session session, Row row) {

		if (row == null) {
			throw new RuntimeException("Result Set is empty");
		}

		this.session = session;

		this.keyspaceName = row.getString("keyspace_name");
		this.cfName = row.getString("columnfamily_name");

		List<Definition> colDefs = row.getColumnDefinitions().asList();
		 for (Definition colDef : colDefs) {
			 
			 String colName = colDef.getName();
			 DataType dataType = colDef.getType();
			 Object value = DataTypeMapping.getDynamicColumn(row, colName, dataType);
			 optionsMap.put(colName, value);
		 }
		readColDefinitions();
	}

	private void processCompositeComparator() {

		int colIndex = 1;
		for (ComponentSerializer<?> componentSerializer : compositeSerializer.getComponents()) {
			String type = CqlTypeMapping.getCqlType(componentSerializer.getSerializer().getComparatorType().getTypeName());
			ColumnDefinition column = new CqlColumnDefinitionImpl().setName("column" + colIndex++).setValidationClass(type);
			clusteringKeyList.add(column);
		}
	}
	
	private void processCompositeComparatorSpec(String comparatorSpec) {
		// e.g  CompositeType(UTF8Type, LongType, UTF8Type)
		
		String regex = "[\\(,\\)]";
		Pattern pattern = Pattern.compile(regex);
		String[] parts = pattern.split(comparatorSpec);

		int colIndex = 1;
		for (int i=1; i<parts.length; i++) {
			String componentTypeString = parts[i].trim();
			String type = CqlTypeMapping.getCqlType(componentTypeString);
			ColumnDefinition column = new CqlColumnDefinitionImpl().setName("column" + colIndex++).setValidationClass(type);
			clusteringKeyList.add(column);
		}
	}
	


	private void createColumnDefinitions() {

		String keyClass = (String) optionsMap.remove("key_validator");
		keyClass = (keyClass == null) ?	keyClass = "blob" : keyClass;
		
		String comparatorClass = (String) optionsMap.remove("comparator");
		comparatorClass = (comparatorClass == null) ?	comparatorClass = "blob" : comparatorClass;
		
		String dataValidationClass = (String) optionsMap.remove("default_validator");
		dataValidationClass = (dataValidationClass == null) ?	dataValidationClass = "blob" : dataValidationClass;

		ColumnDefinition key = new CqlColumnDefinitionImpl().setName("key").setValidationClass(keyClass);
		partitionKeyList.add(key);

		if (compositeSerializer != null) {
			processCompositeComparator();
		} else if (comparatorClass.contains("CompositeType")) {
			processCompositeComparatorSpec(comparatorClass);
		} else {
			ColumnDefinition column1 = new CqlColumnDefinitionImpl().setName("column1").setValidationClass(comparatorClass);
			clusteringKeyList.add(column1);
		}

		ColumnDefinition valueColumn = new CqlColumnDefinitionImpl().setName("value").setValidationClass(dataValidationClass);
		this.regularColumnList.add(valueColumn);
	}
	
	private void readColDefinitions() {
		
		// VALUE COLUMNS AND COLUMNS THAT ARE NOT PART OF THE PRIMARY KEY
		Statement query = QueryBuilder.select().from("system", "schema_columns")
				.where(eq("keyspace_name", keyspaceName))
				.and(eq("columnfamily_name", cfName));
		
		ResultSet rs = session.execute(query);
		List<Row> rows = rs.all();
		if (rows != null && rows.size() > 0) {
			
			List<CqlColumnDefinitionImpl> tmpList = new ArrayList<CqlColumnDefinitionImpl>();
			
			for (Row row : rows) {
				CqlColumnDefinitionImpl colDef = new CqlColumnDefinitionImpl(row);
				switch (colDef.getColumnType()) {
				case partition_key:
					partitionKeyList.add(colDef);
					allColumnsDefinitionList.add(colDef);
					break;
				case clustering_key:
					tmpList.add(colDef);
					allColumnsDefinitionList.add(colDef);
					break;
				case regular:
					regularColumnList.add(colDef);
					allColumnsDefinitionList.add(colDef);
					break;
				case compact_value:
					regularColumnList.add(colDef);
					allColumnsDefinitionList.add(colDef);
					break;
				}
			}

			Collections.sort(tmpList);
			clusteringKeyList.addAll(tmpList);
			tmpList = null;
			
			List<String> allPrimaryKeyColNames = new ArrayList<String>();
			for (ColumnDefinition colDef : partitionKeyList) {
				allPrimaryKeyColNames.add(colDef.getName());
			}
			for (ColumnDefinition colDef : clusteringKeyList) {
				allPrimaryKeyColNames.add(colDef.getName());
			}
			
			allPkColNames = allPrimaryKeyColNames.toArray(new String[allPrimaryKeyColNames.size()]);
		}
	}

	public CqlColumnFamilyDefinitionImpl alterTable() {
		alterTable = true;
		return this;
	}

	@Override
	public ColumnFamilyDefinition setComment(String comment) {
		optionsMap.put("comment", "'" + comment + "'");
		return this;
	}

	@Override
	public String getComment() {
		return (String) optionsMap.get("comment");
	}

	@Override
	public ColumnFamilyDefinition setKeyspace(String keyspace) {
		keyspaceName = keyspace;
		return this;
	}

	@Override
	public String getKeyspace() {
		return keyspaceName;
	}

	@Override
	@Deprecated
	public ColumnFamilyDefinition setMemtableFlushAfterMins(Integer value) {
		throw new UnsupportedOperationException("Operation not supported");
	}

	@Override
	@Deprecated
	public Integer getMemtableFlushAfterMins() {
		throw new UnsupportedOperationException("Operation not supported");
	}

	@Override
	@Deprecated
	public ColumnFamilyDefinition setMemtableOperationsInMillions(Double value) {
		throw new UnsupportedOperationException("Operation not supported");
	}

	@Override
	@Deprecated
	public Double getMemtableOperationsInMillions() {
		throw new UnsupportedOperationException("Operation not supported");
	}

	@Override
	@Deprecated
	public ColumnFamilyDefinition setMemtableThroughputInMb(Integer value) {
		throw new UnsupportedOperationException("Operation not supported");
	}

	@Override
	@Deprecated
	public Integer getMemtableThroughputInMb() {
		throw new UnsupportedOperationException("Operation not supported");
	}

	@Override
	public ColumnFamilyDefinition setMergeShardsChance(Double value) {
		throw new UnsupportedOperationException("Operation not supported");
	}

	@Override
	public Double getMergeShardsChance() {
		throw new UnsupportedOperationException("Operation not supported");
	}

	@Override
	public ColumnFamilyDefinition setMinCompactionThreshold(Integer value) {
		optionsMap.put("min_compaction_threshold", value);
		return this;
	}

	@Override
	public Integer getMinCompactionThreshold() {
		return (Integer) optionsMap.get("min_compaction_threshold");
	}

	@Override
	public ColumnFamilyDefinition setMaxCompactionThreshold(Integer value) {
		optionsMap.put("max_compaction_threshold", value);
		return this;
	}

	@Override
	public Integer getMaxCompactionThreshold() {
		return (Integer) optionsMap.get("max_compaction_threshold");
	}

	@Override
	public ColumnFamilyDefinition setCompactionStrategy(String strategy) {
		optionsMap.put("compaction_strategy_class", strategy);
		return this;
	}

	@Override
	public String getCompactionStrategy() {
		return (String) optionsMap.get("compaction_strategy_class");
	}

	@Override
	public ColumnFamilyDefinition setCompactionStrategyOptions(Map<String, String> options) {
		optionsMap.put("compaction_strategy_options", toJsonString(options));
		return this;
	}

	@Override
	public Map<String, String> getCompactionStrategyOptions() {
		return fromJsonString((String) optionsMap.get("compaction_strategy_options"));
	}

	@Override
	public ColumnFamilyDefinition setCompressionOptions(Map<String, String> options) {
		optionsMap.put("compression_parameters", toJsonString(options));
		return this;
	}

	@Override
	public Map<String, String> getCompressionOptions() {
		return fromJsonString((String) optionsMap.get("compression_parameters"));
	}

	
	@Override
	public ColumnFamilyDefinition setBloomFilterFpChance(Double chance) {
		optionsMap.put("bloom_filter_fp_chance", chance);
		return this;
	}

	@Override
	public Double getBloomFilterFpChance() {
		return (Double) optionsMap.get("bloom_filter_fp_chance");
	}

	@Override
	public ColumnFamilyDefinition setCaching(String caching) {
		optionsMap.put("caching", caching);
		return this;
	}

	@Override
	public String getCaching() {
		return (String) optionsMap.get("caching");
	}

	@Override
	public ColumnFamilyDefinition setName(String name) {
		cfName = name;
		return this;
	}

	@Override
	public String getName() {
		return cfName;
	}

	@Override
	public ColumnFamilyDefinition setReadRepairChance(Double value) {
		optionsMap.put("read_repair_chance", value);
		return this;
	}

	@Override
	public Double getReadRepairChance() {
		return (Double) optionsMap.get("read_repair_chance");
	}

	@Override
	public ColumnFamilyDefinition setLocalReadRepairChance(Double value) {
		optionsMap.put("local_read_repair_chance", value);
		return this;
	}

	@Override
	public Double getLocalReadRepairChance() {
		return (Double) optionsMap.get("local_read_repair_chance");
	}

	@Override
	public ColumnFamilyDefinition setReplicateOnWrite(Boolean value) {
		optionsMap.put("replicate_on_write", value);
		return this;
	}

	@Override
	public Boolean getReplicateOnWrite() {
		return (Boolean) optionsMap.get("replicate_on_write");
	}

	@Override
	public ColumnFamilyDefinition setRowCacheProvider(String value) {
		throw new UnsupportedOperationException("Operation not supported");
	}

	@Override
	public String getRowCacheProvider() {
		throw new UnsupportedOperationException("Operation not supported");
	}

	@Override
	public ColumnFamilyDefinition setRowCacheSavePeriodInSeconds(Integer value) {
		throw new UnsupportedOperationException("Operation not supported");
	}

	@Override
	public Integer getRowCacheSavePeriodInSeconds() {
		throw new UnsupportedOperationException("Operation not supported");
	}

	@Override
	public ColumnFamilyDefinition setRowCacheSize(Double size) {
		throw new UnsupportedOperationException("Operation not supported");
	}

	@Override
	public Double getRowCacheSize() {
		throw new UnsupportedOperationException("Operation not supported");
	}

	@Override
	public ColumnFamilyDefinition setComparatorType(String value) {
		optionsMap.put("comparator", value);
		return this;
	}

	@Override
	public String getComparatorType() {
		return (String) optionsMap.get("comparator");
	}

	@Override
	public ColumnFamilyDefinition setDefaultValidationClass(String value) {
		optionsMap.put("default_validator", value);
		return this;
	}

	@Override
	public String getDefaultValidationClass() {
		return (String) optionsMap.get("default_validator");
	}

	@Override
	public ColumnFamilyDefinition setId(Integer id) {
		optionsMap.put("id", id);
		return this;
	}

	@Override
	public Integer getId() {
		return (Integer) optionsMap.get("id");
	}

	@Override
	public ColumnFamilyDefinition setKeyAlias(ByteBuffer alias) {
		throw new UnsupportedOperationException("Operation not supported");
	}
	
	@Override
	public ByteBuffer getKeyAlias() {
		return null;
	}

	@Override
	public ColumnFamilyDefinition setKeyCacheSavePeriodInSeconds(Integer value) {
		throw new UnsupportedOperationException("Operation not supported");
	}

	@Override
	public Integer getKeyCacheSavePeriodInSeconds() {
		throw new UnsupportedOperationException("Operation not supported");
	}

	@Override
	public ColumnFamilyDefinition setKeyCacheSize(Double keyCacheSize) {
		throw new UnsupportedOperationException("Operation not supported");
	}

	@Override
	public Double getKeyCacheSize() {
		throw new UnsupportedOperationException("Operation not supported");
	}

	@Override
	public ColumnFamilyDefinition setKeyValidationClass(String keyValidationClass) {
		optionsMap.put("key_validator", keyValidationClass);
		return this;
	}
	
	@Override
	public String getKeyValidationClass() {
		return (String) optionsMap.get("key_validator");
	}

	public ColumnDefinition getPartitionKeyColumnDefinition() {
		return partitionKeyList.get(0);
	}

	public List<ColumnDefinition> getRegularColumnDefinitionList() {
		return regularColumnList;
	}

	public List<ColumnDefinition> getPartitionKeyColumnDefinitionList() {
		return partitionKeyList;
	}
	
	public List<ColumnDefinition> getClusteringKeyColumnDefinitionList() {
		return clusteringKeyList;
	}

	public String[] getAllPkColNames() {
		return allPkColNames;
	}

	@Override
	public ColumnFamilyDefinition addColumnDefinition(ColumnDefinition def) {
		throw new UnsupportedOperationException("Operation not supported");
	}

	@Override
	public ColumnDefinition makeColumnDefinition() {
		throw new UnsupportedOperationException("Operation not supported");
	}

	@Override
	public void clearColumnDefinitionList() {
		throw new UnsupportedOperationException("Operation not supported");
	}

	@Override
	public Collection<String> getFieldNames() {
		return optionsMap.keySet();
	}

	@Override
	public Object getFieldValue(String name) {
		return optionsMap.get(name);
	}

	@Override
	public ColumnFamilyDefinition setFieldValue(String name, Object value) {
		optionsMap.put(name, value);
		return this;
	}

	@Override
	public ColumnFamilyDefinition setGcGraceSeconds(Integer seconds) {
		optionsMap.put("gc_grace_seconds", seconds);
		return this;
	}

	@Override
	public Integer getGcGraceSeconds() {
		return (Integer) optionsMap.get("gc_grace_seconds");
	}

	@Override
	public Collection<FieldMetadata> getFieldsMetadata() {
		List<FieldMetadata> list = new ArrayList<FieldMetadata>();
		
		for (String key : optionsMap.keySet()) {
			Object value = optionsMap.get(key);
			
			Class<?> clazz = value.getClass();
			
			String name = key.toUpperCase();
			String type = clazz.getSimpleName().toUpperCase();
			boolean isContainer = Collection.class.isAssignableFrom(clazz) || Map.class.isAssignableFrom(clazz);
			list.add(new FieldMetadata(name, type, isContainer));
		}
		return list;
	}

	@Override
	public void setFields(Map<String, Object> options) {
		optionsMap.putAll(options);
	}

	@Override
	public Properties getProperties() {
		Properties props = new Properties();
		for (String key : optionsMap.keySet()) {
			if (optionsMap.get(key) != null) {
				props.put(key, optionsMap.get(key));
			}
		}
		return props;
	}

	@Override
	public void setProperties(Properties additionalProperties) throws Exception {
		
		Map<String, Object> props = propertiesToMap(additionalProperties);
		optionsMap.putAll(props);
	}

	
	public OperationResult<SchemaChangeResult> execute() {
		
		createColumnDefinitions();
		
		String query = (alterTable) ? getUpdateQuery() : getCreateQuery();
		ResultSet rs = session.execute(query);

		return new CqlOperationResultImpl<SchemaChangeResult>(rs, null);
	}
	
	private String getCreateQuery() {
		StringBuilder sb = new StringBuilder("CREATE TABLE ");
		sb.append(keyspaceName).append(".").append(cfName);
		sb.append(" ( ");
		
		boolean compositePrimaryKey = clusteringKeyList.size() > 0;
		
		if (!compositePrimaryKey) {
			
			appendColDefinition(sb, partitionKeyList.iterator());
			sb.append(" PRIMARY KEY, ");
			appendColDefinition(sb, regularColumnList.iterator());
			
		} else {
			appendColDefinition(sb, partitionKeyList.iterator());
			sb.append(" ,");
			appendColDefinition(sb, clusteringKeyList.iterator());
			sb.append(" ,");
			appendColDefinition(sb, regularColumnList.iterator());
			sb.append(", PRIMARY KEY (");
			appendPrimaryKeyDefinition(sb, partitionKeyList.iterator(), clusteringKeyList.iterator());
			sb.append(") ");
		}
		
		sb.append(")");
		
		if (optionsMap.size() > 0) {
			sb.append(" WITH ");
			
			Iterator<String> propIter = optionsMap.keySet().iterator();
			while(propIter.hasNext()) {
				
				String pKey = propIter.next();
				Object pValue = optionsMap.get(pKey);
				
				if (pValue == null) {
					continue;
				}
				
				if (pValue instanceof String) {
					sb.append(pKey).append(" = '").append(pValue).append("'");
				} else {
					sb.append(pKey).append(" = ").append(pValue);
				}
				
								
				if (propIter.hasNext()) {
					sb.append(" AND ");
				}
			}
		}
		
		String query = sb.toString();

		return query;
	}

	private String getUpdateQuery() {
		
		StringBuilder sb = new StringBuilder("ALTER TABLE ");
		sb.append(keyspaceName).append(".").append(cfName);
		
			sb.append(" WITH ");
			
			Iterator<String> propIter = optionsMap.keySet().iterator();
			while(propIter.hasNext()) {
				
				String pKey = propIter.next();
				Object pValue = optionsMap.get(pKey);
				
				sb.append(pKey).append(" = ").append(pValue);
				
				if (propIter.hasNext()) {
					sb.append(" AND ");
				}
			}
		return sb.toString();
	}

	private void appendColDefinition(StringBuilder sb, Iterator<ColumnDefinition> iter) {
		
		while (iter.hasNext()) {
			CqlColumnDefinitionImpl colDef = (CqlColumnDefinitionImpl) iter.next(); 
			sb.append(colDef.getName()).append(" ").append(colDef.getCqlType());
			if (iter.hasNext()) {
				sb.append(", ");
			}
		}
	}

	private void appendPrimaryKeyDefinition(StringBuilder sb, Iterator<ColumnDefinition> iter1, Iterator<ColumnDefinition> iter2) {
		
		while (iter1.hasNext()) {
			CqlColumnDefinitionImpl colDef = (CqlColumnDefinitionImpl) iter1.next(); 
			sb.append(colDef.getName());
			if (iter1.hasNext()) {
				sb.append(", ");
			}
		}
		if (iter2.hasNext()) {
			sb.append(", ");

			while (iter2.hasNext()) {
				CqlColumnDefinitionImpl colDef = (CqlColumnDefinitionImpl) iter2.next(); 
				sb.append(colDef.getName());
				if (iter2.hasNext()) {
					sb.append(", ");
				}
			}
		}
	}
	
	private static Map<String, Object> propertiesToMap(Properties props) {
		Map<String, Object> root = Maps.newTreeMap();
		if (props == null) {
			return root;
		}
		for (Entry<Object, Object> prop : props.entrySet()) {
			String[] parts = StringUtils.split((String)prop.getKey(), ".");
			Map<String, Object> node = root;
			for (int i = 0; i < parts.length - 1; i++) {
				if (!node.containsKey(parts[i])) {
					node.put(parts[i], new LinkedHashMap<String, Object>());
				}
				node = (Map<String, Object>)node.get(parts[i]);
			}
			node.put(parts[parts.length-1], (String)prop.getValue());
		}
		return root;
	}
	
	private static String toJsonString(Map<String, String> options) {
		if (options == null) {
			return null;
		}
		
		JSONObject json = new JSONObject();
		for(String key : options.keySet()) {
			try {
				json.put(key, options.get(key));
			} catch (JSONException e) {
				throw new RuntimeException(e);
			}
		}
		return json.toString();
	}
	
	private static Map<String, String> fromJsonString(String jsonString) {
		if (jsonString == null) {
			return new HashMap<String, String>();
		}
		try {
			JSONObject json = new JSONObject(jsonString);
			Map<String, String> map = new HashMap<String, String>();
			Iterator<String> iter = json.keys();
			while(iter.hasNext()) {
				String key = iter.next();
				String value = json.getString(key).toString();
				map.put(key, value);
			}
			return map;
		} catch (JSONException e) {
			throw new RuntimeException(e);
		}
	}

	@Override
	public List<ColumnDefinition> getColumnDefinitionList() {
		return allColumnsDefinitionList;
	}
	
	public CFMutationQueryGen getMutationQueryGenerator() {
		return mutationQueryGen;
	}
	
	public CFRowQueryGen getRowQueryGenerator() {
		return rowQueryGen;
	}
	
	public void printOptionsMap() {
		for (String key : optionsMap.keySet()) {
			System.out.println(key + " " +  optionsMap.get(key));
		}
	}
}>>>>>>> YOURS
/home/ramdisk/experiment3/projects/astyanax/revisions/rev_2b15fd9_3de01e3/rev_2b15fd9-3de01e3/astyanax-cql/src/main/java/com/netflix/astyanax/cql/writes/CqlStyleMutationQuery.java;<<<<<<< MINE
=======
package com.netflix.astyanax.cql.writes;

import java.util.List;
import java.util.concurrent.atomic.AtomicReference;

import com.netflix.astyanax.cql.CqlKeyspaceImpl.KeyspaceContext;
import com.netflix.astyanax.cql.schema.CqlColumnFamilyDefinitionImpl;
import com.netflix.astyanax.cql.util.CFQueryContext;
import com.netflix.astyanax.model.ColumnFamily;
import com.netflix.astyanax.model.ConsistencyLevel;

public class CqlStyleMutationQuery {

	protected final KeyspaceContext ksContext;
	protected final CFQueryContext<?,?> cfContext;
	protected final List<CqlColumnMutationImpl<?,?>> mutationList;
	
	protected AtomicReference<Boolean> deleteRow;
	
	protected final AtomicReference<Long> defaultTimestamp;
	protected final AtomicReference<Integer> defaultTTL; 
	protected final ConsistencyLevel consistencyLevel;

	private static final String USING = " USING ";
	private static final String TTL = " TTL ";
	private static final String AND = " AND";
	private static final String TIMESTAMP = " TIMESTAMP ";
	
	public CqlStyleMutationQuery(KeyspaceContext ksCtx, CFQueryContext<?,?> cfCtx, 
								 List<CqlColumnMutationImpl<?,?>> mutationList, AtomicReference<Boolean> deleteRow, 
								 AtomicReference<Integer> ttl, AtomicReference<Long> timestamp, ConsistencyLevel consistencyLevel) {
		
		this.ksContext = ksCtx;
		this.cfContext = cfCtx;
		
		this.mutationList = mutationList;
		this.deleteRow = deleteRow;
		this.defaultTTL = ttl;
		this.defaultTimestamp = timestamp;
		this.consistencyLevel = consistencyLevel;
		
		if (this.consistencyLevel != null) {
			cfContext.setConsistencyLevel(consistencyLevel);
		}
	}
	
	public String getDeleteEntireRowQuery() {
		ColumnFamily<?,?> cf = cfContext.getColumnFamily();
		CqlColumnFamilyDefinitionImpl cfDef = (CqlColumnFamilyDefinitionImpl)cf.getColumnFamilyDefinition();
		return "DELETE FROM " + ksContext.getKeyspace() + "." + cf.getName() + 
				" WHERE " + cfDef.getPartitionKeyColumnDefinition().getName() + " = ?;";
	}

	public void appendWriteOptions(StringBuilder sb, Integer overrideTTL, Long overrideTimestamp) {
		
		Integer ttl = overrideTTL != null ? overrideTTL : defaultTTL.get();
		Long timestamp = overrideTimestamp != null ? overrideTimestamp : defaultTimestamp.get();
				
		if (ttl != null || timestamp != null) {
			sb.append(USING);
		}
		
		if (ttl != null) {
			sb.append(TTL + ttl);
		}
		
		if (timestamp != null) {
			if (ttl != null) {
				sb.append(AND);
			}
			sb.append(TIMESTAMP + timestamp);
		}
	}
	
}>>>>>>> YOURS
/home/ramdisk/experiment3/projects/astyanax/revisions/rev_2b15fd9_3de01e3/rev_2b15fd9-3de01e3/astyanax-cql/src/main/java/com/netflix/astyanax/cql/writes/BatchedStatements.java;<<<<<<< MINE
=======
package com.netflix.astyanax.cql.writes;

import java.util.ArrayList;
import java.util.List;

import org.slf4j.Logger;
import org.slf4j.LoggerFactory;

import com.datastax.driver.core.BoundStatement;
import com.datastax.driver.core.PreparedStatement;
import com.datastax.driver.core.Session;

public class BatchedStatements {

	private static final Logger LOG = LoggerFactory.getLogger(BatchedStatements.class);
	
	private List<String> batchQueries = new ArrayList<String>(); 
	private List<Object> batchValues = new ArrayList<Object>();
	
	public BatchedStatements() {
	}
	
	public List<String> getBatchQueries() {
		return this.batchQueries; 
	}
	
	public List<Object> getBatchValues() {
		return this.batchValues;
	}
	
	public void addBatchQuery(String query) {
		batchQueries.add(query);
	}
	
	public void addBatchValues(List<Object> values) {
		batchValues.addAll(values);
	}
	
	public void addBatchValues(Object ... values) {
		for (Object value : values) {
			batchValues.add(value);
		}
	}

	public void addBatch(String query, Object ... values) {
		batchQueries.add(query);
		for (Object value : values) {
			batchValues.add(value);
		}
	}

	public void addBatch(String query, List<Object> values) {
		batchQueries.add(query);
		batchValues.addAll(values);
	}
	
	public void addBatch(BatchedStatements otherBatch) {
		batchQueries.addAll(otherBatch.getBatchQueries());
		batchValues.addAll(otherBatch.getBatchValues());
	}

	public BoundStatement getBoundStatement(Session session, boolean atomicBatch) {
		
		String query = getBatchQuery(atomicBatch);
		PreparedStatement statement = session.prepare(query);
		
		BoundStatement boundStatement = new BoundStatement(statement);

		Object[] valueArr = batchValues.toArray();
		boundStatement.bind(valueArr);
		
		return boundStatement;
	}
	
	public String getBatchQuery(boolean atomicBatch) {
		StringBuilder sb = new StringBuilder();
		
		boolean isBatch = batchQueries.size() > 1;
		
		if (isBatch) {
			if (atomicBatch) {
				sb.append("BEGIN BATCH ");
			} else {
				sb.append("BEGIN UNLOGGED BATCH ");
			}
		}
		for (String query : batchQueries) {
			sb.append(query);
		}
		
		if (isBatch) {
			sb.append(" APPLY BATCH; ");
		}
		
		String query = sb.toString(); 
		
		if (LOG.isDebugEnabled()) {
			LOG.debug("Query : " + query);
			LOG.debug("Bind values: " + batchValues);
		}
		return query;
	}
}>>>>>>> YOURS
/home/ramdisk/experiment3/projects/astyanax/revisions/rev_2b15fd9_3de01e3/rev_2b15fd9-3de01e3/astyanax-cql/src/main/java/com/netflix/astyanax/cql/writes/AbstractColumnListMutationImpl.java;<<<<<<< MINE
=======
package com.netflix.astyanax.cql.writes;

import java.io.ByteArrayOutputStream;
import java.io.IOException;
import java.nio.ByteBuffer;
import java.util.Date;
import java.util.UUID;
import java.util.concurrent.atomic.AtomicReference;
import java.util.zip.GZIPOutputStream;

import org.apache.commons.codec.binary.StringUtils;

import com.google.common.base.Preconditions;
import com.netflix.astyanax.ColumnListMutation;
import com.netflix.astyanax.Serializer;
import com.netflix.astyanax.serializers.BooleanSerializer;
import com.netflix.astyanax.serializers.ByteBufferSerializer;
import com.netflix.astyanax.serializers.ByteSerializer;
import com.netflix.astyanax.serializers.BytesArraySerializer;
import com.netflix.astyanax.serializers.DateSerializer;
import com.netflix.astyanax.serializers.DoubleSerializer;
import com.netflix.astyanax.serializers.FloatSerializer;
import com.netflix.astyanax.serializers.IntegerSerializer;
import com.netflix.astyanax.serializers.LongSerializer;
import com.netflix.astyanax.serializers.ShortSerializer;
import com.netflix.astyanax.serializers.StringSerializer;
import com.netflix.astyanax.serializers.UUIDSerializer;

public abstract class AbstractColumnListMutationImpl<C> implements ColumnListMutation<C> {
    
	protected final AtomicReference<Long> defaultTimestamp = new AtomicReference<Long>(null);
	protected final AtomicReference<Integer> defaultTTL = new AtomicReference<Integer>(null);

    public AbstractColumnListMutationImpl(Long newTimestamp) {
        this.defaultTimestamp.set(newTimestamp);
    }
    
    @Override
    public ColumnListMutation<C> putColumn(C columnName, String value, Integer ttl) {
        return putColumn(columnName, value, StringSerializer.get(), ttl);
    }

    @Override
    public ColumnListMutation<C> putColumn(final C columnName, final String value) {
        return putColumn(columnName, value, null);
    }

    @Override
    public ColumnListMutation<C> putColumnIfNotNull(C columnName, String value) {
        if (value != null) {
            return putColumn(columnName, value);
        }
        return this;
    }

    @Override
    public ColumnListMutation<C> putColumnIfNotNull(C columnName, String value, Integer ttl) {
        if (value != null) {
            return putColumn(columnName, value, ttl);
        }
        return this;
    }

    @Override
    public ColumnListMutation<C> putColumn(C columnName, byte[] value, Integer ttl) {
        return putColumn(columnName, value, BytesArraySerializer.get(), ttl);
    }

    @Override
    public <V> ColumnListMutation<C> putColumnIfNotNull(C columnName, V value, Serializer<V> valueSerializer, Integer ttl) {
        if (value == null)
            return this;
        return putColumn(columnName, value, valueSerializer, ttl);
    }
    
    @Override
    public ColumnListMutation<C> putColumn(final C columnName, final byte[] value) {
        return putColumn(columnName, value, null);
    }

    @Override
    public ColumnListMutation<C> putColumnIfNotNull(C columnName, byte[] value) {
        if (value != null) {
            return putColumn(columnName, value);
        }
        return this;
    }

    @Override
    public ColumnListMutation<C> putColumnIfNotNull(C columnName, byte[] value, Integer ttl) {
        if (value != null) {
            return putColumn(columnName, value, ttl);
        }
        return this;
    }
    
    @Override
    public ColumnListMutation<C> putColumn(C columnName, byte value, Integer ttl) {
        return putColumn(columnName, value, ByteSerializer.get(), ttl);
    }

    @Override
    public ColumnListMutation<C> putColumn(final C columnName, final byte value) {
        return putColumn(columnName, value, null);
    }
    
    @Override
    public ColumnListMutation<C> putColumnIfNotNull(C columnName, Byte value, Integer ttl) {
        if (value != null) {
            return putColumn(columnName, value, ttl);
        }
        return this;
    }

    @Override
    public ColumnListMutation<C> putColumnIfNotNull(C columnName, Byte value) {
        if (value != null) {
            return putColumn(columnName, value);
        }
        return this;
    }
    
    @Override
    public ColumnListMutation<C> putColumn(C columnName, short value, Integer ttl) {
        return putColumn(columnName, value, ShortSerializer.get(), ttl);
    }

    @Override
    public ColumnListMutation<C> putColumn(final C columnName, final short value) {
        return putColumn(columnName, value, null);
    }
    
    @Override
    public ColumnListMutation<C> putColumnIfNotNull(C columnName, Short value, Integer ttl) {
        if (value != null) {
            return putColumn(columnName, value, ttl);
        }
        return this;
    }

    @Override
    public ColumnListMutation<C> putColumnIfNotNull(C columnName, Short value) {
        if (value != null) {
            return putColumn(columnName, value);
        }
        return this;
    }

    @Override
    public ColumnListMutation<C> putColumn(C columnName, int value, Integer ttl) {
        return putColumn(columnName, value, IntegerSerializer.get(), ttl);
    }

    @Override
    public ColumnListMutation<C> putColumn(final C columnName, final int value) {
        return putColumn(columnName, value, null);
    }

    @Override
    public ColumnListMutation<C> putColumnIfNotNull(C columnName, Integer value) {
        if (value != null) {
            return putColumn(columnName, value);
        }
        return this;
    }

    @Override
    public ColumnListMutation<C> putColumnIfNotNull(C columnName, Integer value, Integer ttl) {
        if (value != null) {
            return putColumn(columnName, value, ttl);
        }
        return this;
    }

    @Override
    public ColumnListMutation<C> putColumn(C columnName, long value, Integer ttl) {
        return putColumn(columnName, value, LongSerializer.get(), ttl);
    }

    @Override
    public ColumnListMutation<C> putColumn(final C columnName, final long value) {
        return putColumn(columnName, value, null);
    }

    @Override
    public ColumnListMutation<C> putColumnIfNotNull(C columnName, Long value) {
        if (value != null) {
            return putColumn(columnName, value);
        }
        return this;
    }

    @Override
    public ColumnListMutation<C> putColumnIfNotNull(C columnName, Long value, Integer ttl) {
        if (value != null) {
            return putColumn(columnName, value, ttl);
        }
        return this;
    }

    @Override
    public ColumnListMutation<C> putColumn(C columnName, boolean value, Integer ttl) {
        return putColumn(columnName, value, BooleanSerializer.get(), ttl);
    }

    @Override
    public ColumnListMutation<C> putColumn(final C columnName, final boolean value) {
        return putColumn(columnName, value, null);
    }

    @Override
    public ColumnListMutation<C> putColumnIfNotNull(C columnName, Boolean value) {
        if (value != null) {
            return putColumn(columnName, value);
        }
        return this;
    }

    @Override
    public ColumnListMutation<C> putColumnIfNotNull(C columnName, Boolean value, Integer ttl) {
        if (value != null) {
            return putColumn(columnName, value, ttl);
        }
        return this;
    }

    @Override
    public ColumnListMutation<C> putColumn(C columnName, ByteBuffer value, Integer ttl) {
        return putColumn(columnName, value, ByteBufferSerializer.get(), ttl);
    }

    @Override
    public ColumnListMutation<C> putColumn(final C columnName, final ByteBuffer value) {
        return putColumn(columnName, value, null);
    }
    
    @Override
    public ColumnListMutation<C> putColumnIfNotNull(C columnName, ByteBuffer value) {
        if (value != null) {
            return putColumn(columnName, value);
        }
        return this;
    }

    @Override
    public ColumnListMutation<C> putColumnIfNotNull(C columnName, ByteBuffer value, Integer ttl) {
        if (value != null) {
            return putColumn(columnName, value, ttl);
        }
        return this;
    }

    @Override
    public ColumnListMutation<C> putColumn(C columnName, Date value, Integer ttl) {
        return putColumn(columnName, value, DateSerializer.get(), ttl);
    }

    @Override
    public ColumnListMutation<C> putColumn(final C columnName, final Date value) {
        return putColumn(columnName, value, null);
    }

    @Override
    public ColumnListMutation<C> putColumnIfNotNull(C columnName, Date value) {
        if (value != null) {
            return putColumn(columnName, value);
        }
        return this;
    }

    @Override
    public ColumnListMutation<C> putColumnIfNotNull(C columnName, Date value, Integer ttl) {
        if (value != null) {
            return putColumn(columnName, value, ttl);
        }
        return this;
    }

    @Override
    public ColumnListMutation<C> putColumn(C columnName, float value, Integer ttl) {
        return putColumn(columnName, value, FloatSerializer.get(), ttl);
    }

    @Override
    public ColumnListMutation<C> putColumn(final C columnName, final float value) {
        return putColumn(columnName, value, null);
    }

    @Override
    public ColumnListMutation<C> putColumnIfNotNull(C columnName, Float value) {
        if (value != null) {
            return putColumn(columnName, value);
        }
        return this;
    }

    @Override
    public ColumnListMutation<C> putColumnIfNotNull(C columnName, Float value, Integer ttl) {
        if (value != null) {
            return putColumn(columnName, value, ttl);
        }
        return this;
    }

    @Override
    public ColumnListMutation<C> putColumn(C columnName, double value, Integer ttl) {
        return putColumn(columnName, value, DoubleSerializer.get(), ttl);
    }

    @Override
    public ColumnListMutation<C> putColumn(final C columnName, final double value) {
        return putColumn(columnName, value, null);
    }

    @Override
    public ColumnListMutation<C> putColumnIfNotNull(C columnName, Double value) {
        if (value != null) {
            return putColumn(columnName, value);
        }
        return this;
    }

    @Override
    public ColumnListMutation<C> putColumnIfNotNull(C columnName, Double value, Integer ttl) {
        if (value != null) {
            return putColumn(columnName, value, ttl);
        }
        return this;
    }

    @Override
    public ColumnListMutation<C> putColumn(C columnName, UUID value, Integer ttl) {
        return putColumn(columnName, value, UUIDSerializer.get(), ttl);
    }

    @Override
    public ColumnListMutation<C> putColumn(final C columnName, final UUID value) {
        return putColumn(columnName, value, null);
    }

    @Override
    public ColumnListMutation<C> putColumnIfNotNull(C columnName, UUID value) {
        if (value != null) {
            return putColumn(columnName, value);
        }
        return this;
    }

    @Override
    public ColumnListMutation<C> putColumnIfNotNull(C columnName, UUID value, Integer ttl) {
        if (value != null) {
            return putColumn(columnName, value, ttl);
        }
        return this;
    }
    
    @Override
    public ColumnListMutation<C> putEmptyColumn(final C columnName) {
        return putEmptyColumn(columnName, null);
    }

    @Override
    public ColumnListMutation<C> setTimestamp(long timestamp) {
        this.defaultTimestamp.set(timestamp);
        return this;
    }

    @Override
    public ColumnListMutation<C> putCompressedColumn(C columnName, String value, Integer ttl) {
        Preconditions.checkNotNull(value, "Can't insert null value");
        
        if (value == null) {
            putEmptyColumn(columnName, ttl);
            return this;
        }
        
        ByteArrayOutputStream out = new ByteArrayOutputStream();
        GZIPOutputStream gzip;
        try {
            gzip = new GZIPOutputStream(out);
            gzip.write(StringUtils.getBytesUtf8(value));
            gzip.close();
            return this.putColumn(columnName, ByteBuffer.wrap(out.toByteArray()), ttl);
        } catch (IOException e) {
            throw new RuntimeException("Error compressing column " + columnName, e);
        }
    }

    @Override
    public ColumnListMutation<C> putCompressedColumn(C columnName, String value) {
        return putCompressedColumn(columnName, value, null);
    }

    @Override
    public ColumnListMutation<C> putCompressedColumnIfNotNull(C columnName, String value, Integer ttl) {
        if (value == null)
            return this;
        return putCompressedColumn(columnName, value, ttl);
    }

    @Override
    public ColumnListMutation<C> putCompressedColumnIfNotNull(C columnName, String value) {
        if (value == null)
            return this;
        return putCompressedColumn(columnName, value);
    }

    public Integer getDefaultTtl() {
        return defaultTTL.get();
    }
    
    public Long getTimestamp() {
        return defaultTimestamp.get();
    }
}>>>>>>> YOURS
/home/ramdisk/experiment3/projects/astyanax/revisions/rev_2b15fd9_3de01e3/rev_2b15fd9-3de01e3/astyanax-cql/src/main/java/com/netflix/astyanax/cql/writes/StatementCache.java;<<<<<<< MINE
=======
package com.netflix.astyanax.cql.writes;

import java.util.concurrent.Callable;
import java.util.concurrent.ConcurrentHashMap;

import com.datastax.driver.core.PreparedStatement;

public class StatementCache {

	private final ConcurrentHashMap<Integer, PreparedStatement> statementCache = new ConcurrentHashMap<Integer, PreparedStatement>();
	
	private StatementCache() {
		
	}
	
	public PreparedStatement getStatement(Integer id) {
		return statementCache.get(id);
	}
	
	public PreparedStatement getStatement(Integer id, Callable<PreparedStatement> func) {
		
		PreparedStatement stmt = statementCache.get(id);
		if (stmt == null) {
			try {
				stmt = func.call();
				statementCache.putIfAbsent(id, stmt);
			} catch (Exception e) {
				throw new RuntimeException(e);
			}
		}
		return stmt;
	}
	
	private static final StatementCache Instance = new StatementCache();
	
	public static StatementCache getInstance() {
		return Instance;
	}
}>>>>>>> YOURS
/home/ramdisk/experiment3/projects/astyanax/revisions/rev_2b15fd9_3de01e3/rev_2b15fd9-3de01e3/astyanax-cql/src/main/java/com/netflix/astyanax/cql/writes/CqlColumnListMutationImpl.java;<<<<<<< MINE
=======
package com.netflix.astyanax.cql.writes;

import java.util.ArrayList;
import java.util.List;
import java.util.concurrent.atomic.AtomicReference;

import com.google.common.base.Preconditions;
import com.netflix.astyanax.ColumnListMutation;
import com.netflix.astyanax.Serializer;
import com.netflix.astyanax.cql.CqlKeyspaceImpl.KeyspaceContext;
import com.netflix.astyanax.cql.schema.CqlColumnFamilyDefinitionImpl;
import com.netflix.astyanax.cql.util.CFQueryContext;
import com.netflix.astyanax.model.ColumnFamily;
import com.netflix.astyanax.model.ColumnPath;
import com.netflix.astyanax.model.ConsistencyLevel;

@SuppressWarnings("deprecation")
public class CqlColumnListMutationImpl<K, C> extends AbstractColumnListMutationImpl<C> {

	public enum ColListMutationType {
		RowDelete, ColumnsUpdate, CounterColumnsUpdate;
	}
	
	private ColListMutationType type = ColListMutationType.ColumnsUpdate;
	
	private final KeyspaceContext ksContext;
	private final CFQueryContext<K,C> cfContext;
	
	private final CqlColumnFamilyDefinitionImpl cfDef;
	
	private final List<CqlColumnMutationImpl<K, C>> mutationList = new ArrayList<CqlColumnMutationImpl<K,C>>();
	private AtomicReference<Boolean> deleteRow = new AtomicReference<Boolean>(false); 
	
	public CqlColumnListMutationImpl(KeyspaceContext ksCtx, ColumnFamily<K,C> cf, K rowKey, ConsistencyLevel level, Long timestamp) {
		
		super(timestamp);
		this.ksContext = ksCtx;
		this.cfContext = new CFQueryContext<K,C>(cf, rowKey, null, level);
		this.cfDef = (CqlColumnFamilyDefinitionImpl) cf.getColumnFamilyDefinition();
	}
	
	@Override
	public <V> ColumnListMutation<C> putColumn(C columnName, V value, Serializer<V> valueSerializer, Integer ttl) {
		
		checkColumnName(columnName);
		
		CqlColumnMutationImpl<K,C> mutation = new CqlColumnMutationImpl<K,C>(ksContext, cfContext, columnName);
		mutation.putValue(value, valueSerializer, getActualTTL(ttl));
		if (this.getTimestamp() != null) {
			mutation.withTimestamp(this.getTimestamp());
		}
		
		mutationList.add(mutation);
		return this;
	}

	@Override
	public <SC> ColumnListMutation<SC> withSuperColumn(ColumnPath<SC> superColumnPath) {
		throw new UnsupportedOperationException("Operation not supported");
	}

	@Override
	public ColumnListMutation<C> putEmptyColumn(C columnName, Integer ttl) {

		checkColumnName(columnName);
		
		Integer theTTL = super.defaultTTL.get();
		if (ttl != null) {
			theTTL = ttl;
		}

		CqlColumnMutationImpl<K,C> mutation = new CqlColumnMutationImpl<K,C>(ksContext, cfContext, columnName);
		mutation.putEmptyColumn(theTTL);
		if (this.getTimestamp() != null) {
			mutation.withTimestamp(this.getTimestamp());
		}
		mutationList.add(mutation);
		
		return this;
	}

	@Override
	public ColumnListMutation<C> incrementCounterColumn(C columnName, long amount) {
		
		checkColumnName(columnName);

		type = ColListMutationType.CounterColumnsUpdate;
		CqlColumnMutationImpl<K,C> mutation = new CqlColumnMutationImpl<K,C>(ksContext, cfContext, columnName);
		mutation.incrementCounterColumn(amount);
		mutationList.add(mutation);
		
		return this;
	}

	@Override
	public ColumnListMutation<C> deleteColumn(C columnName) {
		
		checkColumnName(columnName);

		CqlColumnMutationImpl<K,C> mutation = new CqlColumnMutationImpl<K,C>(ksContext, cfContext, columnName);
		mutation.deleteColumn();
		if (this.getTimestamp() != null) {
			mutation.withTimestamp(this.getTimestamp());
		}
		mutationList.add(mutation);
		
		return this;
	}

	@Override
	public ColumnListMutation<C> delete() {
		deleteRow.set(true);
		type = ColListMutationType.RowDelete;
		return this;
	}
	
	@Override
    public ColumnListMutation<C> setDefaultTtl(Integer newTTL) {
		
		if (super.defaultTTL.get() == null) {
			defaultTTL.set(newTTL);
			return this;
		}
		
		if (!(defaultTTL.equals(newTTL))) {
			throw new RuntimeException("Default TTL has already been set, cannot reset");
		}
        return this;
    }
	
	public void mergeColumnListMutation(CqlColumnListMutationImpl<?, ?> colListMutation) {
		
		for (CqlColumnMutationImpl<?,?> colMutation : colListMutation.getMutationList()) {
			this.mutationList.add((CqlColumnMutationImpl<K, C>) colMutation);
		}
	}
	
	public List<CqlColumnMutationImpl<K,C>> getMutationList() {
		return mutationList;
	}
	
	public ColumnListMutation<C> putColumnWithGenericValue(C columnName, Object value, Integer ttl) {
		
		Preconditions.checkArgument(columnName != null, "Column Name must not be null");
		
		CqlColumnMutationImpl<K,C> mutation = new CqlColumnMutationImpl<K,C>(ksContext, cfContext, columnName);
		mutation.putGenericValue(value, getActualTTL(ttl));
		
		mutationList.add(mutation);
		return this;
	}
	
	private Integer getActualTTL(Integer overrideTTL) {
		Integer theTTL = super.defaultTTL.get();
		if (overrideTTL != null) {
			theTTL = overrideTTL;
		}
		return theTTL;
	}
	
	private void checkColumnName(C columnName) {
		Preconditions.checkArgument(columnName != null, "Column Name must not be null");
		if (columnName instanceof String) {
			Preconditions.checkArgument(!((String)columnName).isEmpty(), "Column Name must not be null");
		}
	}
	
	public String toString() {
		StringBuilder sb = new StringBuilder();
		sb.append(cfContext.toString());
		sb.append(" MutationList: ").append(mutationList.toString());
		return sb.toString();
	}
	
	public CFMutationQueryGen getMutationQueryGen() {
		return cfDef.getMutationQueryGenerator();
	}

	public ColListMutationType getType() {
		return type;
	}

	public Object getRowKey() {
		return cfContext.getRowKey();
	}
}>>>>>>> YOURS
/home/ramdisk/experiment3/projects/astyanax/revisions/rev_2b15fd9_3de01e3/rev_2b15fd9-3de01e3/astyanax-cql/src/main/java/com/netflix/astyanax/cql/writes/MutationQueries.java;<<<<<<< MINE
=======
package com.netflix.astyanax.cql.writes;

public class MutationQueries {

	
}>>>>>>> YOURS
/home/ramdisk/experiment3/projects/astyanax/revisions/rev_2b15fd9_3de01e3/rev_2b15fd9-3de01e3/astyanax-cql/src/main/java/com/netflix/astyanax/cql/writes/CqlColumnMutationImpl.java;<<<<<<< MINE
=======
package com.netflix.astyanax.cql.writes;

import java.nio.ByteBuffer;
import java.util.Date;
import java.util.UUID;
import java.util.concurrent.atomic.AtomicReference;

import com.datastax.driver.core.BoundStatement;
import com.datastax.driver.core.ResultSet;
import com.datastax.driver.core.Statement;
import com.netflix.astyanax.CassandraOperationType;
import com.netflix.astyanax.ColumnMutation;
import com.netflix.astyanax.Execution;
import com.netflix.astyanax.Serializer;
import com.netflix.astyanax.cql.ConsistencyLevelMapping;
import com.netflix.astyanax.cql.CqlAbstractExecutionImpl;
import com.netflix.astyanax.cql.CqlKeyspaceImpl.KeyspaceContext;
import com.netflix.astyanax.cql.schema.CqlColumnFamilyDefinitionImpl;
import com.netflix.astyanax.cql.util.CFQueryContext;
import com.netflix.astyanax.model.ConsistencyLevel;
import com.netflix.astyanax.retry.RetryPolicy;
import com.netflix.astyanax.serializers.BooleanSerializer;
import com.netflix.astyanax.serializers.ByteBufferSerializer;
import com.netflix.astyanax.serializers.DateSerializer;
import com.netflix.astyanax.serializers.DoubleSerializer;
import com.netflix.astyanax.serializers.FloatSerializer;
import com.netflix.astyanax.serializers.IntegerSerializer;
import com.netflix.astyanax.serializers.LongSerializer;
import com.netflix.astyanax.serializers.ShortSerializer;
import com.netflix.astyanax.serializers.StringSerializer;
import com.netflix.astyanax.serializers.UUIDSerializer;

public class CqlColumnMutationImpl<K,C> implements ColumnMutation {

	protected final KeyspaceContext ksContext;
	protected final CFQueryContext<K,C> cfContext;
	protected final CqlColumnFamilyDefinitionImpl cfDef;
	
	protected final Object columnName;
	protected Object columnValue;

	// Tracking state
	public enum ColMutationType {
		UpdateColumn, CounterColumn, DeleteColumn;
	}
	private ColMutationType type = ColMutationType.UpdateColumn;

	private ConsistencyLevel consistencyLevel;
	private final AtomicReference<Long> timestamp = new AtomicReference<Long>(null);
	private final AtomicReference<Integer> ttl = new AtomicReference<Integer>(null);
	
	private final CFMutationQueryGen queryGen; 
	
	public CqlColumnMutationImpl(KeyspaceContext ksCtx, CFQueryContext<K,C> cfCtx, Object cName) {
		this.ksContext = ksCtx;
		this.columnName = cName;
		this.cfContext = cfCtx;
		this.cfDef = (CqlColumnFamilyDefinitionImpl) cfContext.getColumnFamily().getColumnFamilyDefinition();
		this.queryGen = cfDef.getMutationQueryGenerator();
	}

	@Override
	public ColumnMutation setConsistencyLevel(ConsistencyLevel consistencyLevel) {
		this.consistencyLevel = consistencyLevel;
		return this;
	}

	@Override
	public ColumnMutation withRetryPolicy(RetryPolicy retry) {
		this.cfContext.setRetryPolicy(retry);
		return this;
	}

	@Override
	public ColumnMutation withTimestamp(long newValue) {
		this.timestamp.set(newValue);
		return this;
	}

	@Override
	public Execution<Void> putValue(String value, Integer ttl) {
		return putValue(value, StringSerializer.get(), ttl);
	}

	@Override
	public Execution<Void> putValue(byte[] value, Integer ttl) {
		return exec(ByteBuffer.wrap(value), ttl, CassandraOperationType.COLUMN_MUTATE);
	}

	@Override
	public Execution<Void> putValue(byte value, Integer ttl) {
		byte[] bytes = new byte[1];
		bytes[0] = value;
		return exec(ByteBuffer.wrap(bytes), ttl, CassandraOperationType.COLUMN_MUTATE);
	}

	@Override
	public Execution<Void> putValue(short value, Integer ttl) {
		return putValue(value, ShortSerializer.get(), ttl);
	}

	@Override
	public Execution<Void> putValue(int value, Integer ttl) {
		return putValue(value, IntegerSerializer.get(), ttl);
	}

	@Override
	public Execution<Void> putValue(long value, Integer ttl) {
		return putValue(value, LongSerializer.get(), ttl);
	}

	@Override
	public Execution<Void> putValue(boolean value, Integer ttl) {
		return putValue(value, BooleanSerializer.get(), ttl);
	}

	@Override
	public Execution<Void> putValue(ByteBuffer value, Integer ttl) {
		return exec(value, ttl, CassandraOperationType.COLUMN_MUTATE);
	}

	@Override
	public Execution<Void> putValue(Date value, Integer ttl) {
		return putValue(value, DateSerializer.get(), ttl);
	}

	@Override
	public Execution<Void> putValue(float value, Integer ttl) {
		return putValue(value, FloatSerializer.get(), ttl);
	}

	@Override
	public Execution<Void> putValue(double value, Integer ttl) {
		return putValue(value, DoubleSerializer.get(), ttl);
	}

	@Override
	public Execution<Void> putValue(UUID value, Integer ttl) {
		return putValue(value, UUIDSerializer.get(), ttl);
	}

	@Override
	public <T> Execution<Void> putValue(T value, Serializer<T> serializer, Integer ttl) {
		
		if (cfDef.getClusteringKeyColumnDefinitionList().size() == 0) {
			return exec(value, ttl, CassandraOperationType.COLUMN_MUTATE);
		}
		
		if (cfContext.getColumnFamily().getDefaultValueSerializer().getComparatorType() == ByteBufferSerializer.get().getComparatorType()) {
			ByteBuffer valueBytes = ((value instanceof ByteBuffer) ? (ByteBuffer) value : serializer.toByteBuffer(value));
			return exec(valueBytes, ttl, CassandraOperationType.COLUMN_MUTATE);
		} else {
			return exec(value, ttl, CassandraOperationType.COLUMN_MUTATE);
		}
	}
	
	public Execution<Void> putGenericValue(Object value, Integer ttl) {
		return exec(value, ttl, CassandraOperationType.COLUMN_MUTATE);
	}

	@Override
	public Execution<Void> putEmptyColumn(Integer ttl) {
		return exec(null, ttl, CassandraOperationType.COLUMN_MUTATE);
	}

	@Override
	public Execution<Void> incrementCounterColumn(long amount) {
		type = ColMutationType.CounterColumn;
		return exec(amount, null, CassandraOperationType.COUNTER_MUTATE);
	}

	@Override
	public Execution<Void> deleteColumn() {
		type = ColMutationType.DeleteColumn;
		return exec(null, null, CassandraOperationType.COLUMN_DELETE);
	}

	@Override
	public Execution<Void> deleteCounterColumn() {
		type = ColMutationType.DeleteColumn;
		return exec(null, null, CassandraOperationType.COLUMN_DELETE);
	}

	private Execution<Void> exec(final Object value, final Integer overrideTTL, final CassandraOperationType opType) {

		final CqlColumnMutationImpl<K, C> thisMutation = this;
		this.columnValue = value;
		if (overrideTTL != null) {
			this.ttl.set(overrideTTL);
		}
		
		return new CqlAbstractExecutionImpl<Void>(ksContext, cfContext) {

			@Override
			public CassandraOperationType getOperationType() {
				return opType;
			}

			@Override
			public Statement getQuery() {
				BoundStatement bStmt = queryGen.getColumnMutationStatement(thisMutation, false);
				if (thisMutation.getConsistencyLevel() != null) {
					bStmt.setConsistencyLevel(ConsistencyLevelMapping.getCL(getConsistencyLevel()));
				}
				return bStmt;
			}

			@Override
			public Void parseResultSet(ResultSet resultSet) {
				return null;
			}
		};
	}
	
	public Integer getTTL() {
		return ttl.get();
	}

	public Long getTimestamp() {
		return timestamp.get();
	}
	
	public String toString() {
		return columnName.toString();
	}

	public ColMutationType getType() {
		return type;
	}
	
	public Object getRowKey() {
		return cfContext.getRowKey();
	}
	
	public ConsistencyLevel getConsistencyLevel() {
		return this.consistencyLevel;
	}
}>>>>>>> YOURS
/home/ramdisk/experiment3/projects/astyanax/revisions/rev_2b15fd9_3de01e3/rev_2b15fd9-3de01e3/astyanax-cql/src/main/java/com/netflix/astyanax/cql/writes/CqlMutationBatchImpl.java;<<<<<<< MINE
=======
package com.netflix.astyanax.cql.writes;

import java.nio.ByteBuffer;
import java.util.ArrayList;
import java.util.List;
import java.util.Map;
import java.util.Map.Entry;

import com.datastax.driver.core.BatchStatement;
import com.datastax.driver.core.BatchStatement.Type;
import com.datastax.driver.core.ResultSet;
import com.datastax.driver.core.Statement;
import com.google.common.util.concurrent.ListenableFuture;
import com.netflix.astyanax.CassandraOperationType;
import com.netflix.astyanax.Clock;
import com.netflix.astyanax.ColumnListMutation;
import com.netflix.astyanax.MutationBatch;
import com.netflix.astyanax.connectionpool.OperationResult;
import com.netflix.astyanax.connectionpool.exceptions.ConnectionException;
import com.netflix.astyanax.cql.ConsistencyLevelMapping;
import com.netflix.astyanax.cql.CqlAbstractExecutionImpl;
import com.netflix.astyanax.cql.CqlKeyspaceImpl.KeyspaceContext;
import com.netflix.astyanax.cql.writes.CqlColumnListMutationImpl.ColListMutationType;
import com.netflix.astyanax.model.ColumnFamily;
import com.netflix.astyanax.model.ConsistencyLevel;
import com.netflix.astyanax.retry.RetryPolicy;

public class CqlMutationBatchImpl extends AbstractMutationBatchImpl {

	private final KeyspaceContext ksContext; 
	
	// Control to turn use of prepared statement caching ON/OFF
	private boolean useCaching = false;
	
	public CqlMutationBatchImpl(KeyspaceContext ksCtx, Clock clock, ConsistencyLevel consistencyLevel, RetryPolicy retry) {
		super(clock, consistencyLevel, retry);
		this.ksContext = ksCtx;
	}

	@Override
	public <K, C> ColumnListMutation<C> createColumnListMutation(String keyspace, ColumnFamily<K, C> cf, K rowKey) {
		return new CqlColumnListMutationImpl<K, C>(ksContext, cf, rowKey, getConsistencyLevel(), timestamp);
	}

	@Override
	public void mergeColumnListMutation(ColumnListMutation<?> from, ColumnListMutation<?> to) {
	
		CqlColumnListMutationImpl<?, ?> fromCqlListMutation = (CqlColumnListMutationImpl<?, ?>) from;
		CqlColumnListMutationImpl<?, ?> toCqlListMutation = (CqlColumnListMutationImpl<?, ?>) to;
		
		toCqlListMutation.mergeColumnListMutation(fromCqlListMutation);
	}

	@Override
	public OperationResult<Void> execute() throws ConnectionException {
		
		return new CqlAbstractExecutionImpl<Void>(ksContext, getRetryPolicy()) {

			@Override
			public CassandraOperationType getOperationType() {
				return CassandraOperationType.BATCH_MUTATE;
			}

			@Override
			public Statement getQuery() {
				return getCachedPreparedStatement();
			}

			@Override
			public Void parseResultSet(ResultSet resultSet) {
				return null; // do nothing for mutations
			}
		}.execute();
	}

	@Override
	public ListenableFuture<OperationResult<Void>> executeAsync() throws ConnectionException {
		
		return new CqlAbstractExecutionImpl<Void>(ksContext, getRetryPolicy()) {

			@Override
			public CassandraOperationType getOperationType() {
				return CassandraOperationType.BATCH_MUTATE;
			}

			@Override
			public Statement getQuery() {
				return getCachedPreparedStatement();
			}

			@Override
			public Void parseResultSet(ResultSet resultSet) {
				return null; // do nothing for mutations
			}
		}.executeAsync();
	}

	private List<CqlColumnListMutationImpl<?, ?>> getColumnMutations() {
		
		List<CqlColumnListMutationImpl<?,?>> colListMutation = new ArrayList<CqlColumnListMutationImpl<?,?>>();
		
		for (Entry<ByteBuffer, Map<String, ColumnListMutation<?>>> entry : super.getMutationMap().entrySet()) {
			for (ColumnListMutation<?> colMutation : entry.getValue().values()) {
				colListMutation.add((CqlColumnListMutationImpl<?, ?>) colMutation);
			}
		}
		return colListMutation;
	}

	private BatchStatement getCachedPreparedStatement() {
		
		final List<CqlColumnListMutationImpl<?, ?>> colListMutations = getColumnMutations();

		if (colListMutations == null || colListMutations.size() == 0) {
			return new BatchStatement(Type.UNLOGGED);
		}
		
		ColListMutationType mutationType = colListMutations.get(0).getType();

		BatchStatement batch = new BatchStatement(Type.UNLOGGED);
		if (mutationType == ColListMutationType.CounterColumnsUpdate) {
			batch = new BatchStatement(Type.COUNTER);
		} else if (useAtomicBatch()) {
			batch = new BatchStatement(Type.LOGGED);
		}
		
		for (CqlColumnListMutationImpl<?, ?> colListMutation : colListMutations) {
			
			CFMutationQueryGen queryGen = colListMutation.getMutationQueryGen();
			queryGen.addColumnListMutationToBatch(batch, colListMutation, useCaching);
		}
		
		batch.setConsistencyLevel(ConsistencyLevelMapping.getCL(this.getConsistencyLevel()));
		
		return batch;
	}

	@Override
	public MutationBatch withCaching(boolean condition) {
		useCaching = condition;
		return this;
	}
}>>>>>>> YOURS
/home/ramdisk/experiment3/projects/astyanax/revisions/rev_2b15fd9_3de01e3/rev_2b15fd9-3de01e3/astyanax-cql/src/main/java/com/netflix/astyanax/cql/writes/AbstractMutationBatchImpl.java;<<<<<<< MINE
=======
package com.netflix.astyanax.cql.writes;

import java.nio.ByteBuffer;
import java.util.Map;
import java.util.Map.Entry;
import java.util.Set;

import org.apache.commons.codec.binary.Hex;

import com.google.common.base.Preconditions;
import com.google.common.collect.Maps;
import com.google.common.collect.Maps.EntryTransformer;
import com.netflix.astyanax.Clock;
import com.netflix.astyanax.ColumnListMutation;
import com.netflix.astyanax.MutationBatch;
import com.netflix.astyanax.WriteAheadLog;
import com.netflix.astyanax.connectionpool.Host;
import com.netflix.astyanax.model.ColumnFamily;
import com.netflix.astyanax.model.ConsistencyLevel;
import com.netflix.astyanax.retry.RetryPolicy;

public abstract class AbstractMutationBatchImpl implements MutationBatch {

	//private static final long UNSET_TIMESTAMP = -1;

	protected Long              timestamp = null; // UNSET_TIMESTAMP 
	protected ConsistencyLevel    consistencyLevel;
	protected Clock               clock;
	protected Host                pinnedHost;
	protected RetryPolicy         retry;
	protected WriteAheadLog       wal;
	protected boolean             useAtomicBatch = false;
	protected String              keyspace; 

	protected Map<ByteBuffer, Map<String, ColumnListMutation<?>>> mutationMap = Maps.newLinkedHashMap();
	protected Map<KeyAndColumnFamily, ColumnListMutation<?>> rowLookup = Maps.newHashMap();

	private static class KeyAndColumnFamily {
		private final String      columnFamily;
		private final ByteBuffer  key;

		public KeyAndColumnFamily(String columnFamily, ByteBuffer key) {
			this.columnFamily = columnFamily;
			this.key = key;
		}

		public int compareTo(Object obj) {
			if (obj instanceof KeyAndColumnFamily) {
				KeyAndColumnFamily other = (KeyAndColumnFamily)obj;
				int result = columnFamily.compareTo(other.columnFamily);
				if (result == 0) {
					result = key.compareTo(other.key);
				}
				return result;
			}
			return -1;
		}

		@Override
		public int hashCode() {
			final int prime = 31;
			int result = 1;
			result = prime * result + ((columnFamily == null) ? 0 : columnFamily.hashCode());
			result = prime * result + ((key == null) ? 0 : key.hashCode());
			return result;
		}

		@Override
		public boolean equals(Object obj) {
			if (this == obj)
				return true;
			if (obj == null)
				return false;
			if (getClass() != obj.getClass())
				return false;
			KeyAndColumnFamily other = (KeyAndColumnFamily) obj;
			if (columnFamily == null) {
				if (other.columnFamily != null)
					return false;
			} else if (!columnFamily.equals(other.columnFamily))
				return false;
			if (key == null) {
				if (other.key != null)
					return false;
			} else if (!key.equals(other.key))
				return false;
			return true;
		}
	}

	public AbstractMutationBatchImpl(Clock clock, ConsistencyLevel consistencyLevel, RetryPolicy retry) {
		this.clock            = clock;
		this.timestamp        = null; //UNSET_TIMESTAMP;
		this.consistencyLevel = consistencyLevel;
		this.retry            = retry;
	}

	@Override
	public <K, C> ColumnListMutation<C> withRow(ColumnFamily<K, C> columnFamily, K rowKey) {
		Preconditions.checkNotNull(columnFamily, "columnFamily cannot be null");
		Preconditions.checkNotNull(rowKey, "Row key cannot be null");

		ByteBuffer bbKey = columnFamily.getKeySerializer().toByteBuffer(rowKey);
		if (!bbKey.hasRemaining()) {
			throw new RuntimeException("Row key cannot be empty");
		}

		KeyAndColumnFamily kacf = new KeyAndColumnFamily(columnFamily.getName(), bbKey);
		ColumnListMutation<C> clm = (ColumnListMutation<C>) rowLookup.get(kacf);
		if (clm == null) {
			Map<String, ColumnListMutation<?>> innerMutationMap = mutationMap.get(bbKey);
			if (innerMutationMap == null) {
				innerMutationMap = Maps.newHashMap();
				mutationMap.put(bbKey, innerMutationMap);
			}

			ColumnListMutation<?> innerMutationList = innerMutationMap.get(columnFamily.getName());
			if (innerMutationList == null) {
				innerMutationList = createColumnListMutation(keyspace, columnFamily, rowKey);
				innerMutationMap.put(columnFamily.getName(), innerMutationList);
			}

			rowLookup.put(kacf, innerMutationList);
			clm = (ColumnListMutation<C>) innerMutationList;
		}
		return clm;
	}

	public abstract <K,C> ColumnListMutation<C> createColumnListMutation(String keyspace, ColumnFamily<K,C> cf, K rowKey); 

	@Override
	public void discardMutations() {
		this.timestamp = null; //UNSET_TIMESTAMP;
		this.mutationMap.clear();
		this.rowLookup.clear();
		this.withCaching(false); // TURN Caching off.
	}

	@Override
	public <K> void deleteRow(Iterable<? extends ColumnFamily<K, ?>> columnFamilies, K rowKey) {
		for (ColumnFamily<K, ?> cf : columnFamilies) {
			withRow(cf, rowKey).delete();
		}
	}

	/**
	 * Checks whether the mutation object contains rows. While the map may
	 * contain row keys the row keys may not contain any mutations.
	 * 
	 * @return
	 */
	@Override
	public boolean isEmpty() {
		return mutationMap.isEmpty();
	}

	/**
	 * Generate a string representation of the mutation with the following
	 * syntax Key1: [cf1, cf2],  Key2: [cf1, cf3]
	 */
	@Override
	public String toString() {
		StringBuilder sb = new StringBuilder();
		sb.append("MutationBatch[");
		boolean first = true;
		for (Entry<ByteBuffer, Map<String, ColumnListMutation<?>>> row : mutationMap.entrySet()) {
			if (!first)
				sb.append(",");
			sb.append(Hex.encodeHex(row.getKey().array()));
			sb.append(row.getValue().entrySet().toString());
		}
		sb.append("]");
		return sb.toString();
	}

	@Override
	public ByteBuffer serialize() throws Exception {
		throw new UnsupportedOperationException("Operation not supported");
	}

	@Override
	public void deserialize(ByteBuffer data) throws Exception {
		throw new UnsupportedOperationException("Operation not supported");
	}

	@Override
	public Map<ByteBuffer, Set<String>> getRowKeys() {
		return Maps.transformEntries(mutationMap,
				new EntryTransformer<ByteBuffer, Map<String, ColumnListMutation<?>>, Set<String>>() {
			@Override
			public Set<String> transformEntry(ByteBuffer key, Map<String, ColumnListMutation<?>> value) {
				return value.keySet();
			}
		});
	}

	public Map<ByteBuffer, Map<String, ColumnListMutation<?>>> getMutationMap() {
		return mutationMap;
	}

	public void mergeShallow(MutationBatch other) {
		if (!(other instanceof AbstractMutationBatchImpl)) {
			throw new UnsupportedOperationException();
		}

		for (Map.Entry<ByteBuffer, Map<String, ColumnListMutation<?>>> otherRow : ((AbstractMutationBatchImpl) other).mutationMap
				.entrySet()) {
			Map<String, ColumnListMutation<?>> thisRow = mutationMap.get(otherRow.getKey());
			// Key not in the map
			if (thisRow == null) {
				mutationMap.put(otherRow.getKey(), otherRow.getValue());
			}
			else {
				for (Map.Entry<String, ColumnListMutation<?>> otherCf : otherRow.getValue().entrySet()) {
					ColumnListMutation<?> thisCf = thisRow.get(otherCf.getKey());
					// Column family not in the map
					if (thisCf == null) {
						thisRow.put(otherCf.getKey(), otherCf.getValue());
					}
					else {
						mergeColumnListMutation(otherCf.getValue(), thisCf);
					}
				}
			}
		}
	}

	public abstract void mergeColumnListMutation(ColumnListMutation<?> from, ColumnListMutation<?> to);

	@Override
	public int getRowCount() {
		return mutationMap.size();
	}

	@Override
	public MutationBatch setTimeout(long timeout) {
		return this;
	}

	@Override
	public MutationBatch setTimestamp(long timestamp) {
		return withTimestamp(timestamp);
	}

	@Override
	public MutationBatch withTimestamp(long timestamp) {
		this.timestamp = timestamp;
		return this;
	}

	@Override
	public MutationBatch lockCurrentTimestamp() {
		this.timestamp = clock.getCurrentTime();
		return this;
	}

	@Override
	public MutationBatch setConsistencyLevel(ConsistencyLevel consistencyLevel) {
		this.consistencyLevel = consistencyLevel;
		return this;
	}

	@Override
	public MutationBatch withConsistencyLevel(ConsistencyLevel consistencyLevel) {
		this.consistencyLevel = consistencyLevel;
		return this;
	}

	public ConsistencyLevel getConsistencyLevel() {
		return this.consistencyLevel;
	}

	@Override
	public MutationBatch pinToHost(Host host) {
		this.pinnedHost = host;
		return this;
	}

	@Override
	public MutationBatch withRetryPolicy(RetryPolicy retry) {
		this.retry = retry.duplicate();
		return this;
	}

	@Override
	public MutationBatch usingWriteAheadLog(WriteAheadLog manager) {
		throw new UnsupportedOperationException("Operation not supported. ");
	}

	@Override
	public MutationBatch withAtomicBatch(boolean condition) {
		useAtomicBatch = condition;
		return this;
	}

	public boolean useAtomicBatch() {
		return useAtomicBatch;
	}

	public Host getPinnedHost() {
		return this.pinnedHost;
	}

	public RetryPolicy getRetryPolicy() {
		return this.retry;
	}

	public WriteAheadLog getWriteAheadLog() {
		return this.wal;
	}
}>>>>>>> YOURS
/home/ramdisk/experiment3/projects/astyanax/revisions/rev_2b15fd9_3de01e3/rev_2b15fd9-3de01e3/astyanax-cql/src/main/java/com/netflix/astyanax/cql/writes/CFMutationQueryGen.java;<<<<<<< MINE
=======
package com.netflix.astyanax.cql.writes;

import java.util.Iterator;
import java.util.concurrent.Callable;
import java.util.concurrent.atomic.AtomicReference;

import org.slf4j.Logger;
import org.slf4j.LoggerFactory;

import com.datastax.driver.core.BatchStatement;
import com.datastax.driver.core.BoundStatement;
import com.datastax.driver.core.PreparedStatement;
import com.datastax.driver.core.Session;
import com.netflix.astyanax.cql.schema.CqlColumnFamilyDefinitionImpl;
import com.netflix.astyanax.ddl.ColumnDefinition;
import com.netflix.astyanax.model.ColumnFamily;
import com.netflix.astyanax.serializers.AnnotatedCompositeSerializer;
import com.netflix.astyanax.serializers.AnnotatedCompositeSerializer.ComponentSerializer;
import com.netflix.astyanax.serializers.ComparatorType;

public class CFMutationQueryGen {

	private static final Logger Logger = LoggerFactory.getLogger(CFMutationQueryGen.class);

	public static enum MutationType {
		ColumnUpdate, ColumnDelete, RowDelete, CounterColumnUpdate;
	}

	// Constants that are used frequently for constructing the query
	private static final String INSERT_INTO  = "INSERT INTO ";
	private static final String OPEN_PARA  = " (";
	private static final String CLOSE_PARA  = ") ";
	private static final String VALUES  = ") VALUES (";
	private static final String BIND_MARKER  = "?,";
	private static final String LAST_BIND_MARKER  = "?";
	private static final String COMMA  = ",";
	private static final String USING = " USING ";
	private static final String TTL = " TTL ";
	private static final String AND = " AND ";
	private static final String TIMESTAMP = " TIMESTAMP ";

	private static final String DELETE_FROM  = "DELETE FROM ";
	private static final String WHERE  = " WHERE ";
	private static final String EQUALS  = " = ";
	private static final String UPDATE  = " UPDATE ";
	private static final String SET  = " SET ";

	private final String keyspace; 
	private final CqlColumnFamilyDefinitionImpl cfDef;
	private final Session session;

	public CFMutationQueryGen(Session session, String keyspaceName, CqlColumnFamilyDefinitionImpl cfDefinition) {

		this.keyspace = keyspaceName;
		this.cfDef = cfDefinition;
		this.session = session;
	}

	private static void appendWriteOptions(StringBuilder sb, Integer ttl, Long timestamp) {

		if (ttl != null || timestamp != null) {
			sb.append(USING);
		}

		if (ttl != null) {
			sb.append(TTL + ttl);
		}

		if (timestamp != null) {
			if (ttl != null) {
				sb.append(AND);
			}
			sb.append(TIMESTAMP + timestamp);
		}	
	}	
	
	abstract class MutationQueryCache<M> {

		private final AtomicReference<PreparedStatement> cachedStatement = new AtomicReference<PreparedStatement>(null);

		public abstract Callable<String> getQueryGen(M mutation);

		public void addToBatch(BatchStatement batch, M mutation, boolean useCaching) {
			batch.add(getBoundStatement(mutation, useCaching));
		}
		
		public BoundStatement getBoundStatement(M mutation, boolean useCaching) {
			
			PreparedStatement pStatement = getPreparedStatement(mutation, useCaching);
			return bindValues(pStatement, mutation);
		}
		
		public abstract BoundStatement bindValues(PreparedStatement pStatement, M mutation);
		
		public PreparedStatement getPreparedStatement(M mutation, boolean useCaching) {
			
			PreparedStatement pStatement = null;
			
			if (useCaching) {
				pStatement = cachedStatement.get();
			}
			
			if (pStatement == null) {
				try {
					String query = getQueryGen(mutation).call();
					pStatement = session.prepare(query);
					
					if (Logger.isDebugEnabled()) {
						Logger.debug("Query: " + pStatement.getQueryString());
					}
				} catch (Exception e) {
					throw new RuntimeException(e);
				}
			}
			
			if (useCaching && cachedStatement.get() == null) {
				cachedStatement.set(pStatement);
			}
			return pStatement;
		}
	}


	private MutationQueryCache<CqlColumnListMutationImpl<?,?>> DeleteRowQuery = new MutationQueryCache<CqlColumnListMutationImpl<?,?>>() {

		private final Callable<String> queryGen = new Callable<String>() {
			@Override
			public String call() throws Exception {
				return DELETE_FROM + keyspace + "." + cfDef.getName() + 
						WHERE + cfDef.getPartitionKeyColumnDefinition().getName() + EQUALS + LAST_BIND_MARKER;
			}
		};

		@Override
		public Callable<String> getQueryGen(CqlColumnListMutationImpl<?, ?> mutation) {
			return queryGen;
		}

		@Override
		public BoundStatement bindValues(PreparedStatement pStatement, CqlColumnListMutationImpl<?, ?> mutation) {
			return pStatement.bind(mutation.getRowKey());
		}
	};

	abstract class BaseClusteringKeyMutation extends MutationQueryCache<CqlColumnMutationImpl<?,?>> {

		public abstract boolean isDeleteQuery();

		@Override
		public BoundStatement bindValues(PreparedStatement pStatement, CqlColumnMutationImpl<?,?> colMutation) {

			int size = cfDef.getPartitionKeyColumnDefinitionList().size() + cfDef.getClusteringKeyColumnDefinitionList().size();
			if (!isDeleteQuery()) {
				size += cfDef.getRegularColumnDefinitionList().size();  
			} else {
				// we don't need to add the value component here. Just the partition key and the clustering key
			}

			Object[] arr = new Object[size];

			int index = 0;

			arr[index++] = colMutation.getRowKey();

			ColumnFamily<?,?> cf = colMutation.cfContext.getColumnFamily();
			boolean isCompositeColumn = cf.getColumnSerializer().getComparatorType() == ComparatorType.COMPOSITETYPE;

			if (isCompositeColumn) {
				AnnotatedCompositeSerializer<?> compSerializer = (AnnotatedCompositeSerializer<?>) cf.getColumnSerializer();
				for (ComponentSerializer<?> component : compSerializer.getComponents()) {
					try {
						arr[index++] = component.getFieldValueDirectly(colMutation.columnName);
					} catch (Exception e) {
						throw new RuntimeException(e);
					}
				}
			} else {
				arr[index++] = colMutation.columnName;
			}

			if (!isDeleteQuery()) {
				arr[index++] = colMutation.columnValue;
			}

			return pStatement.bind(arr);
		}
	}


	private BaseClusteringKeyMutation InsertColumnWithClusteringKey = new BaseClusteringKeyMutation() {

		@Override
		public Callable<String> getQueryGen(final CqlColumnMutationImpl<?, ?> mutation) {
			return new Callable<String>() {
				@Override
				public String call() throws Exception {
					return genQuery().toString();
				}

				private StringBuilder genQuery() {
					
					/**
					 * e.g 
					 *    insert into t (key, column1, value) values ('a', '2' , 'a2') using ttl 86400 and timestamp = 1234444;
					 */

					int columnCount = 0; 

					StringBuilder sb = new StringBuilder(INSERT_INTO);
					sb.append(keyspace + "." + cfDef.getName());
					sb.append(OPEN_PARA);

					Iterator<ColumnDefinition> iter = cfDef.getPartitionKeyColumnDefinitionList().iterator();
					while (iter.hasNext()) {
						sb.append(iter.next().getName());
						columnCount++;
						if (iter.hasNext()) {
							sb.append(COMMA);
						}
					}

					iter = cfDef.getClusteringKeyColumnDefinitionList().iterator();
					if (iter.hasNext()) {
						sb.append(COMMA);
						while (iter.hasNext()) {
							sb.append(iter.next().getName());
							columnCount++;
							if (iter.hasNext()) {
								sb.append(COMMA);
							}
						}
					}

					iter = cfDef.getRegularColumnDefinitionList().iterator();
					if (iter.hasNext()) {
						sb.append(COMMA);
						while (iter.hasNext()) {
							sb.append(iter.next().getName());
							columnCount++;
							if (iter.hasNext()) {
								sb.append(COMMA);
							}
						}
					}

					sb.append(VALUES);
					for (int i=0; i<columnCount; i++) {
						if (i < (columnCount-1)) {
							sb.append(BIND_MARKER);
						} else {
							sb.append(LAST_BIND_MARKER);
						}
					}
					sb.append(CLOSE_PARA);

					appendWriteOptions(sb, mutation.getTTL(), mutation.getTimestamp());
					
					return sb;
				}	
			};
		}

		@Override
		public boolean isDeleteQuery() {
			return false;
		}
	};

	private BaseClusteringKeyMutation DeleteColumnWithClusteringKey = new BaseClusteringKeyMutation() {

		@Override
		public Callable<String> getQueryGen(final CqlColumnMutationImpl<?, ?> mutation) {
			
			return new Callable<String>() {
				@Override
				public String call() throws Exception {
					return genQuery().toString();
				}

				private StringBuilder genQuery() {

					StringBuilder sb = new StringBuilder(DELETE_FROM);
					sb.append(keyspace + "." + cfDef.getName());

					appendWriteOptions(sb, mutation.getTTL(), mutation.getTimestamp());

					Iterator<ColumnDefinition> iter = cfDef.getPartitionKeyColumnDefinitionList().iterator();

					sb.append(WHERE);
					while (iter.hasNext()) {
						sb.append(iter.next().getName()).append(EQUALS).append(LAST_BIND_MARKER);
						if (iter.hasNext()) {
							sb.append(AND);
						}
					}

					iter = cfDef.getClusteringKeyColumnDefinitionList().iterator();
					if (iter.hasNext()) {
						sb.append(AND);
						while (iter.hasNext()) {
							sb.append(iter.next().getName()).append(EQUALS).append(LAST_BIND_MARKER);
							if (iter.hasNext()) {
								sb.append(AND);
							}
						}
					}


					return sb;
				}
			};
		}

		@Override
		public boolean isDeleteQuery() {
			return true;
		}
	};
	
	private MutationQueryCache<CqlColumnMutationImpl<?,?>> CounterColumnUpdate = new MutationQueryCache<CqlColumnMutationImpl<?,?>>() {

		@Override
		public Callable<String> getQueryGen(final CqlColumnMutationImpl<?, ?> mutation) {
			return new Callable<String>() {

				@Override
				public String call() throws Exception {
					
					String valueAlias = cfDef.getRegularColumnDefinitionList().get(0).getName();
					

					StringBuilder sb = new StringBuilder();
					sb.append(UPDATE + keyspace + "." + cfDef.getName()); 
					appendWriteOptions(sb, mutation.getTTL(), mutation.getTimestamp());
					sb.append(SET + valueAlias + " = " + valueAlias + " + ? ");
					
					Iterator<ColumnDefinition> iter = cfDef.getPartitionKeyColumnDefinitionList().iterator();

					sb.append(WHERE);
					while (iter.hasNext()) {
						sb.append(iter.next().getName()).append(EQUALS).append(LAST_BIND_MARKER);
						if (iter.hasNext()) {
							sb.append(AND);
						}
					}

					iter = cfDef.getClusteringKeyColumnDefinitionList().iterator();
					if (iter.hasNext()) {
						sb.append(AND);
						while (iter.hasNext()) {
							sb.append(iter.next().getName()).append(EQUALS).append(LAST_BIND_MARKER);
							if (iter.hasNext()) {
								sb.append(AND);
							}
						}
					}

					return sb.toString();
				}
			};
		}

		@Override
		public BoundStatement bindValues(PreparedStatement pStatement, CqlColumnMutationImpl<?, ?> mutation) {
			
			int size = 1 + cfDef.getPartitionKeyColumnDefinitionList().size() + cfDef.getClusteringKeyColumnDefinitionList().size();

			Object[] arr = new Object[size];

			int index = 0;

			arr[index++] = mutation.columnValue;
			arr[index++] = mutation.getRowKey();

			ColumnFamily<?,?> cf = mutation.cfContext.getColumnFamily();
			boolean isCompositeColumn = cf.getColumnSerializer().getComparatorType() == ComparatorType.COMPOSITETYPE;

			if (isCompositeColumn) {
				AnnotatedCompositeSerializer<?> compSerializer = (AnnotatedCompositeSerializer<?>) cf.getColumnSerializer();
				for (ComponentSerializer<?> component : compSerializer.getComponents()) {
					try {
						arr[index++] = component.getFieldValueDirectly(mutation.columnName);
					} catch (Exception e) {
						throw new RuntimeException(e);
					}
				}
			} else {
				arr[index++] = mutation.columnName;
			}

			return pStatement.bind(arr);
		}
	};

	private MutationQueryCache<CqlColumnListMutationImpl<?,?>> InsertOrDeleteWithClusteringKey = new MutationQueryCache<CqlColumnListMutationImpl<?,?>>() {

		@Override
		public void addToBatch(BatchStatement batch, CqlColumnListMutationImpl<?,?> colListMutation, boolean useCaching) {
			
			for (CqlColumnMutationImpl<?,?> colMutation : colListMutation.getMutationList()) {

				switch (colMutation.getType()) {

				case UpdateColumn :
					InsertColumnWithClusteringKey.addToBatch(batch, colMutation, useCaching);
					break;
				case DeleteColumn : 
					DeleteColumnWithClusteringKey.addToBatch(batch, colMutation, useCaching);
					break;
				case CounterColumn : 
					throw new RuntimeException("Counter column update not allowed with other updates");
				default:
					throw new RuntimeException("Unsupported type: " + colMutation.getType());
				};
			}
		}

		@Override
		public Callable<String> getQueryGen(CqlColumnListMutationImpl<?, ?> colListMutation) {
			throw new RuntimeException("Not Supported");
		}

		@Override
		public BoundStatement bindValues(PreparedStatement pStatement, CqlColumnListMutationImpl<?, ?> colListMutation) {
			throw new RuntimeException("Not Supported");
		}
	};

	private MutationQueryCache<CqlColumnListMutationImpl<?,?>> InsertOrDeleteColumnListWithClusteringKey = new MutationQueryCache<CqlColumnListMutationImpl<?,?>>() {

		@Override
		public void addToBatch(BatchStatement batch, CqlColumnListMutationImpl<?,?> colListMutation, boolean useCaching) {
			
			for (CqlColumnMutationImpl<?,?> colMutation : colListMutation.getMutationList()) {
				InsertOrDeleteColumnWithClusteringKey.addToBatch(batch, colMutation, useCaching);
			}
		}

		@Override
		public Callable<String> getQueryGen(CqlColumnListMutationImpl<?, ?> colListMutation) {
			throw new RuntimeException("Not Supported");
		}

		@Override
		public BoundStatement bindValues(PreparedStatement pStatement, CqlColumnListMutationImpl<?, ?> colListMutation) {
			throw new RuntimeException("Not Supported");
		}
	};

	private MutationQueryCache<CqlColumnMutationImpl<?,?>> InsertOrDeleteColumnWithClusteringKey = new MutationQueryCache<CqlColumnMutationImpl<?,?>>() {

		@Override
		public BoundStatement getBoundStatement(CqlColumnMutationImpl<?, ?> mutation, boolean useCaching) {
			switch (mutation.getType()) {

			case UpdateColumn :
				return InsertColumnWithClusteringKey.getBoundStatement(mutation, useCaching);
			case DeleteColumn : 
				return DeleteColumnWithClusteringKey.getBoundStatement(mutation, useCaching);
			case CounterColumn : 
				return CounterColumnUpdate.getBoundStatement(mutation, useCaching);
			default:
				throw new RuntimeException("Unsupported type: " + mutation.getType());
			}
		}

		@Override
		public Callable<String> getQueryGen(CqlColumnMutationImpl<?, ?> colMutation) {
			throw new RuntimeException("Not Supported");
		}

		@Override
		public BoundStatement bindValues(PreparedStatement pStatement, CqlColumnMutationImpl<?, ?> colMutation) {
			throw new RuntimeException("Not Supported");
		}
	};


	private MutationQueryCache<CqlColumnListMutationImpl<?,?>> CounterColumnList = new MutationQueryCache<CqlColumnListMutationImpl<?,?>>() {

		@Override
		public void addToBatch(BatchStatement batch, CqlColumnListMutationImpl<?,?> colListMutation, boolean useCaching) {
			
			for (CqlColumnMutationImpl<?,?> colMutation : colListMutation.getMutationList()) {
				CounterColumnUpdate.addToBatch(batch, colMutation, useCaching);
			}
		}

		@Override
		public Callable<String> getQueryGen(CqlColumnListMutationImpl<?, ?> colListMutation) {
			throw new RuntimeException("Not Supported");
		}

		@Override
		public BoundStatement bindValues(PreparedStatement pStatement, CqlColumnListMutationImpl<?, ?> colListMutation) {
			throw new RuntimeException("Not Supported");
		}
	};
	
	private MutationQueryCache<CqlColumnListMutationImpl<?,?>> FlatTableInsertQuery = new MutationQueryCache<CqlColumnListMutationImpl<?,?>> () {

		@Override
		public void addToBatch(BatchStatement batch, CqlColumnListMutationImpl<?, ?> colListMutation, boolean useCaching) {

			StringBuilder sb = new StringBuilder();
			sb.append(INSERT_INTO).append(keyspace + "." + cfDef.getName());
			sb.append(OPEN_PARA);

			// Init the object array for the bind values
			int size = colListMutation.getMutationList().size() + 1;
			Object[] values = new Object[size];
			int index = 0;
			
			// Add in the primary key
			sb.append(cfDef.getPartitionKeyColumnDefinition().getName()).append(COMMA);
			values[index++] = colListMutation.getRowKey();
			
			for (CqlColumnMutationImpl<?,?> colMutation : colListMutation.getMutationList()) {
				sb.append(colMutation.columnName);
				values[index++] = colMutation.columnValue;
				if (index < size) {
					sb.append(COMMA);
				}
			}
			
			sb.append(VALUES); 
			
			for (int i=0; i<size; i++) {
				if (i < (size-1)) {
					sb.append(BIND_MARKER);
				} else {
					sb.append(LAST_BIND_MARKER);
				}
			}
			
			sb.append(CLOSE_PARA);
			
			appendWriteOptions(sb, colListMutation.getDefaultTtl(), colListMutation.getTimestamp());
			
			String query = sb.toString(); 
			
			if (Logger.isDebugEnabled()) {
				Logger.debug("Query: " + query);
			}
			
			try {
				PreparedStatement pStatement = session.prepare(query);
				batch.add(pStatement.bind(values));
			} catch (Exception e) {
				throw new RuntimeException(e);
			}
		}

		@Override
		public Callable<String> getQueryGen(CqlColumnListMutationImpl<?, ?> mutation) {
			throw new RuntimeException("Not Supported");
		}

		@Override
		public BoundStatement bindValues(PreparedStatement pStatement, CqlColumnListMutationImpl<?, ?> mutation) {
			throw new RuntimeException("Not Supported");
		}
	};

	private MutationQueryCache<CqlColumnMutationImpl<?,?>> FlatTableInsertQueryForColumn = new MutationQueryCache<CqlColumnMutationImpl<?,?>> () {

		@Override
		public Callable<String> getQueryGen(CqlColumnMutationImpl<?, ?> mutation) {
			throw new RuntimeException("Not Supported");
		}


		@Override
		public BoundStatement bindValues(PreparedStatement pStatement, CqlColumnMutationImpl<?, ?> mutation) {
			throw new RuntimeException("Not Supported");
		}
		
		@Override
		public void addToBatch(BatchStatement batch, CqlColumnMutationImpl<?, ?> mutation, boolean useCaching) {
			throw new RuntimeException("Not Supported");
		}
		
		@Override
		public BoundStatement getBoundStatement(CqlColumnMutationImpl<?, ?> mutation, boolean useCaching) {

			StringBuilder sb = new StringBuilder();
			sb.append(INSERT_INTO).append(keyspace + "." + cfDef.getName());
			sb.append(OPEN_PARA);

			sb.append(cfDef.getPartitionKeyColumnDefinition().getName());
			sb.append(COMMA);
			sb.append(mutation.columnName);

			sb.append(VALUES); 
			sb.append(BIND_MARKER);
			sb.append(LAST_BIND_MARKER);
			sb.append(CLOSE_PARA);
			
			appendWriteOptions(sb, mutation.getTTL(), mutation.getTimestamp());
			
			String query = sb.toString(); 
			
			if (Logger.isDebugEnabled()) {
				Logger.debug("Query: " + query);
			}

			// Init the object array for the bind values
			Object[] values = new Object[2];
			values[0] = mutation.getRowKey();
			values[1] = mutation.columnValue;
			
			try {
				PreparedStatement pStatement = session.prepare(query);
				return pStatement.bind(values);
			} catch (Exception e) {
				throw new RuntimeException(e);
			}
		}
	};


	public void addColumnListMutationToBatch(BatchStatement batch, CqlColumnListMutationImpl<?,?> colListMutation, boolean useCaching) {

		switch (colListMutation.getType()) {
		case RowDelete:
			DeleteRowQuery.addToBatch(batch, colListMutation, useCaching);
			break;
		case ColumnsUpdate:
			if (cfDef.getClusteringKeyColumnDefinitionList().size() == 0) {
				// THIS IS A FLAT TABLE QUERY
				FlatTableInsertQuery.addToBatch(batch, colListMutation, useCaching);
			} else {
				InsertOrDeleteWithClusteringKey.addToBatch(batch, colListMutation, useCaching);
			}
			break;
		case CounterColumnsUpdate:
			CounterColumnList.addToBatch(batch, colListMutation, useCaching);
			break;
		default:
			throw new RuntimeException("Unrecognized ColumnListMutation Type");
		}
	}

	public BoundStatement getColumnMutationStatement(CqlColumnMutationImpl<?,?> mutation, boolean useCaching) {

		if (cfDef.getClusteringKeyColumnDefinitionList().size() == 0) {
			// THIS IS A FLAT TABLE QUERY
			return FlatTableInsertQueryForColumn.getBoundStatement(mutation, useCaching);
		} else {
			return InsertOrDeleteColumnWithClusteringKey.getBoundStatement(mutation, useCaching);
		}
	}
}>>>>>>> YOURS
/home/ramdisk/experiment3/projects/astyanax/revisions/rev_2b15fd9_3de01e3/rev_2b15fd9-3de01e3/astyanax-cql/src/main/java/com/netflix/astyanax/cql/direct/DirectCqlStatementResultImpl.java;<<<<<<< MINE
=======
package com.netflix.astyanax.cql.direct;

import java.util.List;

import com.datastax.driver.core.ResultSet;
import com.netflix.astyanax.cql.CqlSchema;
import com.netflix.astyanax.cql.CqlStatementResult;
import com.netflix.astyanax.cql.reads.model.CqlRowListImpl;
import com.netflix.astyanax.model.ColumnFamily;
import com.netflix.astyanax.model.Rows;

/**
 * Impl of {@link CqlStatementResult} that parses the result set from java driver query operations. 
 * 
 * @author poberai
 */
public class DirectCqlStatementResultImpl implements CqlStatementResult {
	
	private final ResultSet rs; 
	
	public DirectCqlStatementResultImpl(ResultSet rs) {
		this.rs = rs;
	}

	@Override
	public long asCount() {
		return rs.one().getLong(0);
	}

	@Override
	public <K, C> Rows<K, C> getRows(ColumnFamily<K, C> columnFamily) {

		List<com.datastax.driver.core.Row> rows = rs.all(); 
		return new CqlRowListImpl<K, C>(rows, columnFamily);
	}

	@Override
	public CqlSchema getSchema() {
		return new DirectCqlSchema(rs);
	}
	
	public static class DirectCqlSchema implements CqlSchema {
		
		private final ResultSet rs;
		
		public DirectCqlSchema(ResultSet result) {
			this.rs = result;
		}
		
		public ResultSet getResultSet() {
			return rs;
		}
	}
}>>>>>>> YOURS
/home/ramdisk/experiment3/projects/astyanax/revisions/rev_2b15fd9_3de01e3/rev_2b15fd9-3de01e3/astyanax-cql/src/main/java/com/netflix/astyanax/cql/direct/DirectCqlPreparedStatement.java;<<<<<<< MINE
=======
package com.netflix.astyanax.cql.direct;

import java.nio.ByteBuffer;
import java.util.ArrayList;
import java.util.List;
import java.util.UUID;

import com.datastax.driver.core.BoundStatement;
import com.datastax.driver.core.PreparedStatement;
import com.datastax.driver.core.ResultSet;
import com.datastax.driver.core.ResultSetFuture;
import com.datastax.driver.core.Session;
import com.google.common.util.concurrent.ListenableFuture;
import com.netflix.astyanax.Serializer;
import com.netflix.astyanax.connectionpool.OperationResult;
import com.netflix.astyanax.connectionpool.exceptions.ConnectionException;
import com.netflix.astyanax.cql.CqlOperationResultImpl;
import com.netflix.astyanax.cql.CqlPreparedStatement;
import com.netflix.astyanax.cql.CqlStatementResult;
import com.netflix.astyanax.cql.util.AsyncOperationResult;

/**
 * Impl of {@link CqlPreparedStatement} using java driver.
 * it manages a {@link Session} object that is used when actually performing the real query with the 
 * driver underneath. 
 * 
 * @author poberai
 */
public class DirectCqlPreparedStatement implements CqlPreparedStatement {

	private final Session session;
	private final PreparedStatement pStmt; 
	private final List<Object> bindValues = new ArrayList<Object>(); 
	
	public DirectCqlPreparedStatement(Session session, PreparedStatement pStmt) {
		this.session = session;
		this.pStmt = pStmt;
	}

	@Override
	public OperationResult<CqlStatementResult> execute() throws ConnectionException {
		
		BoundStatement bStmt = pStmt.bind(bindValues.toArray());
		ResultSet resultSet = session.execute(bStmt);
		
		CqlStatementResult result = new DirectCqlStatementResultImpl(resultSet);
		return new CqlOperationResultImpl<CqlStatementResult>(resultSet, result);
	}

	@Override
	public ListenableFuture<OperationResult<CqlStatementResult>> executeAsync() throws ConnectionException {

		BoundStatement bStmt = pStmt.bind(bindValues.toArray());
		ResultSetFuture rsFuture = session.executeAsync(bStmt);

		return new AsyncOperationResult<CqlStatementResult>(rsFuture) {

			@Override
			public OperationResult<CqlStatementResult> getOperationResult(ResultSet rs) {
				CqlStatementResult result = new DirectCqlStatementResultImpl(rs);
				return new CqlOperationResultImpl<CqlStatementResult>(rs, result);			}
		};
	}

	@Override
	public <V> CqlPreparedStatement withByteBufferValue(V value, Serializer<V> serializer) {
		bindValues.add(value);
		return this;
	}

	@Override
	public CqlPreparedStatement withValue(ByteBuffer value) {
		bindValues.add(value);
		return this;
	}

	@Override
	public CqlPreparedStatement withValues(List<ByteBuffer> values) {
		bindValues.addAll(values);
		return this;
	}

	@Override
	public CqlPreparedStatement withStringValue(String value) {
		bindValues.add(value);
		return this;
	}

	@Override
	public CqlPreparedStatement withIntegerValue(Integer value) {
		bindValues.add(value);
		return this;
	}

	@Override
	public CqlPreparedStatement withBooleanValue(Boolean value) {
		bindValues.add(value);
		return this;
	}

	@Override
	public CqlPreparedStatement withDoubleValue(Double value) {
		bindValues.add(value);
		return this;
	}

	@Override
	public CqlPreparedStatement withLongValue(Long value) {
		bindValues.add(value);
		return this;
	}

	@Override
	public CqlPreparedStatement withFloatValue(Float value) {
		bindValues.add(value);
		return this;
	}

	@Override
	public CqlPreparedStatement withShortValue(Short value) {
		bindValues.add(value);
		return this;
	}

	@Override
	public CqlPreparedStatement withUUIDValue(UUID value) {
		bindValues.add(value);
		return this;
	}
	
	public PreparedStatement getInnerPreparedStatement() {
		return pStmt;
	}
}>>>>>>> YOURS
/home/ramdisk/experiment3/projects/astyanax/revisions/rev_2b15fd9_3de01e3/rev_2b15fd9-3de01e3/astyanax-cql/src/main/java/com/netflix/astyanax/cql/direct/DirectCqlStatement.java;<<<<<<< MINE
=======
package com.netflix.astyanax.cql.direct;

import com.datastax.driver.core.PreparedStatement;
import com.datastax.driver.core.ResultSet;
import com.datastax.driver.core.ResultSetFuture;
import com.datastax.driver.core.Session;
import com.datastax.driver.core.SimpleStatement;
import com.datastax.driver.core.Statement;
import com.google.common.util.concurrent.ListenableFuture;
import com.netflix.astyanax.connectionpool.OperationResult;
import com.netflix.astyanax.connectionpool.exceptions.ConnectionException;
import com.netflix.astyanax.cql.CqlOperationResultImpl;
import com.netflix.astyanax.cql.CqlPreparedStatement;
import com.netflix.astyanax.cql.CqlStatement;
import com.netflix.astyanax.cql.CqlStatementResult;
import com.netflix.astyanax.cql.util.AsyncOperationResult;
import com.netflix.astyanax.cql.util.ConsistencyLevelTransform;
import com.netflix.astyanax.model.ConsistencyLevel;

/**
 * Impl of {@link CqlStatement} using java driver.
 * it manages a {@link Session} object that is used when actually performing the real query with the 
 * driver underneath. 
 * 
 * @author poberai
 */
public class DirectCqlStatement implements CqlStatement {

	private final Session session; 
	
	private ConsistencyLevel cLevel = ConsistencyLevel.CL_ONE; // the default cl 
	private String cqlQuery; 
	
	public DirectCqlStatement(Session session) {
		this.session = session;
	}
	
	@Override
	public CqlStatement withConsistencyLevel(ConsistencyLevel cl) {
		this.cLevel = cl;
		return this;
	}

	@Override
	public CqlStatement withCql(String cql) {
		this.cqlQuery = cql;
		return this;
	}

	@Override
	public OperationResult<CqlStatementResult> execute() throws ConnectionException {
		
		Statement q = new SimpleStatement(cqlQuery);
		q.setConsistencyLevel(ConsistencyLevelTransform.getConsistencyLevel(cLevel));
		
		ResultSet resultSet = session.execute(q);
		
		CqlStatementResult result = new DirectCqlStatementResultImpl(resultSet);
		return new CqlOperationResultImpl<CqlStatementResult>(resultSet, result);
	}

	@Override
	public ListenableFuture<OperationResult<CqlStatementResult>> executeAsync() throws ConnectionException {
		Statement q = new SimpleStatement(cqlQuery);
		q.setConsistencyLevel(ConsistencyLevelTransform.getConsistencyLevel(cLevel));

		ResultSetFuture rsFuture = session.executeAsync(q);

		return new AsyncOperationResult<CqlStatementResult>(rsFuture) {

			@Override
			public OperationResult<CqlStatementResult> getOperationResult(ResultSet rs) {
				CqlStatementResult result = new DirectCqlStatementResultImpl(rs);
				return new CqlOperationResultImpl<CqlStatementResult>(rs, result);			}
		};
	}

	@Override
	public CqlPreparedStatement asPreparedStatement() {
		PreparedStatement pStmt = session.prepare(cqlQuery);
		pStmt.setConsistencyLevel(ConsistencyLevelTransform.getConsistencyLevel(cLevel));
		return new DirectCqlPreparedStatement(session, pStmt);
	}

}>>>>>>> YOURS
/home/ramdisk/experiment3/projects/astyanax/revisions/rev_2b15fd9_3de01e3/rev_2b15fd9-3de01e3/astyanax-cql/src/main/java/com/netflix/astyanax/cql/retrypolicies/ChangeConsistencyLevelRetryPolicy.java;<<<<<<< MINE
=======
package com.netflix.astyanax.cql.retrypolicies;

import com.datastax.driver.core.ConsistencyLevel;
import com.datastax.driver.core.Statement;
import com.datastax.driver.core.WriteType;
import com.datastax.driver.core.policies.RetryPolicy.RetryDecision;
import com.netflix.astyanax.cql.ConsistencyLevelMapping;

/**
 * Class that encapsulates a RetryPolicy that can be used in a configurable way when doing reties. 
 * Users can choose whether to retry on just  reads / writes / unavailable exceptions or just all operations. 
 * Users can also decide to change the consistency level on different reties. 
 * 
 * @author poberai
 *
 */
public class ChangeConsistencyLevelRetryPolicy extends JavaDriverBasedRetryPolicy {

	// Policies for specific conditions
	private boolean retryOnAllConditions = true;
	private boolean retryOnReads = false;
	private boolean retryOnWrites = false;
	private boolean retryOnUnavailable = false;
	
	// The retry count
	private int retryCount = 0;
	// the next consistency level to use.
	private ConsistencyLevel nextConsistencyLevel;
	// throw when giving up or ignore failures
	private boolean suppressFinalFailure = false;

	public ChangeConsistencyLevelRetryPolicy() { 
	}
	
	public ChangeConsistencyLevelRetryPolicy retryOnAllConditions(boolean condition) {
		retryOnAllConditions = condition;
		return this;
	}
	
	public ChangeConsistencyLevelRetryPolicy retryOnReadTimeouts(boolean condition) {
		retryOnReads = condition;
		retryOnAllConditions = false;
		return this;
	}

	public ChangeConsistencyLevelRetryPolicy retryOnWriteTimeouts(boolean condition) {
		retryOnWrites = condition;
		retryOnAllConditions = false;
		return this;
	}

	public ChangeConsistencyLevelRetryPolicy retryOnUnavailable(boolean condition) {
		retryOnUnavailable = condition;
		retryOnAllConditions = false;
		return this;
	}
	
	public ChangeConsistencyLevelRetryPolicy withNumRetries(int retries) {
		retryCount = retries;
		return this;
	}

	public ChangeConsistencyLevelRetryPolicy withNextConsistencyLevel(com.netflix.astyanax.model.ConsistencyLevel cl) {
		nextConsistencyLevel = ConsistencyLevelMapping.getCL(cl);
		return this;
	}
	
	public ChangeConsistencyLevelRetryPolicy suppressFinalFailure(boolean condition) {
		suppressFinalFailure = condition;
		return this;
	}

	private com.datastax.driver.core.policies.RetryPolicy jdRetry = new com.datastax.driver.core.policies.RetryPolicy() {

		@Override
		public RetryDecision onReadTimeout(Statement query, ConsistencyLevel cl, 
										  int requiredResponses, int receivedResponses,
										  boolean dataRetrieved, int nbRetry) {
			
			boolean shouldRetry = retryOnAllConditions || retryOnReads;
			return checkRetry(query, cl, shouldRetry);
		}

		@Override
		public RetryDecision onWriteTimeout(Statement query, ConsistencyLevel cl,
											WriteType writeType, int requiredAcks, int receivedAcks,
											int nbRetry) {
			
			boolean shouldRetry = retryOnAllConditions || retryOnWrites;
			return checkRetry(query, cl, shouldRetry);
		}

		@Override
		public RetryDecision onUnavailable(Statement query, ConsistencyLevel cl,
										   int requiredReplica, int aliveReplica, int nbRetry) {

			boolean shouldRetry = retryOnAllConditions || retryOnUnavailable;
			return checkRetry(query, cl, shouldRetry);
		}
	};

	@Override
	public com.datastax.driver.core.policies.RetryPolicy getJDRetryPolicy() {
		return jdRetry;
	}

	private RetryDecision checkRetry(Statement query, ConsistencyLevel cl, boolean shouldRetry) {
		
		if (!shouldRetry || retryCount <= 0) {
			// We are out of retries. 
			if (suppressFinalFailure) {
				return RetryDecision.ignore();
			} else {
				return RetryDecision.rethrow();
			}
		}
		
		// Ok we should retry and have some tries left.
		retryCount--;    // Note this retry
		
		// Check if the consistency level needs to be changed
		if (nextConsistencyLevel != null) {
			return RetryDecision.retry(nextConsistencyLevel);
		} else {
			return RetryDecision.retry(cl);
		}
	}
}>>>>>>> YOURS
/home/ramdisk/experiment3/projects/astyanax/revisions/rev_2b15fd9_3de01e3/rev_2b15fd9-3de01e3/astyanax-cql/src/main/java/com/netflix/astyanax/cql/retrypolicies/JavaDriverBasedRetryPolicy.java;<<<<<<< MINE
=======
package com.netflix.astyanax.cql.retrypolicies;

import com.netflix.astyanax.retry.RetryPolicy;

/**
 * Abstract base for all {@link RetryPolicy} implementation that want to use the retry policy from java driver.
 * @author poberai
 *
 */
public abstract class JavaDriverBasedRetryPolicy implements RetryPolicy {

	@Override
	public void begin() {
	}

	@Override
	public void success() {
	}

	@Override
	public void failure(Exception e) {
	}

	@Override
	public boolean allowRetry() {
		return false;
	}

	@Override
	public int getAttemptCount() {
		return 0;
	}

	@Override
	public RetryPolicy duplicate() {
		return null;
	}
	
	public abstract com.datastax.driver.core.policies.RetryPolicy getJDRetryPolicy();

}>>>>>>> YOURS
/home/ramdisk/experiment3/projects/astyanax/revisions/rev_2b15fd9_3de01e3/rev_2b15fd9-3de01e3/astyanax-cql/src/main/java/com/netflix/astyanax/cql/CqlClusterImpl.java;<<<<<<< MINE
=======
package com.netflix.astyanax.cql;

import static com.datastax.driver.core.querybuilder.QueryBuilder.eq;

import java.util.ArrayList;
import java.util.Collection;
import java.util.List;
import java.util.Map;
import java.util.Properties;

import com.codahale.metrics.MetricRegistryListener;
import com.datastax.driver.core.Cluster;
import com.datastax.driver.core.Configuration;
import com.datastax.driver.core.Row;
import com.datastax.driver.core.Session;
import com.datastax.driver.core.Statement;
import com.datastax.driver.core.querybuilder.QueryBuilder;
import com.google.common.base.Function;
import com.google.common.collect.Lists;
import com.netflix.astyanax.AstyanaxConfiguration;
import com.netflix.astyanax.Keyspace;
import com.netflix.astyanax.KeyspaceTracerFactory;
import com.netflix.astyanax.connectionpool.ConnectionPoolConfiguration;
import com.netflix.astyanax.connectionpool.ConnectionPoolMonitor;
import com.netflix.astyanax.connectionpool.ConnectionPoolProxy.SeedHostListener;
import com.netflix.astyanax.connectionpool.Host;
import com.netflix.astyanax.connectionpool.OperationResult;
import com.netflix.astyanax.connectionpool.exceptions.ConnectionException;
import com.netflix.astyanax.cql.schema.CqlColumnDefinitionImpl;
import com.netflix.astyanax.cql.schema.CqlColumnFamilyDefinitionImpl;
import com.netflix.astyanax.cql.schema.CqlKeyspaceDefinitionImpl;
import com.netflix.astyanax.ddl.ColumnDefinition;
import com.netflix.astyanax.ddl.ColumnFamilyDefinition;
import com.netflix.astyanax.ddl.KeyspaceDefinition;
import com.netflix.astyanax.ddl.SchemaChangeResult;
import com.yammer.metrics.core.MetricsRegistryListener;

/**
 * Java Driver based impl of {@link Cluster} that implements ddl operations.
 * The class encapsulates a java driver cluster and session object to provide all the functionality. 
 *  
 * Note that due to the way the object is setup via AstyanaxContext and CqlFamilyFactory, it needs to implements 
 * a {@link SeedHostListener} so that it can construct the cluster and session object appropriately once the seed hosts
 * have been provided by the {@link HostSupplier} object
 *  
 * @author poberai
 */
public class CqlClusterImpl implements com.netflix.astyanax.Cluster, SeedHostListener {

	public volatile Cluster cluster;
	private volatile Session session;
	private final AstyanaxConfiguration astyanaxConfig; 
	private final KeyspaceTracerFactory tracerFactory; 
	private final Configuration javaDriverConfig; 
	private final ConnectionPoolMonitor cpMonitor;
	private final MetricsRegistryListener metricsRegListener;
	
	public CqlClusterImpl(AstyanaxConfiguration asConfig, KeyspaceTracerFactory tracerFactory, ConnectionPoolConfiguration cpConfig, ConnectionPoolMonitor cpMonitor) {
		this.astyanaxConfig = asConfig;
		this.tracerFactory = tracerFactory;
		this.javaDriverConfig = ((JavaDriverConnectionPoolConfigurationImpl)cpConfig).getJavaDriverConfig();
		this.cpMonitor = cpMonitor;
		this.metricsRegListener = ((JavaDriverConnectionPoolMonitorImpl)cpMonitor).getMetricsRegistryListener();
	}

	@Override
	public String describeClusterName() throws ConnectionException {
		return cluster.getMetadata().getClusterName();
	}

	@Override
	public String getVersion() throws ConnectionException {
		
		Statement query = QueryBuilder.select("release_version")
								  .from("system", "local")
								  .where(eq("key", "local"));
		
		return session.execute(query).one().getString("release_version"); 
	}

	public void shutdown() {
		cluster.shutdown();
	}
	
	@Override
	public String describeSnitch() throws ConnectionException {
		throw new UnsupportedOperationException("Operation not supported");
	}

	@Override
	public String describePartitioner() throws ConnectionException {
		Statement query = QueryBuilder.select("partitioner")
				.from("system", "local")
				.where(eq("key", "local"));

		return session.execute(query).one().getString("partitioner"); 
	}

	@Override
	public Map<String, List<String>> describeSchemaVersions() throws ConnectionException {
		return new CqlSchemaVersionReader(session).exec();
	}

	@Override
	public KeyspaceDefinition makeKeyspaceDefinition() {
        return new CqlKeyspaceDefinitionImpl(session);
	}

	@Override
	public Properties getAllKeyspaceProperties() throws ConnectionException {

		Properties properties = new Properties();
		try {
			List<KeyspaceDefinition> ksDefs = describeKeyspaces();
			for(KeyspaceDefinition ksDef : ksDefs) {
				Properties ksProps = ksDef.getProperties();
				for (Object key : ksProps.keySet()) {
					properties.put(ksDef.getName() + "." + key, ksProps.get(key));
				}
			}
		} catch (Exception e) {
			throw new RuntimeException(e);
		}
		return properties;
	}

	@Override
	public Properties getKeyspaceProperties(String keyspace) throws ConnectionException {
		
		try {
			return describeKeyspace(keyspace.toLowerCase()).getProperties();
		} catch (Exception e) {
			throw new RuntimeException(e);
		}
	}

	@Override
	public List<KeyspaceDefinition> describeKeyspaces() throws ConnectionException {
		
		Statement query = QueryBuilder.select().all().from("system", "schema_keyspaces");

		List<KeyspaceDefinition> ksDefs = new ArrayList<KeyspaceDefinition>();
		try {
			for(Row row : session.execute(query).all()) {
				String keyspaceName = row.getString("keyspace_name");
				if (keyspaceName.equals("system") || keyspaceName.startsWith("system_")) {
					continue;
				}
				ksDefs.add(new CqlKeyspaceDefinitionImpl(session, row));
			}
			return ksDefs;
		} catch (Exception e) {
			throw new RuntimeException(e);
		}
	}

	@Override
	public KeyspaceDefinition describeKeyspace(String ksName) throws ConnectionException {
		return new CqlKeyspaceImpl(session, ksName, astyanaxConfig, tracerFactory,cpMonitor).describeKeyspace();
	}

	@Override
	public Keyspace getKeyspace(String keyspace) throws ConnectionException {
		return new CqlKeyspaceImpl(session, keyspace, astyanaxConfig, tracerFactory,cpMonitor);
	}

	@Override
	public OperationResult<SchemaChangeResult> dropKeyspace(String keyspaceName) throws ConnectionException {
		return new CqlKeyspaceImpl(session, keyspaceName.toLowerCase(), astyanaxConfig, tracerFactory,cpMonitor).dropKeyspace();
	}

	@Override
	public OperationResult<SchemaChangeResult> addKeyspace(KeyspaceDefinition def) throws ConnectionException {
		return ((CqlKeyspaceDefinitionImpl)def).execute();
	}

	@Override
	public OperationResult<SchemaChangeResult> updateKeyspace(KeyspaceDefinition def) throws ConnectionException {
		return ((CqlKeyspaceDefinitionImpl)def).alterKeyspace().execute();
	}

	@Override
	public OperationResult<SchemaChangeResult> createKeyspace(Map<String, Object> options) throws ConnectionException {
		String keyspaceName = (String) options.remove("name");
		if (keyspaceName == null) {
			throw new RuntimeException("Options missing 'name' property for keyspace name");
		}
		return new CqlKeyspaceDefinitionImpl(session, options).setName(keyspaceName).execute();
	}

	@Override
	public OperationResult<SchemaChangeResult> createKeyspace(Properties props) throws ConnectionException {
		String keyspaceName = (String) props.remove("name");
		if (keyspaceName == null) {
			throw new RuntimeException("Options missing 'name' property for keyspace name");
		}
		return new CqlKeyspaceDefinitionImpl(session, props).setName(keyspaceName).execute();
	}

	@Override
	public OperationResult<SchemaChangeResult> updateKeyspace(Map<String, Object> options) throws ConnectionException {
		String keyspaceName = (String) options.remove("name");
		if (keyspaceName == null) {
			throw new RuntimeException("Options missing 'name' property for keyspace name");
		}
		return new CqlKeyspaceDefinitionImpl(session, options).setName(keyspaceName).alterKeyspace().execute();
	}

	@Override
	public OperationResult<SchemaChangeResult> updateKeyspace(Properties props) throws ConnectionException {
		String keyspaceName = (String) props.remove("name");
		if (keyspaceName == null) {
			throw new RuntimeException("Options missing 'name' property for keyspace name");
		}
		return new CqlKeyspaceDefinitionImpl(session, props).setName(keyspaceName).alterKeyspace().execute();
	}
	
	@Override
	public AstyanaxConfiguration getConfig() {
		return astyanaxConfig;
	}

	@Override
	public ColumnFamilyDefinition makeColumnFamilyDefinition() {
		return new CqlColumnFamilyDefinitionImpl(session);
	}

	@Override
	public ColumnDefinition makeColumnDefinition() {
		return new CqlColumnDefinitionImpl();
	}
	
	@Override
	public Properties getColumnFamilyProperties(String keyspace, String columnfamilyName) throws ConnectionException {
		try {
			return new CqlKeyspaceDefinitionImpl(session).setName(keyspace).getColumnFamily(columnfamilyName).getProperties();
		} catch (Exception e) {
			throw new RuntimeException(e);
		}
	}

	@Override
	public OperationResult<SchemaChangeResult> createColumnFamily(Map<String, Object> options) throws ConnectionException {
		return new CqlColumnFamilyDefinitionImpl(session, null, options).execute();
	}

	@Override
	public OperationResult<SchemaChangeResult> createColumnFamily(Properties props) throws ConnectionException {
		return new CqlColumnFamilyDefinitionImpl(session, null, props).execute();
	}

	@Override
	public OperationResult<SchemaChangeResult> updateColumnFamily(Map<String, Object> options) throws ConnectionException {
		return new CqlColumnFamilyDefinitionImpl(session, null, options).alterTable().execute();
	}

	@Override
	public OperationResult<SchemaChangeResult> updateColumnFamily(Properties props) throws ConnectionException {
		return new CqlColumnFamilyDefinitionImpl(session, null, props).alterTable().execute();
	}

	@Override
	public OperationResult<SchemaChangeResult> dropColumnFamily(String keyspaceName, String columnFamilyName) throws ConnectionException {
		return new CqlKeyspaceImpl(session, keyspaceName, astyanaxConfig, tracerFactory,cpMonitor).dropColumnFamily(columnFamilyName);
	}

	@Override
	public OperationResult<SchemaChangeResult> addColumnFamily(ColumnFamilyDefinition def) throws ConnectionException {
		return ((CqlColumnFamilyDefinitionImpl)def).execute();
	}

	@Override
	public OperationResult<SchemaChangeResult> updateColumnFamily(ColumnFamilyDefinition def) throws ConnectionException {
		return ((CqlColumnFamilyDefinitionImpl)def).alterTable().execute();
	}

	@Override
	public void setHosts(Collection<Host> hosts, int port) {
		
		List<Host> hostList = Lists.newArrayList(hosts);
		
		List<String> contactPoints = Lists.transform(hostList, new Function<Host, String>() {
			@Override
			public String apply(Host input) {
				if (input != null) {
					return input.getHostName(); 
				}
				return null;
			}
		});
		
		Configuration config = javaDriverConfig;
		
		// We really need a mechanism to easily override Configuration on the builder
		Cluster.Builder builder = Cluster.builder()
				.addContactPoints(contactPoints.toArray(new String[0]))
				.withPort(port)
				.withLoadBalancingPolicy(config.getPolicies().getLoadBalancingPolicy())
				.withReconnectionPolicy(config.getPolicies().getReconnectionPolicy())
				.withRetryPolicy(config.getPolicies().getRetryPolicy())
				.withCompression(config.getProtocolOptions().getCompression())
				.withPoolingOptions(config.getPoolingOptions())
				.withSocketOptions(config.getSocketOptions())
				.withQueryOptions(config.getQueryOptions());
		
		if (config.getMetricsOptions() == null) {
			builder.withoutMetrics();
		} else if (!config.getMetricsOptions().isJMXReportingEnabled()) {
			builder.withoutJMXReporting();
		}
				
		this.cluster = builder.build(); 
		if (!(this.cpMonitor instanceof JavaDriverConnectionPoolMonitorImpl))
			this.cluster.getMetrics().getRegistry().addListener((MetricRegistryListener) this.metricsRegListener);
		this.session = cluster.connect();
	}
}>>>>>>> YOURS
/home/ramdisk/experiment3/projects/astyanax/revisions/rev_2b15fd9_3de01e3/rev_2b15fd9-3de01e3/astyanax-cql/src/main/java/com/netflix/astyanax/cql/JavaDriverConnectionPoolConfigurationImpl.java;<<<<<<< MINE
=======
package com.netflix.astyanax.cql;

import java.util.List;
import java.util.concurrent.ScheduledExecutorService;

import com.datastax.driver.core.Configuration;
import com.netflix.astyanax.AstyanaxContext;
import com.netflix.astyanax.AuthenticationCredentials;
import com.netflix.astyanax.connectionpool.BadHostDetector;
import com.netflix.astyanax.connectionpool.ConnectionPoolConfiguration;
import com.netflix.astyanax.connectionpool.Host;
import com.netflix.astyanax.connectionpool.LatencyScoreStrategy;
import com.netflix.astyanax.connectionpool.OperationFilterFactory;
import com.netflix.astyanax.connectionpool.RetryBackoffStrategy;
import com.netflix.astyanax.connectionpool.SSLConnectionContext;
import com.netflix.astyanax.connectionpool.impl.HostSelectorStrategy;
import com.netflix.astyanax.partitioner.Partitioner;

/**
 * This class simply acts as a holder for the {@link Configuration} object for the java driver. It can be injected into the 
 * {@link AstyanaxContext} via the regular interface and thus helps serve as a bridge when setting up the java driver using the 
 * regular Astyanax setup route. 
 * 
 * The class does not actually implement any of the actual methods of {@link ConnectionPoolConfiguration}. It's sole purpose is just to 
 * hold a reference to the java driver config object and then be injected via the regular interfaces available in AstyanaxContext. 
 * 
 * @author poberai
 *
 */
public class JavaDriverConnectionPoolConfigurationImpl implements ConnectionPoolConfiguration {

	private Configuration jdConfig = new Configuration(); 
	
	public JavaDriverConnectionPoolConfigurationImpl withJavaDriverConfig(Configuration jdCfg) {
		jdConfig = jdCfg;
		return this;
	}
	
	public Configuration getJavaDriverConfig() {
		return jdConfig;
	}

	@Override
	public LatencyScoreStrategy getLatencyScoreStrategy() {
		return null;
	}

	@Override
	public BadHostDetector getBadHostDetector() {
		return null;
	}

	@Override
	public int getPort() {
		return jdConfig.getProtocolOptions().getPort();
	}

	@Override
	public String getName() {
		return null;
	}

	@Override
	public int getMaxConnsPerHost() {
		return 0;
	}

	@Override
	public int getInitConnsPerHost() {
		return 0;
	}

	@Override
	public int getMaxConns() {
		return 0;
	}

	@Override
	public int getMaxTimeoutWhenExhausted() {
		return 0;
	}

	@Override
	public int getMaxFailoverCount() {
		return 0;
	}

	@Override
	public RetryBackoffStrategy getRetryBackoffStrategy() {
		return null;
	}

	@Override
	public HostSelectorStrategy getHostSelectorStrategy() {
		return null;
	}

	@Override
	public String getSeeds() {
		return null;
	}

	@Override
	public List<Host> getSeedHosts() {
		return null;
	}

	@Override
	public String getLocalDatacenter() {
		return null;
	}

	@Override
	public int getSocketTimeout() {
		return 0;
	}

	@Override
	public int getConnectTimeout() {
		return 0;
	}

	@Override
	public int getConnectionLimiterWindowSize() {
		return 0;
	}

	@Override
	public int getConnectionLimiterMaxPendingCount() {
		return 0;
	}

	@Override
	public int getLatencyAwareWindowSize() {
		return 0;
	}

	@Override
	public float getLatencyAwareSentinelCompare() {
		return 0;
	}

	@Override
	public float getLatencyAwareBadnessThreshold() {
		return 0;
	}

	@Override
	public int getBlockedThreadThreshold() {
		return 0;
	}

	@Override
	public float getMinHostInPoolRatio() {
		return 0;
	}

	@Override
	public int getLatencyAwareUpdateInterval() {
		return 0;
	}

	@Override
	public int getLatencyAwareResetInterval() {
		return 0;
	}

	@Override
	public int getMaxPendingConnectionsPerHost() {
		return 0;
	}

	@Override
	public int getMaxBlockedThreadsPerHost() {
		return 0;
	}

	@Override
	public int getTimeoutWindow() {
		return 0;
	}

	@Override
	public int getMaxTimeoutCount() {
		return 0;
	}

	@Override
	public int getRetrySuspendWindow() {
		return 0;
	}

	@Override
	public int getRetryMaxDelaySlice() {
		return 0;
	}

	@Override
	public int getRetryDelaySlice() {
		return 0;
	}

	@Override
	public int getMaxOperationsPerConnection() {
		return 0;
	}

	@Override
	public AuthenticationCredentials getAuthenticationCredentials() {
		return null;
	}

	@Override
	public OperationFilterFactory getOperationFilterFactory() {
		return null;
	}

	@Override
	public Partitioner getPartitioner() {
		return null;
	}

	@Override
	public SSLConnectionContext getSSLConnectionContext() {
		return null;
	}

	@Override
	public ScheduledExecutorService getMaintainanceScheduler() {
		return null;
	}

	@Override
	public ScheduledExecutorService getHostReconnectExecutor() {
		return null;
	}

	@Override
	public void initialize() {
	}

	@Override
	public void shutdown() {
	}
}>>>>>>> YOURS
/home/ramdisk/experiment3/projects/astyanax/revisions/rev_2b15fd9_3de01e3/rev_2b15fd9-3de01e3/astyanax-cql/src/main/java/com/netflix/astyanax/cql/ConsistencyLevelMapping.java;<<<<<<< MINE
=======
package com.netflix.astyanax.cql;

import com.netflix.astyanax.model.ConsistencyLevel;

/**
 * Helper class for translating Astyanax consistency level to java driver consistency level
 * 
 * @author poberai
 */
public class ConsistencyLevelMapping {

	public static com.datastax.driver.core.ConsistencyLevel getCL(ConsistencyLevel cl) {
		
		switch (cl) {
		
		case CL_ONE:
			return com.datastax.driver.core.ConsistencyLevel.ONE;
		case CL_TWO:
			return com.datastax.driver.core.ConsistencyLevel.TWO;
		case CL_THREE:
			return com.datastax.driver.core.ConsistencyLevel.THREE;
		case CL_QUORUM:
			return com.datastax.driver.core.ConsistencyLevel.QUORUM;
		case CL_LOCAL_QUORUM:
			return com.datastax.driver.core.ConsistencyLevel.LOCAL_QUORUM;
		case CL_EACH_QUORUM:
			return com.datastax.driver.core.ConsistencyLevel.EACH_QUORUM;
		case CL_ALL:
			return com.datastax.driver.core.ConsistencyLevel.ALL;
		case CL_ANY:
			return com.datastax.driver.core.ConsistencyLevel.ANY;
		default:
			throw new RuntimeException("CL Level not recognized: " + cl.name());
		}
	}
}>>>>>>> YOURS
/home/ramdisk/experiment3/projects/astyanax/revisions/rev_2b15fd9_3de01e3/rev_2b15fd9-3de01e3/astyanax-cql/src/main/java/com/netflix/astyanax/cql/CqlOperationResultImpl.java;<<<<<<< MINE
=======
package com.netflix.astyanax.cql;

import java.net.InetAddress;
import java.util.concurrent.TimeUnit;

import com.datastax.driver.core.ExecutionInfo;
import com.datastax.driver.core.QueryTrace;
import com.datastax.driver.core.ResultSet;
import com.netflix.astyanax.connectionpool.Host;
import com.netflix.astyanax.connectionpool.OperationResult;

/**
 * Simple impl of {@link OperationResult} that tracks some basic info for every operation execution, such as
 * 1. The host that was used for the operation
 * 2. The operation attempt count
 * 3. The encapsulated result
 * 4. The overall latency for the operation. 
 * 
 * @author poberai
 *
 * @param <R>
 */
public class CqlOperationResultImpl<R> implements OperationResult<R> {

	private Host host;
	private R result; 
	private int attemptCount = 0;
	private long durationMicros = 0L;
	
	public CqlOperationResultImpl(ResultSet rs, R result) {
		this.host = parseHostInfo(rs);
		this.result = result;
		this.durationMicros = parseDuration(rs);
	}
	
	private Host parseHostInfo(ResultSet rs) {
		
		if (rs == null) {
			return null;
		}
		
		com.datastax.driver.core.Host fromHost = rs.getExecutionInfo().getQueriedHost();
		InetAddress add = fromHost.getAddress();
		
		Host toHost = new Host(add.getHostAddress(), -1);
		toHost.setRack(fromHost.getRack());
		return toHost;
	}
	
	private long parseDuration(ResultSet rs) {
		if (rs != null) {
			ExecutionInfo info = rs.getExecutionInfo();
			if (info !=null) {
				QueryTrace qt = info.getQueryTrace();
				if (qt != null) {
					return qt.getDurationMicros();
				}
			}
		}
		return 0L;
	}

	@Override
	public Host getHost() {
		return host;
	}

	@Override
	public R getResult() {
		return result;
	}

	@Override
	public long getLatency() {
		return durationMicros;
	}

	@Override
	public long getLatency(TimeUnit units) {
		return units.convert(durationMicros, TimeUnit.MICROSECONDS);
	}

	@Override
	public int getAttemptsCount() {
		return attemptCount;
	}

	@Override
	public void setAttemptsCount(int count) {
		attemptCount = count;
	}

}>>>>>>> YOURS
/home/ramdisk/experiment3/projects/astyanax/revisions/rev_2b15fd9_3de01e3/rev_2b15fd9-3de01e3/astyanax-cql/src/main/java/com/netflix/astyanax/cql/JavaDriverConfigBuilder.java;<<<<<<< MINE
=======
package com.netflix.astyanax.cql;

import java.util.concurrent.TimeUnit;

import com.datastax.driver.core.Configuration;
import com.datastax.driver.core.ConsistencyLevel;
import com.datastax.driver.core.HostDistance;
import com.datastax.driver.core.MetricsOptions;
import com.datastax.driver.core.PoolingOptions;
import com.datastax.driver.core.ProtocolOptions;
import com.datastax.driver.core.QueryOptions;
import com.datastax.driver.core.SocketOptions;
import com.datastax.driver.core.policies.DefaultRetryPolicy;
import com.datastax.driver.core.policies.ExponentialReconnectionPolicy;
import com.datastax.driver.core.policies.LoadBalancingPolicy;
import com.datastax.driver.core.policies.Policies;
import com.datastax.driver.core.policies.ReconnectionPolicy;
import com.datastax.driver.core.policies.RetryPolicy;
import com.datastax.driver.core.policies.RoundRobinPolicy;

/**
 * Helpful builder style class for configuring JavaDriver. 
 * 
 * @author poberai
 *
 */
public class JavaDriverConfigBuilder extends JavaDriverConnectionPoolConfigurationImpl {

	// Config for Policies
    private LoadBalancingPolicy loadBalancingPolicy = new RoundRobinPolicy();
    private ReconnectionPolicy reconnectionPolicy = new ExponentialReconnectionPolicy(1000, 10 * 60 * 1000);
    private RetryPolicy retryPolicy = DefaultRetryPolicy.INSTANCE;

    // Config for ProtocolOptions
    private int nativeProtocolPort = -1; 
    
    // Config for PoolingOptions
    private PoolingOptions poolingOptions = new PoolingOptions();
    
    // Config for SocketOptions
    private SocketOptions socketOptions = new SocketOptions();
    
    // Config for MetricsOptions
    private boolean jmxReportingEnabled = true;
    
    // Config for QueryOptions
    private QueryOptions queryOptions = new QueryOptions();
    
	public JavaDriverConfigBuilder() {
		super();
	}
	
	public JavaDriverConfigBuilder build() {
		
		Policies policies = new Policies(loadBalancingPolicy, reconnectionPolicy, retryPolicy);
		ProtocolOptions protocolOptions = (nativeProtocolPort == -1) ? new ProtocolOptions() : new ProtocolOptions(nativeProtocolPort);
		PoolingOptions poolOptions = poolingOptions;
		SocketOptions sockOptions = socketOptions;
		MetricsOptions metricsOptions = new MetricsOptions(jmxReportingEnabled);
		QueryOptions qOptions = queryOptions;
		
		super.withJavaDriverConfig(new Configuration(policies, 
													 protocolOptions, 
													 poolOptions, 
													 sockOptions, 
													 metricsOptions, 
													 qOptions));
		return this;
	}
	
	
	public JavaDriverConfigBuilder withLoadBalancingPolicy(LoadBalancingPolicy lbPolicy) {
		this.loadBalancingPolicy = lbPolicy;
		return this;
	}
	
	public JavaDriverConfigBuilder withReconnectionPolicy(ReconnectionPolicy reconnectPolicy) {
		this.reconnectionPolicy = reconnectPolicy;
		return this;
	}
	
	public JavaDriverConfigBuilder withRetryPolicy(RetryPolicy rPolicy) {
		this.retryPolicy = rPolicy;
		return this;
	}
	
	public JavaDriverConfigBuilder withPort(int nativePort) {
		this.nativeProtocolPort = nativePort;
		return this;
	}
	
	public JavaDriverConfigBuilder withCoreConnsPerHost(HostDistance distance, int coreConnections) {
		this.poolingOptions.setCoreConnectionsPerHost(distance, coreConnections);
		return this;
	}

	public JavaDriverConfigBuilder withMaxConnsPerHost(HostDistance distance, int maxConnections) {
		this.poolingOptions.setMaxConnectionsPerHost(distance, maxConnections);
		return this;
	}

	public JavaDriverConfigBuilder withMinRequestsPerConnection(HostDistance distance, int minRequests) {
		this.poolingOptions.setMinSimultaneousRequestsPerConnectionThreshold(distance, minRequests);
		return this;
	}

	public JavaDriverConfigBuilder withMaxRequestsPerConnection(HostDistance distance, int maxRequests) {
		this.poolingOptions.setMaxSimultaneousRequestsPerConnectionThreshold(distance, maxRequests);
		return this;
	}

	public JavaDriverConfigBuilder withConnectTimeout(int timeout, TimeUnit sourceUnit) {
		Long connectTimeoutMillis = TimeUnit.MILLISECONDS.convert(timeout, sourceUnit);
		this.socketOptions.setConnectTimeoutMillis(connectTimeoutMillis.intValue());
		return this;
	}

	public JavaDriverConfigBuilder withReadTimeout(int timeout, TimeUnit sourceUnit) {
		Long readTimeoutMillis = TimeUnit.MILLISECONDS.convert(timeout, sourceUnit);
		this.socketOptions.setReadTimeoutMillis(readTimeoutMillis.intValue());
		return this;
	}
	
	 public JavaDriverConfigBuilder withKeepAlive(boolean keepAlive) {
		 this.socketOptions.setKeepAlive(keepAlive);
		 return this;
	 }

	 public JavaDriverConfigBuilder withReuseAddress(boolean reuseAddress) {
		 this.socketOptions.setReuseAddress(reuseAddress);
		 return this;
	 }

	 public JavaDriverConfigBuilder withSoLinger(int soLinger) {
		 this.socketOptions.setSoLinger(soLinger);
		 return this;
	 }

	 public JavaDriverConfigBuilder withTcpNoDelay(boolean tcpNoDelay) {
		 this.socketOptions.setTcpNoDelay(tcpNoDelay);
		 return this;
	 }

	 public JavaDriverConfigBuilder withReceiveBufferSize(int receiveBufferSize) {
		 this.socketOptions.setReceiveBufferSize(receiveBufferSize);
		 return this;
	 }

	 public JavaDriverConfigBuilder withSendBufferSize(int sendBufferSize) {
		 this.socketOptions.setSendBufferSize(sendBufferSize);
		 return this;
	 }
	 
	 public JavaDriverConfigBuilder withJmxReportingEnabled(boolean enabled) {
		 this.jmxReportingEnabled = enabled;
		 return this;
	 }
	 
	 public JavaDriverConfigBuilder withConsistencyLevel(ConsistencyLevel consistencyLevel) {
		 this.queryOptions.setConsistencyLevel(consistencyLevel);
		 return this;
	 }
	 
	 public JavaDriverConfigBuilder withSerialConsistencyLevel(ConsistencyLevel consistencyLevel) {
		 this.queryOptions.setSerialConsistencyLevel(consistencyLevel);
		 return this;
	 }
	
	 public JavaDriverConfigBuilder withFetchSize(int fetchSize) {
		 this.queryOptions.setFetchSize(fetchSize);
		 return this;
	 }
}>>>>>>> YOURS
/home/ramdisk/experiment3/projects/astyanax/revisions/rev_2b15fd9_3de01e3/rev_2b15fd9-3de01e3/astyanax-cql/src/main/java/com/netflix/astyanax/cql/CqlAbstractExecutionImpl.java;<<<<<<< MINE
=======
package com.netflix.astyanax.cql;

import org.slf4j.Logger;
import org.slf4j.LoggerFactory;

import com.datastax.driver.core.ResultSet;
import com.datastax.driver.core.ResultSetFuture;
import com.datastax.driver.core.Session;
import com.datastax.driver.core.Statement;
import com.google.common.util.concurrent.ListenableFuture;
import com.netflix.astyanax.CassandraOperationCategory;
import com.netflix.astyanax.CassandraOperationTracer;
import com.netflix.astyanax.CassandraOperationType;
import com.netflix.astyanax.Execution;
import com.netflix.astyanax.KeyspaceTracerFactory;
import com.netflix.astyanax.connectionpool.OperationResult;
import com.netflix.astyanax.connectionpool.exceptions.ConnectionException;
import com.netflix.astyanax.connectionpool.exceptions.IsRetryableException;
import com.netflix.astyanax.connectionpool.exceptions.NotFoundException;
import com.netflix.astyanax.connectionpool.exceptions.OperationException;
import com.netflix.astyanax.cql.CqlKeyspaceImpl.KeyspaceContext;
import com.netflix.astyanax.cql.retrypolicies.JavaDriverBasedRetryPolicy;
import com.netflix.astyanax.cql.util.AsyncOperationResult;
import com.netflix.astyanax.cql.util.CFQueryContext;
import com.netflix.astyanax.model.ColumnFamily;
import com.netflix.astyanax.model.ConsistencyLevel;
import com.netflix.astyanax.retry.RetryPolicy;

/**
 * Abstract class that encapsulates the functionality for executing an operations using the native protocol based java driver
 * Note that class provides only the operation agnostic functionality such as retries, tracking metrics etc. 
 * The actual logic for constructing the query for the operation and then parsing the result set of the operation is left
 * to the implementation of the extending class. 
 * 
 * @author poberai
 *
 * @param <R>
 */
public abstract class CqlAbstractExecutionImpl<R> implements Execution<R> {
	
	private static final Logger LOG = LoggerFactory.getLogger(CqlAbstractExecutionImpl.class);
	
	// The session for executing the query
	protected final Session session;
	// The keyspace being operated on
	protected final String keyspace;
	// The CF being operated on
	protected final ColumnFamily<?, ?> cf;
	// Factory for vending operation metrics
	protected final KeyspaceTracerFactory tracerFactory;
	// Retry policy
	protected final RetryPolicy retry;
	// ConsistencyLevel
	protected final com.datastax.driver.core.ConsistencyLevel clLevel; 
	
	public CqlAbstractExecutionImpl(KeyspaceContext ksContext, CFQueryContext<?,?> cfContext) {
		
		this.session = ksContext.getSession();
		this.keyspace = ksContext.getKeyspace();
		this.cf = (cfContext != null) ? cfContext.getColumnFamily() : null;
		this.tracerFactory = ksContext.getTracerFactory();
		
		// process the override retry policy first
		RetryPolicy retryPolicy = ksContext.getConfig().getRetryPolicy();
		retry = (retryPolicy != null) ? retryPolicy : getRetryPolicy(cfContext.getRetryPolicy()); 
		
		clLevel = resolveConsistencyLevel(ksContext, cfContext);
	}

	public CqlAbstractExecutionImpl(KeyspaceContext ksContext, RetryPolicy retryPolicy) {
		
		this.session = ksContext.getSession();
		this.keyspace = ksContext.getKeyspace();
		this.cf = null;
		this.tracerFactory = ksContext.getTracerFactory();
		
		// process the override retry policy first
		retry = (retryPolicy != null) ? retryPolicy : getRetryPolicy(ksContext.getConfig().getRetryPolicy());
		clLevel = resolveConsistencyLevel(ksContext, null);
	}

	@Override
	public OperationResult<R> execute() throws ConnectionException {
		
        ConnectionException lastException = null;
        
        retry.begin();

        do {
        	try {
                return executeOp();
            } catch (RuntimeException ex) {
            	lastException = new OperationException(ex);
            } catch (ConnectionException ex) {
                if (ex instanceof IsRetryableException)
                    lastException = ex;
                else
                    throw ex;
            }
        } while (retry.allowRetry());

        throw lastException;
	}

	
	private OperationResult<R> executeOp() throws ConnectionException {
		CassandraOperationTracer tracer = null;
		
		if (cf != null) {
			tracer = tracerFactory.newTracer(getOperationType(), cf);
		} else {
			tracer = tracerFactory.newTracer(getOperationType());
		}
		
		tracer.start();
		Statement query = getQuery();
		
		if (LOG.isDebugEnabled()) {
			LOG.debug("Query: " + query);
		}
		
        // Set the consistency level on the query
        query.setConsistencyLevel(clLevel);

        // Set the retry policy on the query
        if (retry instanceof JavaDriverBasedRetryPolicy) {
        	JavaDriverBasedRetryPolicy jdRetryPolicy = (JavaDriverBasedRetryPolicy) retry;
        	query.setRetryPolicy(jdRetryPolicy.getJDRetryPolicy());
        }

		ResultSet resultSet = session.execute(query);
		R result = parseResultSet(resultSet);
		OperationResult<R> opResult = new CqlOperationResultImpl<R>(resultSet, result);
		opResult.setAttemptsCount(retry.getAttemptCount());
		tracer.success();
		return opResult;
	}
	
	@Override
	public ListenableFuture<OperationResult<R>> executeAsync() throws ConnectionException {
		final CassandraOperationTracer tracer = tracerFactory.newTracer(getOperationType());
		tracer.start();
		
		Statement query = getQuery();
		
		if (LOG.isDebugEnabled()) {
			LOG.debug("Query: " + query);
		}
		
		ResultSetFuture rsFuture = session.executeAsync(query);
		return new AsyncOperationResult<R>(rsFuture) {
			@Override
			public OperationResult<R> getOperationResult(ResultSet resultSet) {
				R result = null;
				try {
					result = parseResultSet(resultSet);
				} catch (NotFoundException e) {
					e.printStackTrace();
				}
				tracer.success();
				OperationResult<R> opResult = new CqlOperationResultImpl<R>(resultSet, result);
				opResult.setAttemptsCount(retry.getAttemptCount());
				return opResult;
			}
		};
	}
	
	private RetryPolicy getRetryPolicy(RetryPolicy policy) {
		if (policy != null) {
			return policy.duplicate();
		} else {
			return null;
		}
	}
	
	private ConsistencyLevel getDefaultCL(KeyspaceContext ksContext) {
		
		ConsistencyLevel clLevel = ksContext.getConfig().getDefaultReadConsistencyLevel(); 
		
		CassandraOperationCategory op = getOperationType().getCategory();
		switch (op) {
		case READ:
			clLevel = ksContext.getConfig().getDefaultReadConsistencyLevel(); 
			break;
		case WRITE:
			clLevel = ksContext.getConfig().getDefaultWriteConsistencyLevel();
		default:
			clLevel = ksContext.getConfig().getDefaultReadConsistencyLevel(); 
		}
		
		return clLevel;
	}
	
	private com.datastax.driver.core.ConsistencyLevel resolveConsistencyLevel(KeyspaceContext ksContext, CFQueryContext<?,?> cfContext) {
		ConsistencyLevel clLevel = null; 
		if (cfContext != null) {
			clLevel = cfContext.getConsistencyLevel();
		}
		if (clLevel == null) {
			clLevel = getDefaultCL(ksContext);
		}
		return ConsistencyLevelMapping.getCL(clLevel);
	}
	
	/**
	 * Specify what operation type this is. Used for emitting the right tracers
	 * @return CassandraOperationType
	 */
	public abstract CassandraOperationType getOperationType();
	
	/**
	 * Get the Query for this operation
	 * @return Query
	 */
	public abstract Statement getQuery();

	/**
	 * Parse the result set to get the required response
	 * @param resultSet
	 * @return
	 */
	public abstract R parseResultSet(ResultSet resultSet) throws NotFoundException;
}>>>>>>> YOURS
/home/ramdisk/experiment3/projects/astyanax/revisions/rev_2b15fd9_3de01e3/rev_2b15fd9-3de01e3/astyanax-cql/src/main/java/com/netflix/astyanax/cql/JavaDriverConnectionPoolMonitorImpl.java;<<<<<<< MINE
=======
package com.netflix.astyanax.cql;

import java.util.Map;
import java.util.concurrent.atomic.AtomicReference;

import com.datastax.driver.core.Cluster;
import com.netflix.astyanax.connectionpool.ConnectionPoolMonitor;
import com.netflix.astyanax.connectionpool.Host;
import com.netflix.astyanax.connectionpool.HostConnectionPool;
import com.netflix.astyanax.connectionpool.HostStats;
import com.yammer.metrics.core.Metric;
import com.yammer.metrics.core.MetricName;
import com.yammer.metrics.core.MetricsRegistryListener;

public class JavaDriverConnectionPoolMonitorImpl implements
		ConnectionPoolMonitor {

	private final AtomicReference<Cluster> cluster = new AtomicReference<Cluster>();
	
	private MetricsRegistryListener metricsRegListener = new MetricsRegistryListener(){

		@Override
		public void onMetricAdded(MetricName name, Metric metric) {
			// TODO Auto-generated method stub			
		}

		@Override
		public void onMetricRemoved(MetricName name) {
			// TODO Auto-generated method stub			
		}};
	
	public JavaDriverConnectionPoolMonitorImpl() {	
	}
	
	public JavaDriverConnectionPoolMonitorImpl withJavaDriverMetricsRegistry(MetricsRegistryListener metricsRegListener){
		this.metricsRegListener = metricsRegListener;
		return this;
	}
	
	public MetricsRegistryListener getMetricsRegistryListener(){
		return metricsRegListener;
	}
	
    /**
     * Returns the number of Cassandra hosts currently known by the driver (that is 
     * whether they are currently considered up or down).
     *
     * @return the number of Cassandra hosts currently known by the driver.
     */
	@Override
	public long getHostCount() {
		return cluster.get().getMetrics().getKnownHosts().getValue();
	}

    /**
     * Returns the number of Cassandra hosts the driver is currently connected to
     * (that is have at least one connection opened to).
     *
     * @return the number of Cassandra hosts the driver is currently connected to.
     */
	@Override
	public long getHostActiveCount() {
		return cluster.get().getMetrics().getConnectedToHosts().getValue();
	}

    /**
     * Returns the total number of currently opened connections to Cassandra hosts.
     *
     * @return The total number of currently opened connections to Cassandra hosts.
     */
	public long getNumOpenConnections() {
		return cluster.get().getMetrics().getOpenConnections().getValue();
	}

    /**
     * Returns the number of connection to Cassandra nodes errors.
     * <p>
     * This represents the number of times that a request to a Cassandra node
     * has failed due to a connection problem. This thus also corresponds to
     * how often the driver had to pick a fallback host for a request.
     * <p>
     * You can expect a few connection errors when a Cassandra node fails
     * (or is stopped) ,but if that number grows continuously you likely have
     * a problem.
     *
     * @return the number of connection to Cassandra nodes errors.
     */
	@Override
	public long getConnectionCreateFailedCount() {
		return cluster.get().getMetrics().getErrorMetrics().getConnectionErrors().getCount();
	}

    /**
     * Returns the number of write requests that returned a timeout (independently
     * of the final decision taken by the {@link com.datastax.driver.core.policies.RetryPolicy}).
     *
     * @return the number of write timeout.
     */
    public long getWriteTimeouts() {
        return cluster.get().getMetrics().getErrorMetrics().getWriteTimeouts().getCount();
    }

    /**
     * Returns the number of read requests that returned a timeout (independently
     * of the final decision taken by the {@link com.datastax.driver.core.policies.RetryPolicy}).
     *
     * @return the number of read timeout.
     */
    public long getReadTimeouts() {
        return cluster.get().getMetrics().getErrorMetrics().getReadTimeouts().getCount();
    }

    /**
     * Returns the number of requests that returned errors not accounted for by
     * another metric. This includes all types of invalid requests.
     *
     * @return the number of requests errors not accounted by another
     * metric.
     */
	@Override
	public long getBadRequestCount() {
		return cluster.get().getMetrics().getErrorMetrics().getOthers().getCount();
	}

    /**
     * Returns the number of requests that returned an unavailable exception
     * (independently of the final decision taken by the 
     * {@link com.datastax.driver.core.policies.RetryPolicy}).
     *
     * @return the number of unavailable exceptions.
     */
	@Override
	public long notFoundCount() {
		return cluster.get().getMetrics().getErrorMetrics().getUnavailables().getCount();
	}

    /**
     * Returns the number of times a request was ignored
     * due to the {@link com.datastax.driver.core.policies.RetryPolicy}, for
     * example due to timeouts or unavailability.
     *
     * @return the number of times a request was ignored due to the
     * {@link com.datastax.driver.core.policies.RetryPolicy}.
     */
	@Override
	public long getSocketTimeoutCount() {
		return cluster.get().getMetrics().getErrorMetrics().getIgnores().getCount();
	}

    /**
     * Returns the number of times a request was retried due to the
     * {@link com.datastax.driver.core.policies.RetryPolicy}.
     *
     * @return the number of times a requests was retried due to the 
     * {@link com.datastax.driver.core.policies.RetryPolicy}.
     */
	@Override
	public long getUnknownErrorCount() {
		return cluster.get().getMetrics().getErrorMetrics().getRetries().getCount();
	}

	@Override
	public void incOperationFailure(Host host, Exception reason) {
		// TODO Auto-generated method stub

	}

	@Override
	public long getOperationFailureCount() {
		// TODO Auto-generated method stub
		return 0;
	}

	@Override
	public void incFailover(Host host, Exception reason) {
		// TODO Auto-generated method stub

	}

	@Override
	public long getFailoverCount() {
		// TODO Auto-generated method stub
		return 0;
	}

	@Override
	public void incOperationSuccess(Host host, long latency) {
		// TODO Auto-generated method stub

	}

	@Override
	public long getOperationSuccessCount() {
		// TODO Auto-generated method stub
		return 0;
	}

	@Override
	public void incConnectionCreated(Host host) {
		// TODO Auto-generated method stub

	}

	@Override
	public long getConnectionCreatedCount() {
		// TODO Auto-generated method stub
		return 0;
	}

	@Override
	public void incConnectionClosed(Host host, Exception reason) {
		// TODO Auto-generated method stub

	}

	@Override
	public long getConnectionClosedCount() {
		// TODO Auto-generated method stub
		return 0;
	}

	@Override
	public void incConnectionCreateFailed(Host host, Exception reason) {
		// TODO Auto-generated method stub

	}

	@Override
	public void incConnectionBorrowed(Host host, long delay) {
		// TODO Auto-generated method stub

	}

	@Override
	public long getConnectionBorrowedCount() {
		// TODO Auto-generated method stub
		return 0;
	}

	@Override
	public long getConnectionReturnedCount() {
		return 0;
	}
	
	@Override
	public void incConnectionReturned(Host host) {
		// TODO Auto-generated method stub

	}

	@Override
	public long getPoolExhaustedTimeoutCount() {
		// TODO Auto-generated method stub
		return 0;
	}

	@Override
	public long getOperationTimeoutCount() {
		// TODO Auto-generated method stub
		return 0;
	}

	@Override
	public long getNoHostCount() {
		// TODO Auto-generated method stub
		return 0;
	}

	@Override
	public long getInterruptedCount() {
		// TODO Auto-generated method stub
		return 0;
	}

	@Override
	public long getTransportErrorCount() {
		// TODO Auto-generated method stub
		return 0;
	}

	@Override
	public long getHostAddedCount() {
		// TODO Auto-generated method stub
		return 0;
	}

	@Override
	public long getHostRemovedCount() {
		// TODO Auto-generated method stub
		return 0;
	}

	@Override
	public long getHostDownCount() {
		// TODO Auto-generated method stub
		return 0;
	}

	@Override
	public void onHostAdded(Host host, HostConnectionPool<?> pool) {
		// TODO Auto-generated method stub

	}

	@Override
	public void onHostRemoved(Host host) {
		// TODO Auto-generated method stub

	}

	@Override
	public void onHostDown(Host host, Exception reason) {
		// TODO Auto-generated method stub

	}

	@Override
	public void onHostReactivated(Host host, HostConnectionPool<?> pool) {
		// TODO Auto-generated method stub

	}

	@Override
	public Map<Host, HostStats> getHostStats() {
		// TODO Auto-generated method stub
		return null;
	}

}>>>>>>> YOURS
/home/ramdisk/experiment3/projects/astyanax/revisions/rev_2b15fd9_3de01e3/rev_2b15fd9-3de01e3/astyanax-cql/src/main/java/com/netflix/astyanax/cql/CqlRingDescriber.java;<<<<<<< MINE
=======
package com.netflix.astyanax.cql;

import java.math.BigInteger;
import java.util.ArrayList;
import java.util.Collections;
import java.util.List;
import java.util.Set;
import java.util.UUID;
import java.util.concurrent.atomic.AtomicReference;

import com.datastax.driver.core.ResultSet;
import com.datastax.driver.core.Row;
import com.datastax.driver.core.Session;
import com.datastax.driver.core.Statement;
import com.datastax.driver.core.querybuilder.QueryBuilder;
import com.netflix.astyanax.connectionpool.TokenRange;
import com.netflix.astyanax.connectionpool.impl.TokenRangeImpl;
import com.netflix.astyanax.partitioner.Murmur3Partitioner;

/**
 * Helper class that parses the ring information from the system and peers table. 
 * Note that it maintains a cached reference and allows the user to either reuse the cache or refresh the cahe.
 *  
 * @author poberai
 */
public class CqlRingDescriber {

	private final AtomicReference<List<TokenRange>>  cachedReference = new AtomicReference<List<TokenRange>>(null);
	
	private static CqlRingDescriber Instance = new CqlRingDescriber();
	
	private CqlRingDescriber() {
	}
	
	public static CqlRingDescriber getInstance() {
		return Instance; 
	}
	
	public List<TokenRange> getTokenRanges(Session session, boolean cached) {
		
		if (cached && cachedReference.get() != null) {
			return cachedReference.get();
		}
		
		// else get the actual token range list and then cache it
		List<TokenRange> ranges = getTokenRanges(session, null, null);
		cachedReference.set(ranges);
		return ranges;
	}
	
	public List<TokenRange> getTokenRanges(Session session, String dc, String rack) {
		
		List<HostInfo> hosts = new ArrayList<HostInfo>();

		Statement query = QueryBuilder.select().all().from("system", "local"); 
		ResultSet resultSet = session.execute(query);
		hosts.add(new HostInfo(resultSet.one(), resultSet));
		
		query = QueryBuilder.select("peer", "data_center", "host_id", "rack", "tokens").from("system", "peers"); 
		resultSet = session.execute(query);
		for (Row row : resultSet.all()) {
			hosts.add(new HostInfo(row, null));
		}
		
		Collections.sort(hosts);

		List<TokenRange> ranges = new ArrayList<TokenRange>();

		for (int index = 0; index<hosts.size(); index++) {

			HostInfo thisNode = hosts.get(index);

			List<String> endpoints = new ArrayList<String>();

			if (matchNode(dc, rack, thisNode)) {
				endpoints.add(thisNode.endpoint); // the primary range owner
			}

			// secondary node
			int nextIndex = getNextIndex(index, hosts.size());
			if (nextIndex != index) {

				HostInfo nextNode = hosts.get(nextIndex);
				if (matchNode(dc, rack, nextNode)) {
					endpoints.add(nextNode.endpoint);
				}

				// tertiary node
				nextIndex = getNextIndex(nextIndex, hosts.size());
				nextNode = hosts.get(nextIndex);
				if (matchNode(dc, rack, nextNode)) {
					endpoints.add(nextNode.endpoint);
				}
			}
			int prevIndex = getPrevIndex(index, hosts.size());
			String startToken = hosts.get(prevIndex).token.toString();
			String endToken = thisNode.token.toString();

			if (startToken.equals(endToken)) {
				// TOKENS are the same. This happens during testing. 
				startToken = Murmur3Partitioner.get().getMinToken();
				endToken = Murmur3Partitioner.get().getMinToken();
				
			}
			ranges.add(new TokenRangeImpl(startToken, endToken, endpoints));
		}

		return ranges;
	}
	
	private boolean matchNode(String dc, String rack, HostInfo host) {
		
		if (dc == null && rack == null) {
			return true; // node matches since there is no filter
		}
		
		if (dc != null && !dc.equals(host.datacenter)) {
			return false; // wrong dc 
		}
		
		if (rack != null && !rack.equals(host.rack)) {
			return false; // wrong rack
		}
		
		return true; // match!
	}
	
	private int getNextIndex(int i, int n) {
		
		int next = ++i;
		if (i >= n) {
			return 0; 
		} else {
			return next;
		}
	}
	
	private int getPrevIndex(int i, int n) {
		
		int prev = --i;
		if (i < 0) {
			return n-1; 
		} else {
			return prev;
		}
	}

	private class HostInfo implements Comparable<HostInfo> {
		
		private final BigInteger token;
		private final String endpoint; 
		private final UUID hostId; 
		private final String datacenter;
		private final String rack; 
		
		private HostInfo(Row row, ResultSet rs) {
			
			if (row == null) {
				throw new RuntimeException("RS is empty for system.local query");
			}
		
			Set<String> tokens = row.getSet("tokens", String.class);
			
			String theToken = tokens.iterator().next();
			token = new BigInteger(theToken);
			
			hostId = row.getUUID("host_id");
			datacenter = row.getString("data_center");
			rack = row.getString("rack");

			if (rs != null) {
				endpoint = rs.getExecutionInfo().getQueriedHost().getAddress().getHostAddress();
			} else {
				endpoint = row.getInet("peer").getHostAddress();
			}
		}

		@Override
		public int hashCode() {
			final int prime = 31;
			int result = 1;
			result = prime * result + ((token == null) ? 0 : token.hashCode());
			result = prime * result + ((endpoint == null) ? 0 : endpoint.hashCode());
			result = prime * result + ((hostId == null) ? 0 : hostId.hashCode());
			result = prime * result + ((datacenter == null) ? 0 : datacenter.hashCode());
			result = prime * result + ((rack == null) ? 0 : rack.hashCode());
			return result;
		}

		@Override
		public boolean equals(Object obj) {
			
			if (this == obj) return true;
			if (obj == null) return false;
			if (getClass() != obj.getClass()) return false;
			
			HostInfo other = (HostInfo) obj;
			boolean equal = true; 
			
			equal &= (token != null) ? token.equals(other.token) : (other.token == null);
			equal &= (endpoint != null) ? endpoint.equals(other.endpoint) : (other.endpoint == null);
			equal &= (hostId != null) ? hostId.equals(other.hostId) : (other.hostId == null);
			equal &= (datacenter != null) ? datacenter.equals(other.datacenter) : (other.datacenter == null);
			equal &= (rack != null) ? rack.equals(other.rack) : (other.rack == null);
			
			return equal;
		}

		@Override
		public String toString() {
			return "HostInfo [token=" + token + ", endpoint=" + endpoint
					+ ", hostId=" + hostId.toString() + ", datacenter=" + datacenter
					+ ", rack=" + rack + "]";
		}

		@Override
		public int compareTo(HostInfo o) {
			return this.token.compareTo(o.token);
		}
	}
}>>>>>>> YOURS
/home/ramdisk/experiment3/projects/astyanax/revisions/rev_2b15fd9_3de01e3/rev_2b15fd9-3de01e3/astyanax-cql/src/main/java/com/netflix/astyanax/cql/CqlFamilyFactory.java;<<<<<<< MINE
=======
package com.netflix.astyanax.cql;

import java.util.concurrent.atomic.AtomicBoolean;

import sun.reflect.generics.reflectiveObjects.NotImplementedException;

import com.datastax.driver.core.Cluster;
import com.datastax.driver.core.Configuration;
import com.datastax.driver.core.MetricsOptions;
import com.datastax.driver.core.PoolingOptions;
import com.datastax.driver.core.ProtocolOptions;
import com.datastax.driver.core.QueryOptions;
import com.datastax.driver.core.SocketOptions;
import com.datastax.driver.core.policies.LoadBalancingPolicy;
import com.datastax.driver.core.policies.Policies;
import com.datastax.driver.core.policies.RoundRobinPolicy;
import com.datastax.driver.core.policies.TokenAwarePolicy;
import com.netflix.astyanax.AstyanaxConfiguration;
import com.netflix.astyanax.AstyanaxContext;
import com.netflix.astyanax.AstyanaxTypeFactory;
import com.netflix.astyanax.Keyspace;
import com.netflix.astyanax.KeyspaceTracerFactory;
import com.netflix.astyanax.connectionpool.Connection;
import com.netflix.astyanax.connectionpool.ConnectionFactory;
import com.netflix.astyanax.connectionpool.ConnectionPool;
import com.netflix.astyanax.connectionpool.ConnectionPoolConfiguration;
import com.netflix.astyanax.connectionpool.ConnectionPoolMonitor;
import com.netflix.astyanax.connectionpool.ConnectionPoolProxy;
import com.netflix.astyanax.connectionpool.HostConnectionPool;
import com.netflix.astyanax.connectionpool.exceptions.ThrottledException;
import com.netflix.astyanax.connectionpool.impl.ConnectionPoolType;
import com.netflix.astyanax.connectionpool.impl.CountingConnectionPoolMonitor;

/**
 * Simple impl of {@link AstyanaxTypeFactory} that acts as the bridge between the AstyanaxContext setup and all the java driver setup.
 * The main link is the {@link ConnectionPoolProxy} class which gives us access to the {@link ConnectionPoolConfiguration} object. 
 * The class expects a {@link JavaDriverConnectionPoolConfigurationImpl} based impl which encapsulates all the config that is required
 * by java driver. 
 * 
 * Thus this bridge is built with the intention to let the outside caller to directly use the {@link Configuration} object and inject it 
 * using {@link AstyanaxContext}. 
 * 
 * Restating, the simple flow that enables the bridge is
 * 1. Construct the {@link Configuration} object with all the desired options for configuring the java driver.
 * 2. Construct the {@link JavaDriverConnectionPoolConfigurationImpl} object and pass the java driver configuration object to it. 
 * 3. Set the {@link ConnectionPoolConfiguration} created in step 2. on the {@link AstyanaxContext} builder object when creating the Astyanax {@link Keyspace}  
 * 
 * See {@link AstyanaxContext} for more details on how to do this. 
 * 
 * @author poberai
 *
 */
public class CqlFamilyFactory implements AstyanaxTypeFactory<Cluster> {

	private static CqlFamilyFactory Instance = new CqlFamilyFactory(); 
	
	private static AtomicBoolean BatchColumnUpdates = new AtomicBoolean(false);
	
	public static CqlFamilyFactory getInstance() {
		return Instance;
	}
	
	@Override
	public Keyspace createKeyspace(String ksName, ConnectionPool<Cluster> cp, AstyanaxConfiguration asConfig, KeyspaceTracerFactory tracerFactory) {
		
		if (!(cp instanceof ConnectionPoolProxy)) {
			throw new RuntimeException("Cannot use CqlFamilyFactory with a connection pool type other than ConnectionPoolType.JAVA_DRIVER");
		}

		ConnectionPoolProxy<?> cpProxy = (ConnectionPoolProxy<?>)cp; 
		
		ConnectionPoolConfiguration jdConfig = getOrCreateJDConfiguration(asConfig, cpProxy.getConnectionPoolConfiguration());
		ConnectionPoolMonitor monitor = cpProxy.getConnectionPoolMonitor();
		if (monitor != null && (monitor instanceof CountingConnectionPoolMonitor))
			monitor = new JavaDriverConnectionPoolMonitorImpl();
		CqlKeyspaceImpl keyspace = new CqlKeyspaceImpl(ksName, asConfig, tracerFactory, jdConfig, monitor);
		cpProxy.addListener(keyspace);
		
		return keyspace;
	}

	@Override
	public com.netflix.astyanax.Cluster createCluster(ConnectionPool<Cluster> cp, AstyanaxConfiguration asConfig, KeyspaceTracerFactory tracerFactory) {
		
		if (!(cp instanceof ConnectionPoolProxy)) {
			throw new RuntimeException("Cannot use CqlFamilyFactory with a connection pool type other than ConnectionPoolType.JAVA_DRIVER");
		}
		
		ConnectionPoolProxy<?> cpProxy = (ConnectionPoolProxy<?>)cp; 
		ConnectionPoolConfiguration jdConfig = getOrCreateJDConfiguration(asConfig, cpProxy.getConnectionPoolConfiguration());
		ConnectionPoolMonitor monitor = cpProxy.getConnectionPoolMonitor();
		if (monitor != null && (monitor instanceof CountingConnectionPoolMonitor))
			monitor = new JavaDriverConnectionPoolMonitorImpl();
		CqlClusterImpl cluster = new CqlClusterImpl(asConfig, tracerFactory, jdConfig, monitor);
		((ConnectionPoolProxy<Cluster>)cp).addListener(cluster);
		
		return cluster;
	}

	@Override
	public ConnectionFactory<Cluster> createConnectionFactory(
			AstyanaxConfiguration asConfig,
			ConnectionPoolConfiguration cfConfig,
			KeyspaceTracerFactory tracerFactory, 
			ConnectionPoolMonitor monitor) {
		
		CqlBasedConnectionFactory<Cluster> factory = new CqlBasedConnectionFactory<Cluster>();
		factory.asConfig = asConfig;
		factory.cfConfig = cfConfig;
		factory.tracerFactory = tracerFactory;
		factory.monitor = monitor;
		
		return factory;
	}
	
	@SuppressWarnings("unused")
	private static class CqlBasedConnectionFactory<T> implements ConnectionFactory<T> {
		
		protected AstyanaxConfiguration asConfig;
		protected ConnectionPoolConfiguration cfConfig;
		protected KeyspaceTracerFactory tracerFactory;
		protected ConnectionPoolMonitor monitor;
		
		@Override
		public Connection<T> createConnection(HostConnectionPool<T> pool) throws ThrottledException {
			throw new NotImplementedException();
		}
	}	
	
	public CqlFamilyFactory enableColumnBatchUpdates(boolean condition) {
		BatchColumnUpdates.set(condition);
		return this;
	}
	
	public static boolean batchColumnUpdates() {
		return BatchColumnUpdates.get();
	}
	
	
	private ConnectionPoolConfiguration getOrCreateJDConfiguration(AstyanaxConfiguration asConfig, ConnectionPoolConfiguration cpConfig) {
		
		if (asConfig.getConnectionPoolType() == ConnectionPoolType.BAG) {
		}

		Configuration actualConfig = null; 
		
		JavaDriverConnectionPoolConfigurationImpl jdConfig = (JavaDriverConnectionPoolConfigurationImpl) cpConfig;
		if (jdConfig != null) {
			if (jdConfig.getJavaDriverConfig() != null) {
				actualConfig = jdConfig.getJavaDriverConfig();
				if (actualConfig != null) {
					return jdConfig;
				}
			}
		}
		
		LoadBalancingPolicy lbPolicy = null;
		switch (asConfig.getConnectionPoolType()) {
		case BAG:
				throw new RuntimeException("Cannot use ConnectionPoolType.BAG with java driver, " +
					"use TOKEN_AWARE or ROUND_ROBIN or configure java driver directly");
		case ROUND_ROBIN:
				lbPolicy = new RoundRobinPolicy();
				break;
		case TOKEN_AWARE:
				lbPolicy = new TokenAwarePolicy(new RoundRobinPolicy());
				break;
		};
		
		Policies policies = new Policies(lbPolicy, Policies.defaultReconnectionPolicy(), Policies.defaultRetryPolicy());
		
		actualConfig = new Configuration(
				policies,
				new ProtocolOptions(),
				new PoolingOptions(),
				new SocketOptions(),
				new MetricsOptions(),
				new QueryOptions());
		
		return new JavaDriverConnectionPoolConfigurationImpl().withJavaDriverConfig(actualConfig);
	}
}>>>>>>> YOURS
/home/ramdisk/experiment3/projects/astyanax/revisions/rev_2b15fd9_3de01e3/rev_2b15fd9-3de01e3/astyanax-cql/src/main/java/com/netflix/astyanax/cql/CqlSchemaVersionReader.java;<<<<<<< MINE
=======
package com.netflix.astyanax.cql;

import java.net.InetAddress;
import java.util.ArrayList;
import java.util.HashMap;
import java.util.List;
import java.util.Map;
import java.util.UUID;

import org.slf4j.Logger;
import org.slf4j.LoggerFactory;

import com.datastax.driver.core.ResultSet;
import com.datastax.driver.core.Row;
import com.datastax.driver.core.Session;

/**
 * Simple class that reads the schema versions from the system local and peers table. 
 * 
 * @author poberai
 */
public class CqlSchemaVersionReader {

	private static final Logger Logger = LoggerFactory.getLogger(CqlSchemaVersionReader.class);
	
	private static final String SELECT_SCHEMA_LOCAL = "SELECT schema_version FROM system.local WHERE key='local'";
	private static final String SELECT_SCHEMA_PEERS = "SELECT peer, schema_version FROM system.peers";

	private final Session session;
	
	public CqlSchemaVersionReader(Session session) { 
		this.session = session;
	}
	
	public Map<String, List<String>> exec() {
		
	    Map<String, List<String>> versions = new HashMap<String, List<String>>();

		ResultSet rs = session.execute(SELECT_SCHEMA_LOCAL);
		
        Row localRow = rs.one();
        if (localRow != null && !localRow.isNull("schema_version")) {
        	UUID localSchemaVersion = localRow.getUUID("schema_version");
        	InetAddress localServer = rs.getExecutionInfo().getQueriedHost().getAddress();
            addSchemaVersion(localSchemaVersion, localServer, versions);
        }

		rs = session.execute(SELECT_SCHEMA_PEERS);

        for (Row row : rs.all()) {

        	if (row.isNull("rpc_address") || row.isNull("schema_version"))
                continue;
            
        	UUID schema = row.getUUID("schema_version");
            InetAddress remoteEndpoint = row.getInet("rpc_address");
            addSchemaVersion(schema, remoteEndpoint, versions);
        }

        if (Logger.isDebugEnabled()) {
        	Logger.debug("Checking for schema agreement: versions are {}", versions);
        }
        
        return versions;
	}
	
	private void addSchemaVersion(UUID versionUUID, InetAddress endpoint, Map<String, List<String>> map) {
		
		String version = versionUUID.toString();
		List<String> endpoints = map.get(version);
		if (endpoints == null) {
			endpoints = new ArrayList<String>();
			map.put(version, endpoints);
		}
		endpoints.add(endpoint.toString());
	}
}>>>>>>> YOURS
/home/ramdisk/experiment3/projects/astyanax/revisions/rev_2b15fd9_3de01e3/rev_2b15fd9-3de01e3/astyanax-cql/src/main/java/com/netflix/astyanax/cql/CqlKeyspaceImpl.java;<<<<<<< MINE
=======
package com.netflix.astyanax.cql;

import static com.datastax.driver.core.querybuilder.QueryBuilder.eq;

import java.util.Collection;
import java.util.List;
import java.util.Map;
import java.util.Properties;

import org.slf4j.Logger;
import org.slf4j.LoggerFactory;

import com.codahale.metrics.MetricRegistryListener;
import com.datastax.driver.core.Cluster;
import com.datastax.driver.core.Configuration;
import com.datastax.driver.core.ResultSet;
import com.datastax.driver.core.Row;
import com.datastax.driver.core.Session;
import com.datastax.driver.core.Statement;
import com.datastax.driver.core.querybuilder.QueryBuilder;
import com.google.common.base.Function;
import com.google.common.collect.Lists;
import com.netflix.astyanax.AstyanaxConfiguration;
import com.netflix.astyanax.Clock;
import com.netflix.astyanax.ColumnMutation;
import com.netflix.astyanax.Keyspace;
import com.netflix.astyanax.KeyspaceTracerFactory;
import com.netflix.astyanax.MutationBatch;
import com.netflix.astyanax.SerializerPackage;
import com.netflix.astyanax.clock.MicrosecondsAsyncClock;
import com.netflix.astyanax.connectionpool.ConnectionPool;
import com.netflix.astyanax.connectionpool.ConnectionPoolConfiguration;
import com.netflix.astyanax.connectionpool.ConnectionPoolMonitor;
import com.netflix.astyanax.connectionpool.ConnectionPoolProxy.SeedHostListener;
import com.netflix.astyanax.connectionpool.Host;
import com.netflix.astyanax.connectionpool.Operation;
import com.netflix.astyanax.connectionpool.OperationResult;
import com.netflix.astyanax.connectionpool.TokenRange;
import com.netflix.astyanax.connectionpool.exceptions.ConnectionException;
import com.netflix.astyanax.connectionpool.exceptions.NotFoundException;
import com.netflix.astyanax.connectionpool.exceptions.OperationException;
import com.netflix.astyanax.cql.direct.DirectCqlStatement;
import com.netflix.astyanax.cql.reads.CqlColumnFamilyQueryImpl;
import com.netflix.astyanax.cql.schema.CqlColumnFamilyDefinitionImpl;
import com.netflix.astyanax.cql.schema.CqlKeyspaceDefinitionImpl;
import com.netflix.astyanax.cql.util.CFQueryContext;
import com.netflix.astyanax.cql.writes.CqlColumnMutationImpl;
import com.netflix.astyanax.cql.writes.CqlMutationBatchImpl;
import com.netflix.astyanax.ddl.ColumnFamilyDefinition;
import com.netflix.astyanax.ddl.KeyspaceDefinition;
import com.netflix.astyanax.ddl.SchemaChangeResult;
import com.netflix.astyanax.model.ColumnFamily;
import com.netflix.astyanax.partitioner.BigInteger127Partitioner;
import com.netflix.astyanax.partitioner.Murmur3Partitioner;
import com.netflix.astyanax.partitioner.Partitioner;
import com.netflix.astyanax.query.ColumnFamilyQuery;
import com.netflix.astyanax.retry.RetryPolicy;
import com.netflix.astyanax.serializers.SerializerPackageImpl;
import com.netflix.astyanax.serializers.UnknownComparatorException;
import com.yammer.metrics.core.MetricsRegistryListener;

/**
 * Java Driver based impl of {@link Keyspace} that implements ddl operations as well as row queries and mutation batches.
 * The class encapsulates a java driver cluster and session object to provide all the functionality. 
 *  
 * Note that due to the way the object is setup via AstyanaxContext and CqlFamilyFactory, it needs to implements 
 * a {@link SeedHostListener} so that it can construct the cluster and session object appropriately once the seed hosts
 * have been provided by the {@link HostSupplier} object.
 *  
 * @author poberai
 */
public class CqlKeyspaceImpl implements Keyspace, SeedHostListener {

	private static final Logger Logger = LoggerFactory.getLogger(CqlKeyspaceImpl.class);
	
	private final Clock clock;
	
	public volatile Cluster cluster;
	public volatile Session session;
	
	private final KeyspaceContext ksContext;
	private final String keyspaceName;
	private final AstyanaxConfiguration astyanaxConfig;
	private final KeyspaceTracerFactory tracerFactory; 
	private final Configuration javaDriverConfig;
	private final ConnectionPoolMonitor cpMonitor;
	private final MetricsRegistryListener metricsRegListener;

	public CqlKeyspaceImpl(String ksName, AstyanaxConfiguration asConfig, KeyspaceTracerFactory tracerFactory, ConnectionPoolConfiguration cpConfig, ConnectionPoolMonitor cpMonitor) {
		this(null, ksName, asConfig, tracerFactory, cpConfig,cpMonitor);
	}

	public CqlKeyspaceImpl(KeyspaceContext ksContext) {
		this(ksContext.getSession(), ksContext.getKeyspace(), ksContext.getConfig(), ksContext.getTracerFactory(), null, ksContext.getConnectionPoolMonitor());
	}

	CqlKeyspaceImpl(Session session, String ksName, AstyanaxConfiguration asConfig, KeyspaceTracerFactory tracerFactory, ConnectionPoolMonitor cpMonitor) {
		this(session, ksName, asConfig, tracerFactory, null,cpMonitor);
	}

	private CqlKeyspaceImpl(Session session, String ksName, AstyanaxConfiguration asConfig, KeyspaceTracerFactory tracerFactory, ConnectionPoolConfiguration cpConfig, ConnectionPoolMonitor cpMonitor) {
		this.session = session;
		this.keyspaceName = ksName.toLowerCase();
		this.astyanaxConfig = asConfig;
		this.tracerFactory = tracerFactory;
		this.cpMonitor = cpMonitor;
		this.metricsRegListener = ((JavaDriverConnectionPoolMonitorImpl)cpMonitor).getMetricsRegistryListener();
		this.ksContext = new KeyspaceContext(this);
		
		if (asConfig.getClock() != null) {
			clock = asConfig.getClock();
		} else {
			clock = new MicrosecondsAsyncClock();
		}
		
		if (cpConfig != null) {
			javaDriverConfig = ((JavaDriverConnectionPoolConfigurationImpl)cpConfig).getJavaDriverConfig();
		} else {
			javaDriverConfig = null;
		}
	}
	
	
	@Override
	public AstyanaxConfiguration getConfig() {
		return astyanaxConfig;
	}

	@Override
	public String getKeyspaceName() {
		return keyspaceName;
	}

	@Override
	public Partitioner getPartitioner() throws ConnectionException {
		String pName = describePartitioner();
		if (pName.contains("Murmur3Partitioner")) {
			return Murmur3Partitioner.get();
		} else if (pName.contains("RandomPartitioner")) {
			return BigInteger127Partitioner.get();
		} else {
			throw new RuntimeException("Unrecognized partitioner: " + pName);
		}
	}

	@Override
	public String describePartitioner() throws ConnectionException {
		Statement q = QueryBuilder.select("partitioner").from("system", "local");
		ResultSet result = session.execute(q);
		com.datastax.driver.core.Row row = result.one();
		if (row == null) {
			throw new RuntimeException("Missing paritioner");
		}
		String pName = row.getString(0);
		return pName;
	}

	@Override
	public List<TokenRange> describeRing() throws ConnectionException {
		return CqlRingDescriber.getInstance().getTokenRanges(session, false);
	}

	@Override
	public List<TokenRange> describeRing(String dc) throws ConnectionException {
		return CqlRingDescriber.getInstance().getTokenRanges(session, dc, null);
	}

	@Override
	public List<TokenRange> describeRing(String dc, String rack) throws ConnectionException {
		return CqlRingDescriber.getInstance().getTokenRanges(session, dc, rack);
	}

	@Override
	public List<TokenRange> describeRing(boolean cached) throws ConnectionException {
		return CqlRingDescriber.getInstance().getTokenRanges(session, cached);
	}

	@Override
	public KeyspaceDefinition describeKeyspace() throws ConnectionException {
		
		Statement query = QueryBuilder.select().from("system", "schema_keyspaces").where(eq("keyspace_name", keyspaceName));
		Row row = session.execute(query).one();
		if (row == null) {
			throw new RuntimeException("Keyspace not found: " + keyspaceName);
		}
		return (new CqlKeyspaceDefinitionImpl(session, row));
	}

	@Override
	public Properties getKeyspaceProperties() throws ConnectionException {
		try {
			return describeKeyspace().getProperties();
		} catch (Exception e) {
			throw new RuntimeException(e);
		}
	}

	@Override
	public Properties getColumnFamilyProperties(String columnFamily) throws ConnectionException {
        KeyspaceDefinition ksDef = this.describeKeyspace();
        ColumnFamilyDefinition cfDef = ksDef.getColumnFamily(columnFamily);
        if (cfDef == null)
            throw new NotFoundException(String.format("Column family '%s' in keyspace '%s' not found", columnFamily, getKeyspaceName()));
        try {
			return cfDef.getProperties();
		} catch (Exception e) {
			throw new RuntimeException();
		}
	}

	@Override
	public SerializerPackage getSerializerPackage(String cfName, boolean ignoreErrors) throws ConnectionException, UnknownComparatorException {
		
		ColumnFamilyDefinition cfDef = describeKeyspace().getColumnFamily(cfName);
		return new SerializerPackageImpl(cfDef, ignoreErrors);
	}

	@Override
	public MutationBatch prepareMutationBatch() {
		return new CqlMutationBatchImpl(ksContext, clock, astyanaxConfig.getDefaultWriteConsistencyLevel(), astyanaxConfig.getRetryPolicy());
	}

	@Override
	public <K, C> ColumnMutation prepareColumnMutation(ColumnFamily<K, C> columnFamily, K rowKey, C column) {
		return new CqlColumnMutationImpl<K,C>(ksContext, new CFQueryContext<K, C>(columnFamily, rowKey), column);
	}

	@Override
	public <K, C> ColumnFamilyQuery<K, C> prepareQuery(ColumnFamily<K, C> cf) {
		return new CqlColumnFamilyQueryImpl<K,C>(ksContext, cf);
	}

	@Override
	public OperationResult<SchemaChangeResult> createKeyspace(Map<String, Object> options) throws ConnectionException {
		return new CqlKeyspaceDefinitionImpl(session, options).setName(keyspaceName).execute();
	}

	@Override
	public OperationResult<SchemaChangeResult> createKeyspace(Properties properties) throws ConnectionException {
		return new CqlKeyspaceDefinitionImpl(session, properties).setName(keyspaceName).execute();
	}

	@SuppressWarnings("rawtypes")
	@Override
	public OperationResult<SchemaChangeResult> createKeyspace(Map<String, Object> options, Map<ColumnFamily, Map<String, Object>> cfs) throws ConnectionException {
		
		CqlKeyspaceDefinitionImpl ksDef = new CqlKeyspaceDefinitionImpl(session, options);
		if (ksDef.getName() == null) {
			ksDef.setName(keyspaceName);
		}
		
		OperationResult<SchemaChangeResult> result = ksDef.execute();
		
		for (ColumnFamily cf : cfs.keySet()) {
			CqlColumnFamilyDefinitionImpl cfDef = new CqlColumnFamilyDefinitionImpl(session, ksDef.getName(), cf, cfs.get(cf));
			ksDef.addColumnFamily(cfDef);
		}
		
		return result;
	}

	@Override
	public OperationResult<SchemaChangeResult> updateKeyspace(Map<String, Object> options) throws ConnectionException {
		return new CqlKeyspaceDefinitionImpl(session, options).setName(keyspaceName).alterKeyspace().execute();
	}

	@Override
	public OperationResult<SchemaChangeResult> updateKeyspace(Properties props) throws ConnectionException {
		return new CqlKeyspaceDefinitionImpl(session, props).setName(keyspaceName).alterKeyspace().execute();
	}

	@Override
	public OperationResult<SchemaChangeResult> dropKeyspace() throws ConnectionException {
		return new CqlOperationResultImpl<SchemaChangeResult>(session.execute("DROP KEYSPACE " + keyspaceName), null);
	}
	
	@Override
	public <K, C> OperationResult<Void> truncateColumnFamily(ColumnFamily<K, C> columnFamily) throws OperationException, ConnectionException {
		ResultSet result = session.execute("TRUNCATE " + keyspaceName + "." + columnFamily.getName());
		return new CqlOperationResultImpl<Void>(result, null);
	}

	@Override
	public OperationResult<Void> truncateColumnFamily(String columnFamily) throws ConnectionException {
		ResultSet result = session.execute("TRUNCATE " + keyspaceName + "." + columnFamily);
		return new CqlOperationResultImpl<Void>(result, null);
	}

	@Override
	public <K, C> OperationResult<SchemaChangeResult> createColumnFamily(ColumnFamily<K, C> columnFamily, Map<String, Object> options) throws ConnectionException {
		return new CqlColumnFamilyDefinitionImpl(session, keyspaceName, columnFamily, options).execute();
	}

	@Override
	public OperationResult<SchemaChangeResult> createColumnFamily(Properties props) throws ConnectionException {
		return new CqlColumnFamilyDefinitionImpl(session, keyspaceName, props).execute();
	}

	@Override
	public OperationResult<SchemaChangeResult> createColumnFamily(Map<String, Object> options) throws ConnectionException {
		return new CqlColumnFamilyDefinitionImpl(session, keyspaceName, options).execute();
	}

	@Override
	public <K, C> OperationResult<SchemaChangeResult> updateColumnFamily(ColumnFamily<K, C> columnFamily, Map<String, Object> options) throws ConnectionException {
		return new CqlColumnFamilyDefinitionImpl(session, keyspaceName, columnFamily, options).alterTable().execute();
	}

	@Override
	public OperationResult<SchemaChangeResult> updateColumnFamily(Properties props) throws ConnectionException {
		return new CqlColumnFamilyDefinitionImpl(session, keyspaceName, props).alterTable().execute();
	}

	@Override
	public OperationResult<SchemaChangeResult> updateColumnFamily(Map<String, Object> options) throws ConnectionException {
		return new CqlColumnFamilyDefinitionImpl(session, keyspaceName, options).alterTable().execute();
	}

	@Override
	public OperationResult<SchemaChangeResult> dropColumnFamily(String columnFamilyName) throws ConnectionException {
		return new CqlOperationResultImpl<SchemaChangeResult>(session.execute("DROP TABLE " + keyspaceName + "." + columnFamilyName), null);
	}

	@Override
	public <K, C> OperationResult<SchemaChangeResult> dropColumnFamily(ColumnFamily<K, C> columnFamily) throws ConnectionException {
		return dropColumnFamily(columnFamily.getName());
	}

	@Override
	public Map<String, List<String>> describeSchemaVersions() throws ConnectionException {
		return new CqlSchemaVersionReader(session).exec();
	}

	@Override
	public CqlStatement prepareCqlStatement() {
		return new DirectCqlStatement(session);
	}

	@Override
	public ConnectionPool<?> getConnectionPool() throws ConnectionException {
		throw new UnsupportedOperationException("Operation not supported");
	}


	@Override
	public OperationResult<Void> testOperation(Operation<?, ?> operation) throws ConnectionException {
		throw new UnsupportedOperationException("Operation not supported");
	}

	@Override
	public OperationResult<Void> testOperation(Operation<?, ?> operation, RetryPolicy retry) throws ConnectionException {
		throw new UnsupportedOperationException("Operation not supported");
	}

	@Override
	public void setHosts(Collection<Host> hosts, int port) {

		try {
			if (session != null) {
				Logger.info("Session has already been set, SKIPPING SET HOSTS");
				return;
			}
			List<Host> hostList = Lists.newArrayList(hosts);

			List<String> contactPoints = Lists.transform(hostList, new Function<Host, String>() {
				@Override
				public String apply(Host input) {
					if (input != null) {
						return input.getHostName(); 
					}
					return null;
				}
			});

			Configuration config = javaDriverConfig;
			
			// We really need a mechanism to easily override Configuration on the builder
			Logger.info("Using port: " + port);
			
			Cluster.Builder builder = Cluster.builder()
					.addContactPoints(contactPoints.toArray(new String[0]))
					.withPort(port)
					.withLoadBalancingPolicy(config.getPolicies().getLoadBalancingPolicy())
					.withReconnectionPolicy(config.getPolicies().getReconnectionPolicy())
					.withRetryPolicy(config.getPolicies().getRetryPolicy())
					.withCompression(config.getProtocolOptions().getCompression())
					.withPoolingOptions(config.getPoolingOptions())
					.withSocketOptions(config.getSocketOptions())
					.withQueryOptions(config.getQueryOptions());
			
			if (config.getMetricsOptions() == null) {
				builder.withoutMetrics();
			} else if (!config.getMetricsOptions().isJMXReportingEnabled()) {
				builder.withoutJMXReporting();
			}
					
			cluster = builder.build();
			if (!(this.cpMonitor instanceof JavaDriverConnectionPoolMonitorImpl))
				this.cluster.getMetrics().getRegistry().addListener((MetricRegistryListener) this.metricsRegListener);
			
			Logger.info("Connecting to cluster");
			session = cluster.connect();
			Logger.info("Done connecting to cluster, session object created");

		} catch (RuntimeException e) {
			Logger.error("Failed to set hosts for keyspace impl", e);
			
		} catch (Exception e) {
			Logger.error("Failed to set hosts for keyspace impl", e);
		}
	}
	
	@Override
	public void shutdown() {
		cluster.shutdown();
	}


	public class KeyspaceContext {
		
		private final Keyspace ks; 
		
		public KeyspaceContext(Keyspace keyspaceCtx) {
			this.ks = keyspaceCtx;
		}
		public Session getSession() {
			return session;
		}
		public String getKeyspace() {
			return keyspaceName;
		}
		public AstyanaxConfiguration getConfig() {
			return astyanaxConfig;
		}
		public KeyspaceTracerFactory getTracerFactory() {
			return tracerFactory;
		}
		
		public Keyspace getKeyspaceContext() {
			return ks;
		}
		public ConnectionPoolMonitor getConnectionPoolMonitor(){
			return cpMonitor;
		}
	}
}>>>>>>> YOURS
/home/ramdisk/experiment3/projects/astyanax/revisions/rev_2b15fd9_3de01e3/rev_2b15fd9-3de01e3/astyanax-test/src/main/java/com/netflix/astyanax/cql/test/utils/TestUtils.java;<<<<<<< MINE
=======
package com.netflix.astyanax.cql.test.utils;

import java.util.ArrayList;
import java.util.Arrays;
import java.util.List;

import com.netflix.astyanax.ColumnListMutation;
import com.netflix.astyanax.Keyspace;
import com.netflix.astyanax.MutationBatch;
import com.netflix.astyanax.model.ColumnFamily;
import com.netflix.astyanax.serializers.IntegerSerializer;
import com.netflix.astyanax.serializers.StringSerializer;

public class TestUtils {

	public static class TestTokenRange {
		
		public String startToken;
		public String endToken;
		public List<String> expectedRowKeys = new ArrayList<String>();
		
		public TestTokenRange(String start, String end, String ... expectedKeys) {
			startToken = start;
			endToken = end;
			expectedRowKeys.addAll(Arrays.asList(expectedKeys));
		}
	}
	
	public static List<TestTokenRange> getTestTokenRanges() {
		
		/**
		 * HERE IS THE ACTUAL ORDER OF KEYS SORTED BY THEIR TOKENS	
		 * 
		 * -1671667184962092389   = Q,   -1884162317724288694   = O,   -2875471597373478633   = K, 
		 * -300136452241384611    = L,   -4197513287269367591   = H,   -422884050476930919    = Y, 
		 * -4837624800923759386   = B,   -4942250469937744623   = W,   -7139673851965614954   = J, 
		 * -7311855499978618814   = X,   -7912594386904524724   = M,   -8357705097978550118   = T, 
		 * -8692134701444027338   = C,    243126998722523514    = A,    3625209262381227179   = F, 
		 *  3846318681772828433   = R,    3914548583414697851   = N,    4834152074310082538   = I, 
		 *  4943864740760620945   = S,    576608558731393772    = V,    585625305377507626    = G, 
		 *  7170693507665539118   = E,    8086064298967168788   = Z,    83360928582194826     = P, 
		 *  8889191829175541774   = D,    9176724567785656400   = U
		 * 
		 */
		List<TestTokenRange> tokenRanges = new ArrayList<TestTokenRange>();
		
		tokenRanges.add(new TestTokenRange("-8692134701444027338", "-7912594386904524724","C", "T", "M"));
		tokenRanges.add(new TestTokenRange("-7311855499978618814", "-4942250469937744623","X", "J", "W"));
		tokenRanges.add(new TestTokenRange("-4837624800923759386", "-2875471597373478633","B", "H", "K"));
		tokenRanges.add(new TestTokenRange("-1884162317724288694", "-422884050476930919","O", "Q", "Y"));
		tokenRanges.add(new TestTokenRange("-300136452241384611", "243126998722523514","L", "P", "A"));
		tokenRanges.add(new TestTokenRange("576608558731393772", "3625209262381227179","V", "G", "F"));
		tokenRanges.add(new TestTokenRange("3846318681772828433", "4834152074310082538","R", "N", "I"));
		tokenRanges.add(new TestTokenRange("4943864740760620945", "8086064298967168788","S", "E", "Z"));
		tokenRanges.add(new TestTokenRange("8889191829175541774", "9176724567785656400","D", "U"));

		return tokenRanges;
	}

	/** CERTAIN COLUMN FAMILIES THAT GET RE-USED A LOT FOR DIFFERENT UNIT TESTS */
	
	public static ColumnFamily<String, String> CF_COLUMN_RANGE_TEST = ColumnFamily.newColumnFamily(
			"columnrange", // Column Family Name
			StringSerializer.get(), // Key Serializer
			StringSerializer.get(), // Column Serializer
			IntegerSerializer.get()); // Data serializer;
	
	public static void createColumnFamilyForColumnRange(Keyspace keyspace) throws Exception {
		keyspace.createColumnFamily(CF_COLUMN_RANGE_TEST, null);
	}
	
	public static void populateRowsForColumnRange(Keyspace keyspace) throws Exception {
		
        MutationBatch m = keyspace.prepareMutationBatch();

        for (char keyName = 'A'; keyName <= 'Z'; keyName++) {
        	String rowKey = Character.toString(keyName);
        	ColumnListMutation<String> colMutation = m.withRow(CF_COLUMN_RANGE_TEST, rowKey);
              for (char cName = 'a'; cName <= 'z'; cName++) {
            	  colMutation.putColumn(Character.toString(cName), (int) (cName - 'a') + 1, null);
              }
              m.withCaching(true);
              m.execute();
              m.discardMutations();
        }
	}

	public static void deleteRowsForColumnRange(Keyspace keyspace) throws Exception {
		
        for (char keyName = 'A'; keyName <= 'Z'; keyName++) {
            MutationBatch m = keyspace.prepareMutationBatch();
        	String rowKey = Character.toString(keyName);
        	m.withRow(CF_COLUMN_RANGE_TEST, rowKey).delete();
        	m.execute();
        	m.discardMutations();
        }
	}
}>>>>>>> YOURS
/home/ramdisk/experiment3/projects/astyanax/revisions/rev_2b15fd9_3de01e3/rev_2b15fd9-3de01e3/astyanax-test/src/main/java/com/netflix/astyanax/cql/test/utils/ReadTests.java;<<<<<<< MINE
=======
package com.netflix.astyanax.cql.test.utils;

import java.nio.ByteBuffer;
import java.util.ArrayList;
import java.util.Arrays;
import java.util.Collection;
import java.util.Collections;
import java.util.Date;
import java.util.HashSet;
import java.util.Iterator;
import java.util.List;
import java.util.Random;
import java.util.Set;
import java.util.UUID;

import junit.framework.Assert;

import org.joda.time.DateTime;

import com.netflix.astyanax.MutationBatch;
import com.netflix.astyanax.cql.test.KeyspaceTests;
import com.netflix.astyanax.model.Column;
import com.netflix.astyanax.model.ColumnFamily;
import com.netflix.astyanax.model.ColumnList;
import com.netflix.astyanax.serializers.BytesArraySerializer;
import com.netflix.astyanax.serializers.StringSerializer;


public class ReadTests extends KeyspaceTests {

	public static DateTime OriginalDate = new DateTime().withMillisOfSecond(0).withSecondOfMinute(0).withMinuteOfHour(0).withHourOfDay(0);
	public static byte[] TestBytes = new String("TestBytes").getBytes();
	public static UUID TestUUID = UUID.fromString("edeb3d70-15ce-11e3-8ffd-0800200c9a66");
	//public static int RowCount = 1;
	
	public static String[] columnNamesArr = {"firstname", "lastname", "address","age","ageShort", "ageLong","percentile", "married","single", "birthdate", "bytes", "uuid", "empty"};
	public static List<String> columnNames = new ArrayList<String>(Arrays.asList(columnNamesArr));
	
	public static ColumnFamily<String, String> CF_USER_INFO = ColumnFamily.newColumnFamily(
			"UserInfo", // Column Family Name
			StringSerializer.get(), // Key Serializer
			StringSerializer.get()); // Column Serializer

	public static void initReadTests() throws Exception {
		initContext();
		Collections.sort(columnNames); 
	}


    public void testAllColumnsForRow(ColumnList<String> resultColumns, int i) throws Exception {

    	Date date = OriginalDate.plusMinutes(i).toDate();

    	testColumnValue(resultColumns, "firstname", columnNames, "john_" + i);
    	testColumnValue(resultColumns, "lastname", columnNames, "smith_" + i);
    	testColumnValue(resultColumns, "address", columnNames, "john smith address " + i);
    	testColumnValue(resultColumns, "age", columnNames, 30 + i);
    	testColumnValue(resultColumns, "ageShort", columnNames, new Integer(30+i).shortValue());
    	testColumnValue(resultColumns, "ageLong", columnNames, new Integer(30+i).longValue());
    	testColumnValue(resultColumns, "percentile", columnNames, 30.1);
    	testColumnValue(resultColumns, "married", columnNames, true);
    	testColumnValue(resultColumns, "single", columnNames, false);
    	testColumnValue(resultColumns, "birthdate", columnNames, date);
    	testColumnValue(resultColumns, "bytes", columnNames, TestBytes);
    	testColumnValue(resultColumns, "uuid", columnNames, TestUUID);
    	testColumnValue(resultColumns, "empty", columnNames, null);
    	
    	/** TEST THE ITERATOR INTERFACE */
    	Iterator<Column<String>> iter = resultColumns.iterator();
    	while (iter.hasNext()) {
    		Column<String> col = iter.next();
    		Assert.assertNotNull(col.getName());
    	}
    }
    
    
    private <T> void testColumnValue(ColumnList<String> result, String columnName, List<String> columnNames, T expectedValue) {
    	
    	// by column name
    	Column<String> column = result.getColumnByName(columnName);
    	Assert.assertEquals(columnName, column.getName());
    	testColumnValue(column, expectedValue);
    	
//    	// by column index
//    	int index = columnNames.indexOf(columnName);
//    	column = result.getColumnByIndex(index);
//    	testColumnValue(column, expectedValue);
    }
    
    private <T> void testColumnValue(Column<String> column, T value) {

    	// Check the column name
    	// check if value exists
    	if (value != null) {
    		Assert.assertTrue(column.hasValue());
    		if (value instanceof String) {
        		Assert.assertEquals(value, column.getStringValue());
    		} else if (value instanceof Integer) {
        		Assert.assertEquals(value, column.getIntegerValue());
    		} else if (value instanceof Short) {
        		Assert.assertEquals(value, column.getShortValue());
    		} else if (value instanceof Long) {
        		Assert.assertEquals(value, column.getLongValue());
    		} else if (value instanceof Double) {
        		Assert.assertEquals(value, column.getDoubleValue());
    		} else if (value instanceof Boolean) {
        		Assert.assertEquals(value, column.getBooleanValue());
    		} else if (value instanceof Date) {
        		Assert.assertEquals(value, column.getDateValue());
    		} else if (value instanceof byte[]) {
    			ByteBuffer bbuf = column.getByteBufferValue();
    			String result = new String(BytesArraySerializer.get().fromByteBuffer(bbuf));
    			Assert.assertEquals(new String((byte[])value), result);
    		} else if (value instanceof UUID) {
        		Assert.assertEquals(value, column.getUUIDValue());
    		} else {
    			Assert.fail("Value not recognized for column: " + column.getName()); 
    		}
    	} else {
    		// check that value does not exist
    		Assert.assertFalse(column.hasValue());
    	}
    }
    
    public void populateRows(int numRows) throws Exception {

    	MutationBatch mb = keyspace.prepareMutationBatch();

    	for (int i=0; i<numRows; i++) {

    		Date date = OriginalDate.plusMinutes(i).toDate();
    		mb.withRow(CF_USER_INFO, "acct_" + i)
    		.putColumn("firstname", "john_" + i, null)
    		.putColumn("lastname", "smith_" + i, null)
    		.putColumn("address", "john smith address " + i, null)
    		.putColumn("age", 30+i, null)
    		.putColumn("ageShort", new Integer(30+i).shortValue(), null)
    		.putColumn("ageLong", new Integer(30+i).longValue(), null)
    		.putColumn("percentile", 30.1)
    		.putColumn("married", true)
    		.putColumn("single", false)
    		.putColumn("birthdate", date)
    		.putColumn("bytes", TestBytes)
    		.putColumn("uuid", TestUUID)
    		.putEmptyColumn("empty");

    		mb.execute();
    		mb.discardMutations();
    	}
    }
	
	
	public void deleteRows(int numRows) throws Exception {

        MutationBatch mb = keyspace.prepareMutationBatch();

        for (int i=0; i<numRows; i++) {
            mb.withRow(CF_USER_INFO, "acct_" + i).delete();
            mb.execute();
            mb.discardMutations();
        }
	}


	public Collection<String> getRandomColumns(int numColumns) {

		Random random = new Random();
		Set<String> hashSet = new HashSet<String>();

		while(hashSet.size() < numColumns) {
			int pick = random.nextInt(26);
			char ch = (char) ('a' + pick);
			hashSet.add(String.valueOf(ch));
		}
		return hashSet;
	}
    
}>>>>>>> YOURS
/home/ramdisk/experiment3/projects/astyanax/revisions/rev_2b15fd9_3de01e3/rev_2b15fd9-3de01e3/astyanax-test/src/main/java/com/netflix/astyanax/cql/test/utils/AstyanaxContextFactory.java;<<<<<<< MINE
=======
package com.netflix.astyanax.cql.test.utils;

import static com.netflix.astyanax.cql.test.utils.ClusterConfiguration.TEST_CLUSTER_NAME;
import static com.netflix.astyanax.cql.test.utils.ClusterConfiguration.TEST_KEYSPACE_NAME;
import static com.netflix.astyanax.cql.test.utils.ClusterConfiguration.TheDriver;

import java.util.Collections;
import java.util.List;
import java.util.concurrent.atomic.AtomicReference;

import org.apache.log4j.PropertyConfigurator;

import com.datastax.driver.core.Configuration;
import com.datastax.driver.core.MetricsOptions;
import com.datastax.driver.core.PoolingOptions;
import com.datastax.driver.core.ProtocolOptions;
import com.datastax.driver.core.QueryOptions;
import com.datastax.driver.core.SocketOptions;
import com.datastax.driver.core.policies.Policies;
import com.google.common.base.Supplier;
import com.netflix.astyanax.AstyanaxContext;
import com.netflix.astyanax.Cluster;
import com.netflix.astyanax.Keyspace;
import com.netflix.astyanax.connectionpool.Host;
import com.netflix.astyanax.connectionpool.NodeDiscoveryType;
import com.netflix.astyanax.connectionpool.impl.ConnectionPoolConfigurationImpl;
import com.netflix.astyanax.connectionpool.impl.ConnectionPoolType;
import com.netflix.astyanax.connectionpool.impl.CountingConnectionPoolMonitor;
import com.netflix.astyanax.cql.CqlFamilyFactory;
import com.netflix.astyanax.cql.JavaDriverConnectionPoolConfigurationImpl;
import com.netflix.astyanax.cql.test.utils.ClusterConfiguration.Driver;
import com.netflix.astyanax.impl.AstyanaxConfigurationImpl;
import com.netflix.astyanax.thrift.ThriftFamilyFactory;

public class AstyanaxContextFactory {

    private static final AtomicReference<Keyspace> keyspaceReference = new AtomicReference<Keyspace>(null);
    
    static {
    	PropertyConfigurator.configure("./src/main/java/test-log4j.properties");

    	AstyanaxContext<Keyspace> context = AstyanaxContextFactory.getKeyspace();
    	context.start();
    	keyspaceReference.set(context.getClient());
    }
    
    public static AstyanaxContext<Cluster> getCluster() {
    	return getCluster(TEST_CLUSTER_NAME, TheDriver);
    }
    
    public static AstyanaxContext<Cluster> getCluster(String clusterName, Driver driver) {
    	if (driver == Driver.JAVA_DRIVER) {
    		return clusterWithJavaDriver(clusterName);
    	} else {
    		return clusterWithThriftDriver(clusterName);
    	}
    }

    private static AstyanaxContext<Cluster> clusterWithJavaDriver(String clusterName) {

    	final String SEEDS = "localhost";

		Supplier<List<Host>> HostSupplier = new Supplier<List<Host>>() {

			@Override
			public List<Host> get() {
				Host host = new Host(SEEDS, -1);
				return Collections.singletonList(host);
			}
    	};
    	
    	AstyanaxContext<Cluster> context = new AstyanaxContext.Builder()
                .forCluster(clusterName)
                .withAstyanaxConfiguration(
                        new AstyanaxConfigurationImpl()
                                .setDiscoveryType(NodeDiscoveryType.RING_DESCRIBE)
                                .setDiscoveryDelayInSeconds(60000))
                .withConnectionPoolConfiguration(
                        new ConnectionPoolConfigurationImpl(TEST_CLUSTER_NAME
                                + "_" + TEST_KEYSPACE_NAME)
                                .setSocketTimeout(30000)
                                .setMaxTimeoutWhenExhausted(2000)
                                .setMaxConnsPerHost(20)
                                .setInitConnsPerHost(10)
                                .setSeeds(SEEDS)
                                .setPort(9042)
                                )
                .withHostSupplier(HostSupplier)
                .withConnectionPoolMonitor(new CountingConnectionPoolMonitor())
                .buildCluster(CqlFamilyFactory.getInstance());

    	return context;
    }
    
    private static AstyanaxContext<Cluster> clusterWithThriftDriver(String clusterName) {
    	final String SEEDS = "localhost";

		Supplier<List<Host>> HostSupplier = new Supplier<List<Host>>() {

			@Override
			public List<Host> get() {
				Host host = new Host(SEEDS, -1);
				return Collections.singletonList(host);
			}
    	};
    	
    	AstyanaxContext<Cluster> context = new AstyanaxContext.Builder()
    	.forCluster(clusterName)
    	.withAstyanaxConfiguration(
    			new AstyanaxConfigurationImpl()
    			.setDiscoveryType(NodeDiscoveryType.DISCOVERY_SERVICE)
    			.setConnectionPoolType(ConnectionPoolType.ROUND_ROBIN)
    			.setDiscoveryDelayInSeconds(60000))
    			.withConnectionPoolConfiguration(
    					new ConnectionPoolConfigurationImpl(TEST_CLUSTER_NAME
    							+ "_" + TEST_KEYSPACE_NAME)
    					.setSocketTimeout(30000)
    					.setMaxTimeoutWhenExhausted(2000)
    					.setMaxConnsPerHost(20)
    					.setInitConnsPerHost(10)
    					.setSeeds(SEEDS)
    					.setPort(9160)
    					)
    					.withHostSupplier(HostSupplier)
    					.withConnectionPoolMonitor(new CountingConnectionPoolMonitor())
    					.buildCluster(ThriftFamilyFactory.getInstance());

    	return context;
    }

    public static AstyanaxContext<Keyspace> getKeyspace() {
    	return getKeyspace(TEST_KEYSPACE_NAME, TheDriver); 
    }

    public static AstyanaxContext<Keyspace> getKeyspace(String keyspaceName) {
    	return getKeyspace(keyspaceName, TheDriver); 
    }

    public static AstyanaxContext<Keyspace> getKeyspace(String keyspaceName, Driver driver) {
    	if (driver == Driver.THRIFT) {
        	return keyspaceWithThriftDriver(keyspaceName); 
    	} else {
    		return keyspaceWithJavaDriver(keyspaceName);
    	}
    }
    
    private static AstyanaxContext<Keyspace> keyspaceWithJavaDriver(String keyspaceName) {

    	final String SEEDS = "localhost";

		Supplier<List<Host>> HostSupplier = new Supplier<List<Host>>() {

			@Override
			public List<Host> get() {
				Host host = new Host(SEEDS, -1);
				return Collections.singletonList(host);
			}
    	};
    	
    	ProtocolOptions protocolOptions = new ProtocolOptions(9042);
		
		Configuration jdConfig = new Configuration(new Policies(),
	             protocolOptions,
	             new PoolingOptions(),
	             new SocketOptions(),
	             new MetricsOptions(),
	             new QueryOptions());

		AstyanaxContext<Keyspace> context = new AstyanaxContext.Builder()
		.forKeyspace(keyspaceName)
		.withHostSupplier(HostSupplier)
		.withAstyanaxConfiguration(new AstyanaxConfigurationImpl())
		.withConnectionPoolConfiguration(new JavaDriverConnectionPoolConfigurationImpl()
										.withJavaDriverConfig(jdConfig)
										)
		.buildKeyspace(CqlFamilyFactory.getInstance());

    	return context;
    }
    
    private static AstyanaxContext<Keyspace> keyspaceWithThriftDriver(String keyspaceName) {

    	final String SEEDS = "localhost";

		Supplier<List<Host>> HostSupplier = new Supplier<List<Host>>() {

			@Override
			public List<Host> get() {
				Host host = new Host(SEEDS, 9160);
				return Collections.singletonList(host);
			}
    	};
    	
    	AstyanaxContext<Keyspace> context = new AstyanaxContext.Builder()
                .forCluster(TEST_CLUSTER_NAME)
                .forKeyspace(keyspaceName)
                .withAstyanaxConfiguration(
                        new AstyanaxConfigurationImpl()
                                .setDiscoveryType(NodeDiscoveryType.DISCOVERY_SERVICE)
                                .setConnectionPoolType(ConnectionPoolType.ROUND_ROBIN)
                                .setDiscoveryDelayInSeconds(60000)
                                .setTargetCassandraVersion("1.2")
                                )
                                .withConnectionPoolConfiguration(
                        new ConnectionPoolConfigurationImpl(TEST_CLUSTER_NAME
                                + "_" + TEST_KEYSPACE_NAME)
                                .setSocketTimeout(30000)
                                .setMaxTimeoutWhenExhausted(2000)
                                .setMaxConnsPerHost(20)
                                .setInitConnsPerHost(10)
                                .setSeeds(SEEDS)
                                .setPort(9160)
                                )
                .withHostSupplier(HostSupplier)
                .withConnectionPoolMonitor(new CountingConnectionPoolMonitor())
                .buildKeyspace(ThriftFamilyFactory.getInstance());

    	return context;
    }

    public static Keyspace getCachedKeyspace() {
    	return keyspaceReference.get();
    }
}>>>>>>> YOURS
/home/ramdisk/experiment3/projects/astyanax/revisions/rev_2b15fd9_3de01e3/rev_2b15fd9-3de01e3/astyanax-test/src/main/java/com/netflix/astyanax/cql/test/utils/ClusterConfiguration.java;<<<<<<< MINE
=======
package com.netflix.astyanax.cql.test.utils;


/**
 * Configuration class that dictates what Driver to use under the covers for the Astyanax Client.
 * Users can explicitly set the driver to use - default if JAVA_DRIVER. 
 * 
 * Note that this class provides helpful utilities to setup the AstyanaxContext using the 
 * specified driver. 
 * 
 * @author poberai
 *
 */
public class ClusterConfiguration {
	
    public static String TEST_CLUSTER_NAME  = "Test Cluster"; // use cass_sandbox
    public static String TEST_KEYSPACE_NAME = "astyanaxunittests";
    
    public static Driver TheDriver = Driver.JAVA_DRIVER;
    //public static Driver TheDriver = Driver.THRIFT;
    
    public static enum Driver {
    	THRIFT, JAVA_DRIVER; 
    }
    
    public static void setDriver(Driver driver) {
    	TheDriver = driver;
    }

    public static Driver getDriver() {
    	return TheDriver;
    }
}>>>>>>> YOURS
/home/ramdisk/experiment3/projects/astyanax/revisions/rev_2b15fd9_3de01e3/rev_2b15fd9-3de01e3/astyanax-test/src/main/java/com/netflix/astyanax/cql/test/entitymapper/EntityMapperTests.java;<<<<<<< MINE
=======
package com.netflix.astyanax.cql.test.entitymapper;

import java.util.ArrayList;
import java.util.List;

import javax.persistence.Column;
import javax.persistence.Entity;
import javax.persistence.Id;

import junit.framework.Assert;

import org.junit.AfterClass;
import org.junit.BeforeClass;
import org.junit.Test;

import com.netflix.astyanax.cql.test.KeyspaceTests;
import com.netflix.astyanax.cql.test.entitymapper.EntityMapperTests.SampleTestCompositeEntity.InnerEntity;
import com.netflix.astyanax.entitystore.DefaultEntityManager;
import com.netflix.astyanax.entitystore.EntityManager;
import com.netflix.astyanax.model.ColumnFamily;
import com.netflix.astyanax.serializers.StringSerializer;

public class EntityMapperTests extends KeyspaceTests {
	
    private static ColumnFamily<String, String> CF_SAMPLE_TEST_ENTITY = ColumnFamily
            .newColumnFamily(
                    "sampletestentity", 
                    StringSerializer.get(),
                    StringSerializer.get());

	private static EntityManager<SampleTestEntity, String> entityManager;
	private static EntityManager<SampleTestCompositeEntity, String> compositeEntityManager;

    @BeforeClass
	public static void init() throws Exception {
		initContext();
		keyspace.createColumnFamily(CF_SAMPLE_TEST_ENTITY, null);
		
    	CF_SAMPLE_TEST_ENTITY.describe(keyspace);
    	
    	entityManager = 
        		new DefaultEntityManager.Builder<SampleTestEntity, String>()
        		.withEntityType(SampleTestEntity.class)
        		.withKeyspace(keyspace)
        		.withColumnFamily(CF_SAMPLE_TEST_ENTITY)
        		.build();

    	compositeEntityManager = 
        		new DefaultEntityManager.Builder<SampleTestCompositeEntity, String>()
        		.withEntityType(SampleTestCompositeEntity.class)
        		.withKeyspace(keyspace)
        		.withColumnFamily(CF_SAMPLE_TEST_ENTITY)
        		.build();
    }

    @AfterClass
	public static void tearDown() throws Exception {
    	keyspace.dropColumnFamily(CF_SAMPLE_TEST_ENTITY);
	}
    
    
    @Test
    public void testSimpleEntityCRUD() throws Exception {
    	
    	final String ID = "testSimpleEntityCRUD";
    	
    	final SampleTestEntity testEntity = new SampleTestEntity();
    	
    	testEntity.id = ID;
    	testEntity.testInt = 1;
    	testEntity.testLong = 2L;
    	testEntity.testString = "testString1";
    	testEntity.testDouble = 3.0;
    	testEntity.testFloat = 4.0f; 
    	testEntity.testBoolean = true;
    	
    	// PUT
    	entityManager.put(testEntity);
    	
    	// GET
    	SampleTestEntity getEntity = entityManager.get(ID);
    	Assert.assertNotNull(getEntity);
    	Assert.assertTrue(testEntity.equals(getEntity));
    	
    	// DELETE
    	entityManager.delete(ID);
    	getEntity = entityManager.get(ID);
    	Assert.assertNull(getEntity);
    }

    @Test
    public void testSimpleEntityList() throws Exception {
    	
    	List<SampleTestEntity> entities = new ArrayList<SampleTestEntity>();
    	List<String> ids = new ArrayList<String>();
    	
    	int entityCount = 11;
    	
    	for (int i=0; i<entityCount; i++) {
    		
        	SampleTestEntity testEntity = new SampleTestEntity();
        	
        	testEntity.id = "id" + i;
        	testEntity.testInt = i;
        	testEntity.testLong = i;
        	testEntity.testString = "testString" + i;
        	testEntity.testDouble = i;
        	testEntity.testFloat = i; 
        	testEntity.testBoolean = true;
        	
        	entities.add(testEntity);
        	ids.add("id" + i);
    	}
    	
    	// PUT COLLECTION
    	entityManager.put(entities);
    	
    	// GET
    	List<SampleTestEntity> getEntities = entityManager.get(ids);
    	Assert.assertTrue(entityCount == getEntities.size());
    	
    	int count = 0;
    	for (SampleTestEntity e : getEntities) {
    		Assert.assertTrue(count == e.testInt);
    		Assert.assertTrue(count == e.testLong);
    		Assert.assertTrue(("testString" + count).equals(e.testString));
    		count++;
    	}
    	
    	// DELETE
    	entityManager.delete(ids);
    	
    	// GET AFTER DELETE
    	getEntities = entityManager.get(ids);
    	Assert.assertTrue(0 == getEntities.size());
    }

    @Test
    public void testGetAll() throws Exception {
    	
    	List<SampleTestEntity> entities = new ArrayList<SampleTestEntity>();
    	List<String> ids = new ArrayList<String>();
    	
    	int entityCount = 11;
    	
    	for (int i=0; i<entityCount; i++) {
    		
        	SampleTestEntity testEntity = new SampleTestEntity();
        	
        	testEntity.id = "id" + i;
        	testEntity.testInt = i;
        	testEntity.testLong = i;
        	testEntity.testString = "testString" + i;
        	testEntity.testDouble = i;
        	testEntity.testFloat = i; 
        	testEntity.testBoolean = true;
        	
        	entities.add(testEntity);
        	ids.add("id" + i);
    	}
    	
    	// PUT COLLECTION
    	entityManager.put(entities);
    	
    	List<SampleTestEntity> getEntities = entityManager.getAll();
    	Assert.assertTrue(entityCount == getEntities.size());
    	for (SampleTestEntity e : getEntities) {
    		String id = e.id;
    		int i = Integer.parseInt(id.substring("id".length()));
    		Assert.assertTrue(i == e.testInt);
    		Assert.assertTrue(i == e.testLong);
    		Assert.assertTrue(("testString" + i).equals(e.testString));
    	}
    	
    	// DELETE
    	entityManager.delete(ids);
    	// GET AFTER DELETE
    	getEntities = entityManager.getAll();
    	Assert.assertTrue(0 == getEntities.size());
    }

    @Test
    public void testCompositeEntityCRUD() throws Exception {
    	
    	final String ID = "testCompositeEntityCRUD";

    	final SampleTestCompositeEntity testEntity = new SampleTestCompositeEntity();
    	
    	testEntity.id = ID;
    	testEntity.testInt = 1;
    	testEntity.testLong = 2L;
    	testEntity.testString = "testString1";
    	testEntity.testDouble = 3.0;
    	testEntity.testFloat = 4.0f; 
    	testEntity.testBoolean = true;
    	
    	testEntity.inner = new InnerEntity();
    	testEntity.inner.testInnerInt = 11;
    	testEntity.inner.testInnerLong = 22L;
    	testEntity.inner.testInnerString = "testInnserString1";
    	
    	// PUT
    	compositeEntityManager.put(testEntity);
    	
    	// GET
    	SampleTestCompositeEntity getEntity = compositeEntityManager.get(ID);
    	System.out.println(getEntity);
    	Assert.assertNotNull(getEntity);
    	Assert.assertTrue(testEntity.equals(getEntity));
    	
    	// DELETE
    	entityManager.delete(ID);
    	getEntity = compositeEntityManager.get(ID);
    	Assert.assertNull(getEntity);
    }

    @Entity
    public static class SampleTestEntity {
    	
    	@Id
    	private String id;
    	
    	@Column(name="integer")
    	private int testInt; 
    	@Column(name="long")
    	private long testLong;
    	@Column(name="string")
    	private String testString; 
    	@Column(name="float")
    	private float testFloat;
    	@Column(name="double")
    	private double testDouble;
    	@Column(name="boolean")
    	private boolean testBoolean;
    	
    	public SampleTestEntity() {
    		
    	}
    	
		@Override
		public String toString() {
			return "SampleTestEntity [\nid=" + id + "\ntestInt=" + testInt
					+ "\ntestLong=" + testLong + "\ntestString=" + testString
					+ "\ntestFloat=" + testFloat + "\ntestDouble=" + testDouble
					+ "\ntestBoolean=" + testBoolean + "]";
		}

		@Override
		public int hashCode() {
			final int prime = 31;
			int result = 1;
			result = prime * result + ((id == null) ? 0 : id.hashCode());
			result = prime * result + (testBoolean ? 1231 : 1237);
			long temp;
			temp = Double.doubleToLongBits(testDouble);
			result = prime * result + (int) (temp ^ (temp >>> 32));
			result = prime * result + Float.floatToIntBits(testFloat);
			result = prime * result + testInt;
			result = prime * result + (int) (testLong ^ (testLong >>> 32));
			result = prime * result + ((testString == null) ? 0 : testString.hashCode());
			return result;
		}

		@Override
		public boolean equals(Object obj) {
			
			if (this == obj) return true;
			if (obj == null) return false;
			if (getClass() != obj.getClass()) return false;
			
			SampleTestEntity other = (SampleTestEntity) obj;
			boolean equal = true;

			equal &=  (id != null) ? id.equals(other.id) : other.id == null;
			equal &=  testInt == other.testInt;
			equal &=  testLong == other.testLong;
			equal &=  testBoolean == other.testBoolean;
			equal &=  (testString != null) ? testString.equals(other.testString) : other.testString == null;
			equal &= (Double.doubleToLongBits(testDouble) == Double.doubleToLongBits(other.testDouble));
			equal &= (Float.floatToIntBits(testFloat) == Float.floatToIntBits(other.testFloat));
			
			return equal;
		}
    }
    
    @Entity
    public static class SampleTestCompositeEntity {
    	
    	@Id
    	private String id;
    	
    	@Column(name="integer")
    	private int testInt; 
    	@Column(name="long")
    	private long testLong;
    	@Column(name="string")
    	private String testString; 
    	@Column(name="float")
    	private float testFloat;
    	@Column(name="double")
    	private double testDouble;
    	@Column(name="boolean")
    	private boolean testBoolean;
    	
    	@Entity
    	public static class InnerEntity {
    		
        	@Column(name="inner_integer")
        	private int testInnerInt; 
        	@Column(name="inner_long")
        	private long testInnerLong;
        	@Column(name="inner_string")
        	private String testInnerString; 
        	
    		@Override
    		public String toString() {
    			return "InnerEntity [\ninnerInt="  + testInnerInt
    					+ "\ninnerLong=" + testInnerLong + "\ninnerString=" + testInnerString + "]";
    		}

    		@Override
    		public int hashCode() {
    			final int prime = 31;
    			int result = 1;
    			result = prime * result + testInnerInt;
    			result = prime * result + (int) (testInnerLong ^ (testInnerLong >>> 32));
    			result = prime * result + ((testInnerString == null) ? 0 : testInnerString.hashCode());
    			return result;
    		}

    		@Override
    		public boolean equals(Object obj) {
    			
    			if (this == obj) return true;
    			if (obj == null) return false;
    			if (getClass() != obj.getClass()) return false;
    			
    			InnerEntity other = (InnerEntity) obj;
    			boolean equal = true;
    			equal &=  testInnerInt == other.testInnerInt;
    			equal &=  testInnerLong == other.testInnerLong;
    			equal &=  (testInnerString != null) ? testInnerString.equals(other.testInnerString) : other.testInnerString == null;
    			return equal;
    		}

    	}
    	
    	@Column(name="inner")
    	private InnerEntity inner; 
    	
    	public SampleTestCompositeEntity() {
    		
    	}
    	
		@Override
		public String toString() {
			return "SampleTestEntity [\nid=" + id + "\ntestInt=" + testInt
					+ "\ntestLong=" + testLong + "\ntestString=" + testString
					+ "\ntestFloat=" + testFloat + "\ntestDouble=" + testDouble
					+ "\ntestBoolean=" + testBoolean 
					+ "\ninner = " + inner.toString() + "]";
		}

		@Override
		public int hashCode() {
			final int prime = 31;
			int result = 1;
			result = prime * result + ((id == null) ? 0 : id.hashCode());
			result = prime * result + (testBoolean ? 1231 : 1237);
			long temp;
			temp = Double.doubleToLongBits(testDouble);
			result = prime * result + (int) (temp ^ (temp >>> 32));
			result = prime * result + Float.floatToIntBits(testFloat);
			result = prime * result + testInt;
			result = prime * result + (int) (testLong ^ (testLong >>> 32));
			result = prime * result + ((testString == null) ? 0 : testString.hashCode());
			result = prime * result + ((inner == null) ? 0 : inner.hashCode());
			return result;
		}

		@Override
		public boolean equals(Object obj) {
			
			if (this == obj) return true;
			if (obj == null) return false;
			if (getClass() != obj.getClass()) return false;
			
			SampleTestCompositeEntity other = (SampleTestCompositeEntity) obj;
			boolean equal = true;

			equal &=  (id != null) ? id.equals(other.id) : other.id == null;
			equal &=  testInt == other.testInt;
			equal &=  testLong == other.testLong;
			equal &=  testBoolean == other.testBoolean;
			equal &=  (testString != null) ? testString.equals(other.testString) : other.testString == null;
			equal &= (Double.doubleToLongBits(testDouble) == Double.doubleToLongBits(other.testDouble));
			equal &= (Float.floatToIntBits(testFloat) == Float.floatToIntBits(other.testFloat));
			equal &=  (inner != null) ? inner.equals(other.inner) : other.inner == null;
			
			return equal;
		}
    }
}>>>>>>> YOURS
/home/ramdisk/experiment3/projects/astyanax/revisions/rev_2b15fd9_3de01e3/rev_2b15fd9-3de01e3/astyanax-test/src/main/java/com/netflix/astyanax/cql/test/recipes/AllRowsReaderTest.java;<<<<<<< MINE
=======
package com.netflix.astyanax.cql.test.recipes;

import java.util.concurrent.atomic.AtomicInteger;

import junit.framework.Assert;

import org.junit.AfterClass;
import org.junit.BeforeClass;
import org.junit.Test;

import com.google.common.base.Function;
import com.netflix.astyanax.ColumnListMutation;
import com.netflix.astyanax.MutationBatch;
import com.netflix.astyanax.cql.test.KeyspaceTests;
import com.netflix.astyanax.model.ColumnFamily;
import com.netflix.astyanax.model.ColumnList;
import com.netflix.astyanax.model.Row;
import com.netflix.astyanax.model.Rows;
import com.netflix.astyanax.partitioner.Murmur3Partitioner;
import com.netflix.astyanax.recipes.reader.AllRowsReader;
import com.netflix.astyanax.serializers.StringSerializer;

public class AllRowsReaderTest extends KeyspaceTests {

	public static ColumnFamily<String, String> CF_ALL_ROWS_READER = ColumnFamily
            .newColumnFamily(
                    "allrowsreader", 
                    StringSerializer.get(),
                    StringSerializer.get());

    @BeforeClass
	public static void init() throws Exception {
		initContext();
		
		keyspace.createColumnFamily(CF_ALL_ROWS_READER, null);
		CF_ALL_ROWS_READER.describe(keyspace);
		
		/** POPULATE ROWS FOR TESTS */
    	MutationBatch m = keyspace.prepareMutationBatch();

        for (char keyName = 'A'; keyName <= 'Z'; keyName++) {
            String rowKey = Character.toString(keyName);
            ColumnListMutation<String> cfmStandard = m.withRow(CF_ALL_ROWS_READER, rowKey);
            for (char cName = 'a'; cName <= 'z'; cName++) {
                cfmStandard.putColumn(Character.toString(cName), (int) (cName - 'a') + 1, null);
            }
            m.execute();
            m.discardMutations();
        }
    }

    @AfterClass
	public static void tearDown() throws Exception {
    	keyspace.dropColumnFamily(CF_ALL_ROWS_READER);
    }

	
	@Test
	public void testAllRowsReader() throws Exception {
		
		final AtomicInteger rowCount = new AtomicInteger(0);
		
		new AllRowsReader.Builder<String, String>(keyspace, CF_ALL_ROWS_READER)
				.withPageSize(100) // Read 100 rows at a time
				.withConcurrencyLevel(10) // Split entire token range into 10.  Default is by number of nodes.
				.withPartitioner(Murmur3Partitioner.get())
				.forEachPage(new Function<Rows<String, String>, Boolean>() {
					@Override
					public Boolean apply(Rows<String, String> rows) {
						// Process the row here ...
						// This will be called from multiple threads so make sure your code is thread safe
						for (Row<String, String> row : rows) {
							ColumnList<String> colList = row.getColumns();
							Assert.assertTrue("ColList: " + colList.size(), 26 == colList.size());
							rowCount.incrementAndGet();
						}
						return true;
					}
				})
				.build()
				.call();

		Assert.assertTrue("RowCount: " + rowCount.get(), 26 == rowCount.get());
	}
}>>>>>>> YOURS
/home/ramdisk/experiment3/projects/astyanax/revisions/rev_2b15fd9_3de01e3/rev_2b15fd9-3de01e3/astyanax-test/src/main/java/com/netflix/astyanax/cql/test/recipes/ColumnPrefixDistributedLockTest.java;<<<<<<< MINE
=======
package com.netflix.astyanax.cql.test.recipes;

import java.util.concurrent.TimeUnit;

import junit.framework.Assert;

import org.junit.AfterClass;
import org.junit.BeforeClass;
import org.junit.Test;

import com.netflix.astyanax.MutationBatch;
import com.netflix.astyanax.cql.test.KeyspaceTests;
import com.netflix.astyanax.model.ColumnFamily;
import com.netflix.astyanax.model.ColumnList;
import com.netflix.astyanax.model.ConsistencyLevel;
import com.netflix.astyanax.recipes.locks.BusyLockException;
import com.netflix.astyanax.recipes.locks.ColumnPrefixDistributedRowLock;
import com.netflix.astyanax.recipes.locks.StaleLockException;
import com.netflix.astyanax.serializers.StringSerializer;

public class ColumnPrefixDistributedLockTest extends KeyspaceTests {

	public static ColumnFamily<String, String> CF_DIST_LOCK = ColumnFamily
            .newColumnFamily(
                    "distlock", 
                    StringSerializer.get(),
                    StringSerializer.get());

    @BeforeClass
	public static void init() throws Exception {
		initContext();
		keyspace.createColumnFamily(CF_DIST_LOCK, null);
		CF_DIST_LOCK.describe(keyspace);
    }

    @AfterClass
	public static void tearDown() throws Exception {
    	keyspace.dropColumnFamily(CF_DIST_LOCK);
    }

    @Test
    public void testLockSuccess() throws Exception {

    	ColumnPrefixDistributedRowLock<String> lock = 
    			new ColumnPrefixDistributedRowLock<String>(keyspace, CF_DIST_LOCK, "RowKeyToLock")
    			.expireLockAfter(1, TimeUnit.SECONDS)
    			.withConsistencyLevel(ConsistencyLevel.CL_ONE);

    	try {
    		lock.acquire();
    		System.out.println("Successfully acquired lock");
    	} catch (StaleLockException e) {
    		// The row contains a stale or abandoned lock
    		// These can either be manually clean up or automatically
    		// cleaned up (and ignored) by calling failOnStaleLock(false)
    		e.printStackTrace();
    		Assert.fail(e.getMessage());
    	} catch (BusyLockException e) {
    		// The row is currently locked.
    		e.printStackTrace();
    		Assert.fail(e.getMessage());
    	} finally {
    		lock.release();
    	}
    	
    	// VERIFY THAT THE LOCK ROW IS EMPTY
    	ColumnList<String> result = keyspace.prepareQuery(CF_DIST_LOCK).getRow("RowKeyToLock").execute().getResult();
    	Assert.assertTrue(result.isEmpty());
    }
    

    @Test
    public void testStaleLock() throws Exception {

    	String rowKey = "StaleRowKeyToLock";
    	String lockPrefix = "TestLockPrefix";
    	
    	Long pastTime = System.currentTimeMillis() - 2000L; 
    	Long timeInMicros = TimeUnit.MICROSECONDS.convert(pastTime, TimeUnit.MILLISECONDS);
    	
    	MutationBatch m = keyspace.prepareMutationBatch();
    	m.withRow(CF_DIST_LOCK, rowKey).putColumn(lockPrefix + "1", timeInMicros);
    	m.execute();

    	ColumnPrefixDistributedRowLock<String> lock = 
    			new ColumnPrefixDistributedRowLock<String>(keyspace, CF_DIST_LOCK, rowKey)
    			.withColumnPrefix(lockPrefix)
    			.expireLockAfter(1, TimeUnit.SECONDS)
    			.failOnStaleLock(true)
    			.withConsistencyLevel(ConsistencyLevel.CL_ONE);

    	try {
    		lock.acquire();
    		Assert.fail("Acquired lock when there was a STALE LOCK");
    		
    	} catch (StaleLockException e) {
    		// The row contains a stale or abandoned lock
    		// These can either be manually clean up or automatically
    		// cleaned up (and ignored) by calling failOnStaleLock(false)
    		System.out.println("STALE LOCK "  + e.getMessage());
    		lock.releaseExpiredLocks();
    		lock.release();
    	} 
    	
    	// VERIFY THAT THE LOCK ROW IS EMPTY
    	ColumnList<String> result = keyspace.prepareQuery(CF_DIST_LOCK).getRow(rowKey).execute().getResult();
    	Assert.assertTrue(result.isEmpty());

    }
    
    @Test
    public void testBusyLock() throws Exception {

    	String rowKey = "BusyRowKeyToLock";
    	String lockPrefix = "TestLockPrefix";
    	
    	Long futureTime = System.currentTimeMillis() + 20000L; 
    	Long timeInMicros = TimeUnit.MICROSECONDS.convert(futureTime, TimeUnit.MILLISECONDS);

    	MutationBatch m = keyspace.prepareMutationBatch();
    	m.withRow(CF_DIST_LOCK, rowKey).putColumn(lockPrefix + "1", timeInMicros);
    	m.execute();

    	ColumnPrefixDistributedRowLock<String> lock = 
    			new ColumnPrefixDistributedRowLock<String>(keyspace, CF_DIST_LOCK, rowKey)
    			.withColumnPrefix(lockPrefix)
    			.expireLockAfter(1, TimeUnit.SECONDS)
    			.failOnStaleLock(true)
    			.withConsistencyLevel(ConsistencyLevel.CL_ONE);

    	try {
    		lock.acquire();
    		Assert.fail("Acquired lock when there was a STALE LOCK");
    		
    	} catch (BusyLockException e) {
    		// The row is currently locked.
    		System.out.println("BUSY LOCK "  + e.getMessage());
    		lock.releaseAllLocks();
    	} 
    	
    	// VERIFY THAT THE LOCK ROW IS EMPTY
    	ColumnList<String> result = keyspace.prepareQuery(CF_DIST_LOCK).getRow(rowKey).execute().getResult();
    	Assert.assertTrue(result.isEmpty());
    }
}>>>>>>> YOURS
/home/ramdisk/experiment3/projects/astyanax/revisions/rev_2b15fd9_3de01e3/rev_2b15fd9-3de01e3/astyanax-test/src/main/java/com/netflix/astyanax/cql/test/recipes/ChunkedObjectStoreTest.java;<<<<<<< MINE
=======
package com.netflix.astyanax.cql.test.recipes;

import java.io.ByteArrayInputStream;
import java.io.ByteArrayOutputStream;

import junit.framework.Assert;

import org.junit.AfterClass;
import org.junit.BeforeClass;
import org.junit.Test;

import com.netflix.astyanax.cql.test.KeyspaceTests;
import com.netflix.astyanax.model.ColumnFamily;
import com.netflix.astyanax.model.ColumnList;
import com.netflix.astyanax.recipes.storage.CassandraChunkedStorageProvider;
import com.netflix.astyanax.recipes.storage.ChunkedStorage;
import com.netflix.astyanax.recipes.storage.ObjectMetadata;
import com.netflix.astyanax.serializers.StringSerializer;

public class ChunkedObjectStoreTest extends KeyspaceTests {

	public static ColumnFamily<String, String> CF_CHUNK = ColumnFamily.newColumnFamily("cfchunk", StringSerializer.get(), StringSerializer.get());

	@BeforeClass
	public static void init() throws Exception {
		initContext();
		keyspace.createColumnFamily(CF_CHUNK, null);
		CF_CHUNK.describe(keyspace);
	}

	@AfterClass
	public static void tearDown() throws Exception {
		keyspace.dropColumnFamily(CF_CHUNK);
	}
	
	@Test
	public void testAll() throws Exception {
		
		CassandraChunkedStorageProvider provider = new CassandraChunkedStorageProvider(keyspace, CF_CHUNK);
		
		StringBuilder sb = new StringBuilder();
		for (int i=0; i<100; i++) {
			sb.append("abcdefghijklmnopqrstuvwxyz_");
		}
		
		String input = sb.toString();
		
		ByteArrayInputStream in = new ByteArrayInputStream(input.getBytes());
		
		ObjectMetadata meta = ChunkedStorage.newWriter(provider, "MyObject", in)
			    .withChunkSize(100)
			    .call();
		
		meta = ChunkedStorage.newInfoReader(provider, "MyObject").call();
		System.out.println(meta.getObjectSize().intValue());
		System.out.println(meta.getChunkCount());
		
		ByteArrayOutputStream os = new ByteArrayOutputStream(meta.getObjectSize().intValue());
		
		meta = ChunkedStorage.newReader(provider, "MyObject", os)
				  .withBatchSize(11)       // Randomize fetching blocks within a batch. 
				  .withConcurrencyLevel(3)
				.call();
		
		String output = os.toString();
		
		Assert.assertEquals(input, output);
		
		ChunkedStorage.newDeleter(provider, "MyObject").call();
		
		for (int i=0; i<meta.getChunkCount(); i++) {
			ColumnList<String> result = keyspace.prepareQuery(CF_CHUNK).getKey("MyObject$" + i).execute().getResult();
			Assert.assertTrue(result.isEmpty());
		}
	}
}>>>>>>> YOURS
/home/ramdisk/experiment3/projects/astyanax/revisions/rev_2b15fd9_3de01e3/rev_2b15fd9-3de01e3/astyanax-test/src/main/java/com/netflix/astyanax/cql/test/recipes/ColumnPrefixUniquenessConstraintTest.java;<<<<<<< MINE
=======
package com.netflix.astyanax.cql.test.recipes;

import java.util.UUID;

import junit.framework.Assert;

import org.junit.AfterClass;
import org.junit.BeforeClass;
import org.junit.Test;

import com.google.common.base.Function;
import com.google.common.base.Supplier;
import com.netflix.astyanax.MutationBatch;
import com.netflix.astyanax.cql.test.KeyspaceTests;
import com.netflix.astyanax.model.ColumnFamily;
import com.netflix.astyanax.model.ColumnList;
import com.netflix.astyanax.model.ConsistencyLevel;
import com.netflix.astyanax.recipes.locks.BusyLockException;
import com.netflix.astyanax.recipes.uniqueness.ColumnPrefixUniquenessConstraint;
import com.netflix.astyanax.serializers.LongSerializer;
import com.netflix.astyanax.serializers.StringSerializer;

public class ColumnPrefixUniquenessConstraintTest extends KeyspaceTests {

	public static ColumnFamily<Long, String> CF_UNIQUE_CONSTRAINT = ColumnFamily
			.newColumnFamily(
					"cfunique2", 
					LongSerializer.get(),
					StringSerializer.get());

	@BeforeClass
	public static void init() throws Exception {
		initContext();
		keyspace.createColumnFamily(CF_UNIQUE_CONSTRAINT, null);
		CF_UNIQUE_CONSTRAINT.describe(keyspace);
	}

	@AfterClass
	public static void tearDown() throws Exception {
		keyspace.dropColumnFamily(CF_UNIQUE_CONSTRAINT);
	}


	Supplier<String> UniqueColumnSupplier = new Supplier<String>() {

		@Override
		public String get() {
			return UUID.randomUUID().toString();
		}
	};

	@Test
	public void testUnique() throws Exception {

		ColumnPrefixUniquenessConstraint<Long> unique = 
				new ColumnPrefixUniquenessConstraint<Long>(keyspace, CF_UNIQUE_CONSTRAINT, 1L)
				.withConsistencyLevel(ConsistencyLevel.CL_ONE);

		unique.acquire();

		try { 
			unique = 
					new ColumnPrefixUniquenessConstraint<Long>(keyspace, CF_UNIQUE_CONSTRAINT, 1L)
					.withConsistencyLevel(ConsistencyLevel.CL_ONE);
			unique.acquire();
			Assert.fail("Should have gotten a BusyLockException");
		} catch (BusyLockException e) {
			System.out.println(e.getMessage());
		}
	}
	

	@Test
	public void testUniqueAndRelease() throws Exception {

		ColumnPrefixUniquenessConstraint<Long> unique = 
				new ColumnPrefixUniquenessConstraint<Long>(keyspace, CF_UNIQUE_CONSTRAINT, 2L)
				.withConsistencyLevel(ConsistencyLevel.CL_ONE);

		unique.acquire();
		unique.release();

		unique = new ColumnPrefixUniquenessConstraint<Long>(keyspace, CF_UNIQUE_CONSTRAINT, 2L)
					.withConsistencyLevel(ConsistencyLevel.CL_ONE);
		unique.acquire();
	}


	@Test
	public void testUniquenessWithCustomMutation() throws Exception {

		ColumnList<String> result = keyspace.prepareQuery(CF_UNIQUE_CONSTRAINT).getRow(10L).execute().getResult();
		Assert.assertTrue(result.isEmpty());

		ColumnPrefixUniquenessConstraint<Long> unique = 
				new ColumnPrefixUniquenessConstraint<Long>(keyspace, CF_UNIQUE_CONSTRAINT, 3L)
				.withConsistencyLevel(ConsistencyLevel.CL_ONE);

		unique.acquireAndApplyMutation(new Function<MutationBatch, Boolean>() {
			public Boolean apply(MutationBatch input) {

				input.withRow(CF_UNIQUE_CONSTRAINT, 10L).putEmptyColumn("MyCustomColumn", null);
				return true;
			}
		});

		result = keyspace.prepareQuery(CF_UNIQUE_CONSTRAINT).getRow(10L).execute().getResult();
		Assert.assertFalse(result.isEmpty());
	}
}>>>>>>> YOURS
/home/ramdisk/experiment3/projects/astyanax/revisions/rev_2b15fd9_3de01e3/rev_2b15fd9-3de01e3/astyanax-test/src/main/java/com/netflix/astyanax/cql/test/CFStandardTests.java;<<<<<<< MINE
=======
package com.netflix.astyanax.cql.test;

import java.io.Serializable;
import java.nio.ByteBuffer;
import java.util.ArrayList;
import java.util.Iterator;
import java.util.List;
import java.util.Map;
import java.util.Map.Entry;
import java.util.Random;
import java.util.concurrent.ExecutionException;
import java.util.concurrent.Future;
import java.util.concurrent.TimeUnit;
import java.util.concurrent.TimeoutException;

import junit.framework.Assert;

import org.apache.commons.lang.RandomStringUtils;
import org.apache.log4j.Logger;
import org.junit.AfterClass;
import org.junit.BeforeClass;
import org.junit.Test;

import com.netflix.astyanax.ColumnListMutation;
import com.netflix.astyanax.MutationBatch;
import com.netflix.astyanax.connectionpool.OperationResult;
import com.netflix.astyanax.connectionpool.exceptions.ConnectionException;
import com.netflix.astyanax.connectionpool.exceptions.NotFoundException;
import com.netflix.astyanax.model.Column;
import com.netflix.astyanax.model.ColumnFamily;
import com.netflix.astyanax.model.ColumnList;
import com.netflix.astyanax.model.ColumnSlice;
import com.netflix.astyanax.model.ConsistencyLevel;
import com.netflix.astyanax.model.CqlResult;
import com.netflix.astyanax.model.Row;
import com.netflix.astyanax.model.Rows;
import com.netflix.astyanax.query.ColumnQuery;
import com.netflix.astyanax.query.RowQuery;
import com.netflix.astyanax.serializers.ObjectSerializer;
import com.netflix.astyanax.serializers.StringSerializer;
import com.netflix.astyanax.util.RangeBuilder;

public class CFStandardTests extends KeyspaceTests {
	
	private static final Logger LOG = Logger.getLogger(CFStandardTests.class);
	
    public static ColumnFamily<String, String> CF_STANDARD1 = ColumnFamily
            .newColumnFamily(
                    "Standard1", 
                    StringSerializer.get(),
                    StringSerializer.get());
    
    public static ColumnFamily<String, String> CF_STANDARD2 = ColumnFamily
            .newColumnFamily(
                    "Standard2", 
                    StringSerializer.get(),
                    StringSerializer.get());

    private static ColumnFamily<String, String> CF_USER_INFO = ColumnFamily.newColumnFamily(
            "UserInfo", // Column Family Name
            StringSerializer.get(), // Key Serializer
            StringSerializer.get()); // Column Serializer

    @BeforeClass
	public static void init() throws Exception {
    	initContext();
		
		keyspace.createColumnFamily(CF_STANDARD1, null);
		keyspace.createColumnFamily(CF_STANDARD2, null);
		keyspace.createColumnFamily(CF_USER_INFO, null);
    	
		CF_STANDARD1.describe(keyspace);
    	CF_STANDARD2.describe(keyspace);
    	CF_USER_INFO.describe(keyspace);
	}

    @AfterClass
	public static void tearDown() throws Exception {
    	keyspace.dropColumnFamily(CF_STANDARD1);
    	keyspace.dropColumnFamily(CF_STANDARD2);
    	keyspace.dropColumnFamily(CF_USER_INFO);
	}

    @Test
    public void testSerializedClassValue() throws Exception {

    	UserInfo smo = new UserInfo();
    	smo.setLastName("Landau");
    	smo.setFirstName("Eran");

    	ByteBuffer bb = ObjectSerializer.get().toByteBuffer(smo);
    	
    	keyspace.prepareColumnMutation(CF_STANDARD1, "Key_SerializeTest",
    			"Column1").putValue(bb, null).execute();

    	UserInfo smo2 = (UserInfo) keyspace.prepareQuery(CF_STANDARD1)
    			.getKey("Key_SerializeTest").getColumn("Column1").execute()
    			.getResult().getValue(ObjectSerializer.get());
    	
    	Assert.assertEquals(smo, smo2);
    }    

    @Test
    public void testSingleOps() throws Exception {
    	
        String key = "SingleOpsTest";
        Random prng = new Random();

        // Set a string value
        {
            String column = "StringColumn";
            String value = RandomStringUtils.randomAlphanumeric(32);
            // Set
            keyspace.prepareColumnMutation(CF_STANDARD1, key, column)
                    .putValue(value, null).execute();
            
            // Read
            ColumnQuery<String> query = keyspace.prepareQuery(CF_STANDARD1).getKey(key).getColumn(column);
            
            String v = query.execute().getResult().getStringValue();
            Assert.assertEquals(value, v);

            // Delete
            keyspace.prepareColumnMutation(CF_STANDARD1, key, column)
                    .deleteColumn().execute();
            try {
                keyspace.prepareQuery(CF_STANDARD1).getKey(key)
                        .getColumn(column).execute().getResult()
                        .getStringValue();
                Assert.fail();
            } catch (RuntimeException e) {
            } catch (NotFoundException e) {
            } catch (ConnectionException e) {
            	e.printStackTrace();
                Assert.fail();
            }
        } 
        
        // Set a byte value
        {
            String column = "ByteColumn";
            byte value = (byte) prng.nextInt(Byte.MAX_VALUE);
            // Set
            keyspace.prepareColumnMutation(CF_STANDARD1, key, column)
                    .putValue(value, null).execute();
            // Read
            byte v = keyspace.prepareQuery(CF_STANDARD1).getKey(key)
                    .getColumn(column).execute().getResult().getByteValue();
            Assert.assertEquals(value, v);
            // Delete
            keyspace.prepareColumnMutation(CF_STANDARD1, key, column)
                    .deleteColumn().execute();
            // verify column gone
            try {
                keyspace.prepareQuery(CF_STANDARD1).getKey(key)
                        .getColumn(column).execute().getResult().getByteValue();
                Assert.fail();
            } catch (NullPointerException e) {
            } catch (NotFoundException e) {
            	// expected
            }
        } 
        
        // Set a short value
        {
            String column = "ShortColumn";
            short value = (short) prng.nextInt(Short.MAX_VALUE);
            // Set
            keyspace.prepareColumnMutation(CF_STANDARD1, key, column)
                    .putValue(value, null).execute();
            
            // Read
            short v = keyspace.prepareQuery(CF_STANDARD1).getKey(key)
                    .getColumn(column).execute().getResult().getShortValue();
            Assert.assertEquals(value, v);
            // Delete
            keyspace.prepareColumnMutation(CF_STANDARD1, key, column)
                    .deleteColumn().execute();
            // verify column gone
            try {
            	
                keyspace.prepareQuery(CF_STANDARD1).getKey(key)
                        .getColumn(column).execute().getResult().getShortValue();
                Assert.fail();
            } catch (NullPointerException e) {
            	// expected
            } catch (NotFoundException e) {
            	// expected
            }
        } 
        
        // Set a int value
        {
            String column = "IntColumn";
            int value = prng.nextInt();
            // Set
            keyspace.prepareColumnMutation(CF_STANDARD1, key, column)
                    .putValue(value, null).execute();
            // Read
            int v = keyspace.prepareQuery(CF_STANDARD1).getKey(key)
                    .getColumn(column).execute().getResult().getIntegerValue();
            Assert.assertEquals(value, v);
            // Delete
            keyspace.prepareColumnMutation(CF_STANDARD1, key, column)
                    .deleteColumn().execute();
            // verify column gone
            try {
                keyspace.prepareQuery(CF_STANDARD1).getKey(key)
                        .getColumn(column).execute().getResult().getIntegerValue();
                Assert.fail();
            } catch (NullPointerException e) {
            	// expected
            } catch (NotFoundException e) {
            	// expected
            }
        }
        
        // Set a long value
        {
            String column = "LongColumn";
            long value = prng.nextLong();
            // Set
            keyspace.prepareColumnMutation(CF_STANDARD1, key, column)
                    .putValue(value, null).execute();
            // Read
            long v = keyspace.prepareQuery(CF_STANDARD1).getKey(key)
                    .getColumn(column).execute().getResult().getLongValue();
            Assert.assertEquals(value, v);
         // get as integer should fail
            try {
                keyspace.prepareQuery(CF_STANDARD1).getKey(key)
                        .getColumn(column).execute().getResult()
                        .getIntegerValue();
                Assert.fail();
            } catch (Exception e) {
            	// expected
            }
            // Delete
            keyspace.prepareColumnMutation(CF_STANDARD1, key, column)
                    .deleteColumn().execute();
            // verify column gone
            try {
                keyspace.prepareQuery(CF_STANDARD1).getKey(key)
                        .getColumn(column).execute().getResult().getLongValue();
                Assert.fail();
            } catch (NullPointerException e) {
            	// expected
            } catch (NotFoundException e) {
            	// expected
            }
        }
        
        // Set a float value
        {
            String column = "FloatColumn";
            float value = prng.nextFloat();
            // Set
            keyspace.prepareColumnMutation(CF_STANDARD1, key, column)
                    .putValue(value, null).execute();
            // Read
            float v = keyspace.prepareQuery(CF_STANDARD1).getKey(key)
                    .getColumn(column).execute().getResult().getFloatValue();
            Assert.assertEquals(value, v);
            // Delete
            keyspace.prepareColumnMutation(CF_STANDARD1, key, column)
                    .deleteColumn().execute();
            // verify column gone
            try {
                keyspace.prepareQuery(CF_STANDARD1).getKey(key)
                        .getColumn(column).execute().getResult().getFloatValue();
                Assert.fail();
            } catch (NullPointerException e) {
            	// expected
            } catch (NotFoundException e) {
            	// expected
            }
        }

        // Set a double value
        {
            String column = "DoubleColumn";
            double value = prng.nextDouble();
            // Set
            keyspace.prepareColumnMutation(CF_STANDARD1, key, column)
                    .putValue(value, null).execute();
            // Read
            double v = keyspace.prepareQuery(CF_STANDARD1).getKey(key)
                    .getColumn(column).execute().getResult().getDoubleValue();
            Assert.assertEquals(value, v);
            // get as integer should fail
            try {
                keyspace.prepareQuery(CF_STANDARD1).getKey(key)
                        .getColumn(column).execute().getResult()
                        .getIntegerValue();
                Assert.fail();
            } catch (Exception e) {
            	// expected
            }
            // Delete
            keyspace.prepareColumnMutation(CF_STANDARD1, key, column)
                    .deleteColumn().execute();
            try {
                keyspace.prepareQuery(CF_STANDARD1).getKey(key)
                        .getColumn(column).execute().getResult()
                        .getDoubleValue();
                Assert.fail();
            } catch (NullPointerException e) {
            	// expected
            } catch (NotFoundException e) {
            } catch (ConnectionException e) {
                Assert.fail();
            }
        } 
        
        // Set long column with timestamp
        {
            String column = "TimestampColumn";
            long value = prng.nextLong();

            // Set
            keyspace.prepareColumnMutation(CF_STANDARD1, key, column)
                    .withTimestamp(100)
                    .putValue(value, null)
                    .execute();

            // Read
            Column<String> c = keyspace.prepareQuery(CF_STANDARD1).getKey(key)
                    .getColumn(column).execute().getResult();
            Assert.assertEquals(100,  c.getTimestamp());
        }
    }
    
    
    @Test
    public void testEmptyColumn() {
        ColumnListMutation<String> mutation = keyspace.prepareMutationBatch().withRow(CF_STANDARD1, "ABC");
        
        try {
            mutation.putColumn(null,  1L);
            Assert.fail();
        }
        catch (Exception e) {
            LOG.info(e.getMessage());
        }
        
        try {
            mutation.putColumn("",  1L);
            Assert.fail();
        }
        catch (Exception e) {
            LOG.info(e.getMessage());
        }

        try {
            mutation.deleteColumn("");
            Assert.fail();
        }
        catch (Exception e) {
            LOG.info(e.getMessage());
        }
        
        try {
            mutation.deleteColumn(null);
            Assert.fail();
        }
        catch (Exception e) {
            LOG.info(e.getMessage());
        }
    }

    @Test
    public void testCqlCount() throws Exception {
    	LOG.info("CQL Test");
    	OperationResult<CqlResult<String, String>> result = keyspace
    			.prepareQuery(CF_STANDARD1)
    			.withCql("SELECT count(*) FROM astyanaxunittests.standard1 where KEY='A';")
    			.execute();

    	long count = result.getResult().getNumber();
    	LOG.info("CQL Count: " + count);
    	Assert.assertTrue(0 <= count);
    }

    @Test
    public void testGetSingleColumn() throws Exception {
    	
    	keyspace.prepareColumnMutation(CF_STANDARD1, "A", "a").putValue(1, null).execute();
        Column<String> column = keyspace.prepareQuery(CF_STANDARD1).getRow("A").getColumn("a").execute().getResult();
        Assert.assertNotNull(column);
        Assert.assertEquals(1, column.getIntegerValue());
    }

    @Test
    public void testGetSingleKeyNotExists() throws Exception {
        Column<String> column = keyspace.prepareQuery(CF_STANDARD1).getRow("AA").getColumn("ab").execute().getResult();
        Assert.assertNull(column);
    }
    
    @Test
    public void testFunctionalQuery() throws Exception {
    	MutationBatch m = keyspace.prepareMutationBatch();

        for (char keyName = 'A'; keyName <= 'Z'; keyName++) {
            String rowKey = Character.toString(keyName);
            ColumnListMutation<String> cfmStandard = m.withRow(CF_STANDARD1, rowKey);
            for (char cName = 'a'; cName <= 'z'; cName++) {
                cfmStandard.putColumn(Character.toString(cName),
                        (int) (cName - 'a') + 1, null);
            }
            m.withCaching(true);
            m.execute();
            m.discardMutations();
        }
        
        OperationResult<ColumnList<String>> r1 = keyspace
                .prepareQuery(CF_STANDARD1).getKey("A").execute();
        Assert.assertTrue(26 <= r1.getResult().size());
    }
    
    @Test
    public void testNullKeyInMutation() throws Exception {
        try {
            keyspace.prepareMutationBatch().withRow(CF_STANDARD1,  null).putColumn("abc", "def");
            Assert.fail();
        }
        catch (Exception e) {
        }
    }
    
    @Test
    public void testColumnSlice() throws ConnectionException {
        OperationResult<ColumnList<String>> r1 = keyspace
                .prepareQuery(CF_STANDARD1).getKey("A")
                .withColumnSlice("a", "b").execute();
        Assert.assertEquals(2, r1.getResult().size());
    }
    
    @Test
    public void testColumnRangeSlice() throws ConnectionException {
        OperationResult<ColumnList<String>> r1 = keyspace
                .prepareQuery(CF_STANDARD1)
                .getKey("A")
                .withColumnRange(
                        new RangeBuilder().setStart("a").setEnd("b")
                                .setLimit(5).build()).execute();
        Assert.assertEquals(2, r1.getResult().size());

        OperationResult<ColumnList<String>> r2 = keyspace
                .prepareQuery(CF_STANDARD1).getKey("A")
                .withColumnRange("a", null, false, 5).execute();
        Assert.assertEquals(5, r2.getResult().size());
        Assert.assertEquals("a", r2.getResult().getColumnByIndex(0).getName());

        ByteBuffer EMPTY_BUFFER = ByteBuffer.wrap(new byte[0]);
        OperationResult<ColumnList<String>> r3 = keyspace
                .prepareQuery(CF_STANDARD1).getKey("A")
                .withColumnRange(EMPTY_BUFFER, EMPTY_BUFFER, true, 5).execute();
        Assert.assertEquals(5, r3.getResult().size());
        Assert.assertEquals("z", r3.getResult().getColumnByIndex(0).getName());
    }
    

    @Test
    public void testGetSingleColumnNotExists() throws ConnectionException {
        Column<String> column = keyspace.prepareQuery(CF_STANDARD1).getRow("A").getColumn("DoesNotExist").execute().getResult();
        Assert.assertNull(column);
    }
    
    @Test
    public void testGetSingleColumnNotExistsAsync() {
        Future<OperationResult<Column<String>>> future = null;
        try {
            future = keyspace.prepareQuery(CF_STANDARD1).getKey("A")
                    .getColumn("DoesNotExist").executeAsync();
            future.get(1000, TimeUnit.MILLISECONDS);
        } catch (ConnectionException e) {
            LOG.info("ConnectionException: " + e.getMessage());
            Assert.fail();
        } catch (InterruptedException e) {
            LOG.info(e.getMessage());
            Assert.fail();
        } catch (ExecutionException e) {
            if (e.getCause() instanceof NotFoundException)
                LOG.info(e.getCause().getMessage());
            else {
                Assert.fail(e.getMessage());
            }
        } catch (TimeoutException e) {
            future.cancel(true);
            LOG.info(e.getMessage());
            Assert.fail();
        }
    }

    @Test
    public void testGetSingleKey() throws ConnectionException {
    	for (char key = 'A'; key <= 'Z'; key++) {
    		String keyName = Character.toString(key);
    		OperationResult<ColumnList<String>> result = keyspace.prepareQuery(CF_STANDARD1).getKey(keyName).execute();
    		Assert.assertNotNull(result.getResult());
    		Assert.assertFalse(result.getResult().isEmpty());
    	}
    }
    
    @Test
    public void testGetSingleKeyAsync() throws Exception {
    	Future<OperationResult<ColumnList<String>>> future = keyspace.prepareQuery(CF_STANDARD1).getKey("A").executeAsync();

    	ColumnList<String> result = future.get(1000, TimeUnit.MILLISECONDS).getResult();
    	Assert.assertFalse(result.isEmpty());
    }

    @Test
    public void testGetAllKeysRoot() throws ConnectionException {
    	LOG.info("Starting testGetAllKeysRoot...");

    	List<String> keys = new ArrayList<String>();
    	for (char key = 'A'; key <= 'Z'; key++) {
    		String keyName = Character.toString(key);
    		keys.add(keyName);
    	}

    	OperationResult<Rows<String, String>> result = keyspace
    			.prepareQuery(CF_STANDARD1)
    			.getKeySlice(keys.toArray(new String[keys.size()]))
    			.execute();

    	Assert.assertEquals(26,  result.getResult().size());

    	Row<String, String> row;

    	row = result.getResult().getRow("A");
    	Assert.assertEquals("A", row.getKey());

    	row = result.getResult().getRow("B");
    	Assert.assertEquals("B", row.getKey());

    	row = result.getResult().getRow("NonExistent");
    	Assert.assertNull(row);

    	row = result.getResult().getRowByIndex(10);
    	Assert.assertEquals("K", row.getKey());

    	LOG.info("... testGetAllKeysRoot");
    }
    
    @Test
    public void testEmptyRowKey() {
        try {
            keyspace.prepareMutationBatch().withRow(CF_STANDARD1, "");
            Assert.fail();
        }
        catch (Exception e) {
            LOG.info(e.getMessage());
        }
        
        try {
            keyspace.prepareMutationBatch().withRow(CF_STANDARD1, null);
            Assert.fail();
        }
        catch (Exception e) {
            LOG.info(e.getMessage());
        }
    }

    @Test
    public void testGetColumnSlice() throws ConnectionException {
    	LOG.info("Starting testGetColumnSlice...");
    	OperationResult<ColumnList<String>> result = keyspace
    			.prepareQuery(CF_STANDARD1)
    			.getKey("A")
    			.withColumnSlice(
    					new ColumnSlice<String>("c", "h").setLimit(5))
    					.execute();
    	Assert.assertNotNull(result.getResult());
    	Assert.assertEquals(5, result.getResult().size());
    }


    @Test
    public void testGetAllKeysPath() throws ConnectionException {
    	LOG.info("Starting testGetAllKeysPath...");

    	List<String> keys = new ArrayList<String>();
    	for (char key = 'A'; key <= 'Z'; key++) {
    		String keyName = Character.toString(key);
    		keys.add(keyName);
    	}

    	OperationResult<Rows<String, String>> result = keyspace
    			.prepareQuery(CF_STANDARD1)
    			.getKeySlice(keys.toArray(new String[keys.size()]))
    			.execute();

    	for (Row<String, String> row : result.getResult()) {
    		System.out.println(row.getColumns().size());
    	}

    	OperationResult<Map<String, Integer>> counts = keyspace
    			.prepareQuery(CF_STANDARD1)
    			.getKeySlice(keys.toArray(new String[keys.size()]))
    			.getColumnCounts()
    			.execute();

    	Assert.assertEquals(26, counts.getResult().size());

    	for (Entry<String, Integer> count : counts.getResult().entrySet()) {
    		Assert.assertEquals(new Integer(26), count.getValue());
    	}

    	LOG.info("Starting testGetAllKeysPath...");
    }

    public static class UserInfo implements Serializable {
        private static final long serialVersionUID = 6366200973810770033L;

        private String firstName;
        private String lastName;

        public UserInfo() {

        }

        public void setFirstName(String firstName) {
            this.firstName = firstName;
        }

        public String getFirstName() {
            return this.firstName;
        }

        public void setLastName(String lastName) {
            this.lastName = lastName;
        }

        public String getLastName() {
            return this.lastName;
        }

        public boolean equals(Object other) {
            UserInfo smo = (UserInfo) other;
            return firstName.equals(smo.firstName)
                    && lastName.equals(smo.lastName);
        }
    }

    @Test
    public void testHasValue() throws Exception {

    	MutationBatch m = keyspace.prepareMutationBatch();
    	m.withRow(CF_USER_INFO, "acct1234")
    	.putColumn("firstname", "john", null)
    	.putColumn("lastname", "smith", null)
    	.putColumn("address", "555 Elm St", null)
    	.putColumn("age", 30, null)
    	.putEmptyColumn("empty");

    	m.execute();
    	ColumnList<String> response = keyspace.prepareQuery(CF_USER_INFO).getRow("acct1234").execute().getResult();
    	Assert.assertEquals("firstname", response.getColumnByName("firstname").getName());
    	Assert.assertEquals("firstname", response.getColumnByName("firstname").getName());
    	Assert.assertEquals("john", response.getColumnByName("firstname").getStringValue());
    	Assert.assertEquals("john", response.getColumnByName("firstname").getStringValue());
    	Assert.assertEquals(true,  response.getColumnByName("firstname").hasValue());
    	Assert.assertEquals(false, response.getColumnByName("empty").hasValue());
    }

    @Test
    public void testDelete() throws Exception {
        LOG.info("Starting testDelete...");

        String rowKey = "DeleteMe_testDelete";

        MutationBatch m = keyspace.prepareMutationBatch();
        m.withRow(CF_STANDARD1, rowKey).putColumn("Column1", "X", null).putColumn("Column2", "X", null);
        m.execute();

        Column<String> column = keyspace.prepareQuery(CF_STANDARD1).getRow(rowKey).getColumn("Column1").execute().getResult();
        Assert.assertEquals("X", column.getStringValue());
        
        m = keyspace.prepareMutationBatch();
        m.withRow(CF_STANDARD1, rowKey).deleteColumn("Column1");
        m.execute();

        column = keyspace.prepareQuery(CF_STANDARD1).getRow(rowKey).getColumn("Column1").execute().getResult();
        Assert.assertNull(column);

        LOG.info("... testDelete");
    }

    @Test
    public void testDeleteLotsOfColumns() throws Exception {
        LOG.info("Starting testDelete...");

        String rowKey = "DeleteMe_testDeleteLotsOfColumns";

        int nColumns = 100;
        int pageSize = 25;

        // Insert a bunch of rows
        MutationBatch m = keyspace.prepareMutationBatch();
        ColumnListMutation<String> rm = m.withRow(CF_STANDARD1, rowKey);

        for (int i = 0; i < nColumns; i++) {
            rm.putEmptyColumn("" + i, null);
        }

        m.execute();
        
        // Verify count
        int count = keyspace.prepareQuery(CF_STANDARD1)
                    .setConsistencyLevel(ConsistencyLevel.CL_QUORUM)
                    .getKey(rowKey).getCount().execute().getResult();
        Assert.assertEquals(nColumns, count);

        // Delete half of the columns
        m = keyspace.prepareMutationBatch().setConsistencyLevel(ConsistencyLevel.CL_QUORUM);
        rm = m.withRow(CF_STANDARD1, rowKey);

        for (int i = 0; i < nColumns / 2; i++) {
            rm.deleteColumn("" + i);
        }

        m.execute();
        
        // Verify count
        count = keyspace.prepareQuery(CF_STANDARD1)
                .setConsistencyLevel(ConsistencyLevel.CL_QUORUM)
                .getKey(rowKey).getCount().execute().getResult();
        Assert.assertEquals(nColumns / 2, count);
        
        // GET ROW COUNT WITH PAGINATION
        RowQuery<String, String> query = keyspace.prepareQuery(CF_STANDARD1)
                .setConsistencyLevel(ConsistencyLevel.CL_QUORUM).getKey(rowKey)
                .withColumnRange(new RangeBuilder().setLimit(pageSize).build())
                .autoPaginate(true);

        ColumnList<String> result;
        count = 0;
        while (!(result = query.execute().getResult()).isEmpty()) {
            count += result.size();
        }

        Assert.assertEquals(nColumns / 2, count);

        // Delete all of the columns
        m = keyspace.prepareMutationBatch().setConsistencyLevel(ConsistencyLevel.CL_QUORUM);
        rm = m.withRow(CF_STANDARD1, rowKey);

        for (int i = 0; i < nColumns; i++) {
            rm.deleteColumn("" + i);
        }

        m.execute();
        
        // Verify count
        count = keyspace.prepareQuery(CF_STANDARD1)
                .setConsistencyLevel(ConsistencyLevel.CL_QUORUM)
                .getKey(rowKey).getCount().execute().getResult();
        Assert.assertEquals(0, count);

        LOG.info("... testDelete");
    }
    
    @Test
    public void testCopy() throws ConnectionException {
    	
        String keyName = "A";

        keyspace.prepareQuery(CF_STANDARD1).getKey(keyName).copyTo(CF_STANDARD2, keyName).execute();

        ColumnList<String> list1 = keyspace.prepareQuery(CF_STANDARD1).getKey(keyName).execute().getResult();
        ColumnList<String> list2 = keyspace.prepareQuery(CF_STANDARD2).getKey(keyName).execute().getResult();

        Iterator<Column<String>> iter1 = list1.iterator();
        Iterator<Column<String>> iter2 = list2.iterator();

        while (iter1.hasNext()) {
        	Column<String> column1 = iter1.next();
        	Column<String> column2 = iter2.next();
        	Assert.assertEquals(column1.getName(), column2.getName());
        	Assert.assertEquals(column1.getByteBufferValue(),column2.getByteBufferValue());
        }
        Assert.assertFalse(iter2.hasNext());
    }
    
    @Test
    public void testMutationMerge() throws Exception {
        MutationBatch m1 = keyspace.prepareMutationBatch();
        MutationBatch m2 = keyspace.prepareMutationBatch();
        MutationBatch m3 = keyspace.prepareMutationBatch();
        MutationBatch m4 = keyspace.prepareMutationBatch();
        MutationBatch m5 = keyspace.prepareMutationBatch();

        m1.withRow(CF_STANDARD1, "1").putColumn("1", "X", null);
        m2.withRow(CF_STANDARD1, "2").putColumn("2", "X", null).putColumn("3", "X", null);
        m3.withRow(CF_STANDARD1, "3").putColumn("4", "X", null).putColumn("5", "X", null).putColumn("6", "X", null);
        m4.withRow(CF_STANDARD1, "1").putColumn("7", "X", null).putColumn("8", "X", null).putColumn("9", "X", null).putColumn("10", "X", null);

        MutationBatch merged = keyspace.prepareMutationBatch();

        Assert.assertEquals(merged.getRowCount(), 0);

        merged.mergeShallow(m1);
        Assert.assertEquals(merged.getRowCount(), 1);

        merged.mergeShallow(m2);
        Assert.assertEquals(merged.getRowCount(), 2);

        merged.mergeShallow(m3);
        Assert.assertEquals(merged.getRowCount(), 3);

        merged.mergeShallow(m4);
        Assert.assertEquals(merged.getRowCount(), 3);

        merged.mergeShallow(m5);
        Assert.assertEquals(merged.getRowCount(), 3);
        
        merged.execute();
        
        Rows<String, String> result = keyspace.prepareQuery(CF_STANDARD1).getRowSlice("1", "2", "3").execute().getResult();
        
        Assert.assertTrue(5 == result.getRow("1").getColumns().size());
        Assert.assertTrue(2 == result.getRow("2").getColumns().size());
        Assert.assertTrue(3 == result.getRow("3").getColumns().size());
    }

}>>>>>>> YOURS
/home/ramdisk/experiment3/projects/astyanax/revisions/rev_2b15fd9_3de01e3/rev_2b15fd9-3de01e3/astyanax-test/src/main/java/com/netflix/astyanax/cql/test/CompositeKeyTests.java;<<<<<<< MINE
=======
package com.netflix.astyanax.cql.test;

import junit.framework.Assert;

import org.apache.log4j.Logger;
import org.junit.AfterClass;
import org.junit.BeforeClass;
import org.junit.Test;

import com.google.common.collect.ImmutableMap;
import com.netflix.astyanax.MutationBatch;
import com.netflix.astyanax.connectionpool.exceptions.ConnectionException;
import com.netflix.astyanax.cql.test.MockCompositeTypeTests.MockCompositeType;
import com.netflix.astyanax.model.ColumnFamily;
import com.netflix.astyanax.model.ColumnList;
import com.netflix.astyanax.serializers.AnnotatedCompositeSerializer;
import com.netflix.astyanax.serializers.StringSerializer;

public class CompositeKeyTests extends KeyspaceTests {

	private static final Logger LOG = Logger.getLogger(CompositeKeyTests.class);
	
	@BeforeClass
	public static void init() throws Exception {
		initContext();
		
		keyspace.createColumnFamily(CF_COMPOSITE_KEY, ImmutableMap.<String, Object>builder()
				.put("key_validation_class", "BytesType")
				.build());
		 
        CF_COMPOSITE_KEY.describe(keyspace);
	}
	
	@AfterClass
	public static void tearDown() throws Exception {
		keyspace.dropColumnFamily(CF_COMPOSITE_KEY);
	}

	private static AnnotatedCompositeSerializer<MockCompositeType> M_SERIALIZER 
    	= new AnnotatedCompositeSerializer<MockCompositeType>(MockCompositeType.class);

    private static ColumnFamily<MockCompositeType, String> CF_COMPOSITE_KEY 
    	= ColumnFamily.newColumnFamily("compositekey", M_SERIALIZER, StringSerializer.get());
	
    @Test
    public void testCompositeKey() {
        MockCompositeType key = new MockCompositeType("A", 1, 2, true, "B");
        MutationBatch m = keyspace.prepareMutationBatch();
        m.withRow(CF_COMPOSITE_KEY, key).putColumn("Test", "Value", null);
        try {
            m.execute();
        } catch (ConnectionException e) {
            LOG.error(e.getMessage(), e);
            Assert.fail();
        }

        try {
            ColumnList<String> row = keyspace.prepareQuery(CF_COMPOSITE_KEY)
                    .getKey(key).execute().getResult();
            Assert.assertFalse(row.isEmpty());
        } catch (ConnectionException e) {
            LOG.error(e.getMessage(), e);
            Assert.fail();
        }
    }
}>>>>>>> YOURS
/home/ramdisk/experiment3/projects/astyanax/revisions/rev_2b15fd9_3de01e3/rev_2b15fd9-3de01e3/astyanax-test/src/main/java/com/netflix/astyanax/cql/test/RowSliceRowRangeQueryTests.java;<<<<<<< MINE
=======
package com.netflix.astyanax.cql.test;

import java.util.ArrayList;
import java.util.Collections;
import java.util.HashSet;
import java.util.Iterator;
import java.util.List;
import java.util.Random;
import java.util.Set;

import junit.framework.Assert;

import org.junit.AfterClass;
import org.junit.BeforeClass;
import org.junit.Test;

import com.netflix.astyanax.cql.reads.model.CqlRangeBuilder;
import com.netflix.astyanax.cql.reads.model.CqlRangeImpl;
import com.netflix.astyanax.cql.test.utils.ReadTests;
import com.netflix.astyanax.cql.test.utils.TestUtils;
import com.netflix.astyanax.cql.test.utils.TestUtils.TestTokenRange;
import com.netflix.astyanax.model.ByteBufferRange;
import com.netflix.astyanax.model.Column;
import com.netflix.astyanax.model.ColumnFamily;
import com.netflix.astyanax.model.ColumnList;
import com.netflix.astyanax.model.Row;
import com.netflix.astyanax.model.Rows;


public class RowSliceRowRangeQueryTests extends ReadTests {

	private static ColumnFamily<String, String> CF_COLUMN_RANGE_TEST = TestUtils.CF_COLUMN_RANGE_TEST;

	@BeforeClass
	public static void init() throws Exception {
		initContext();
		keyspace.createColumnFamily(CF_COLUMN_RANGE_TEST, null);
		CF_COLUMN_RANGE_TEST.describe(keyspace);
	}
	
	@AfterClass
	public static void tearDown() throws Exception {
		keyspace.dropColumnFamily(CF_COLUMN_RANGE_TEST);
	}

	@Test
	public void runAllTests() throws Exception {
		
		boolean rowDeleted = false; 
		
		TestUtils.populateRowsForColumnRange(keyspace);
		Thread.sleep(1000);
		
		testRowKeysWithAllColumns(rowDeleted);
		testRowKeysWithColumnSet(rowDeleted);
		testRowKeysWithColumnRange(rowDeleted);
		
		testRowRangeWithAllColumns(rowDeleted);
		testRowRangeWithColumnSet(rowDeleted);
		testRowRangeWithColumnRange(rowDeleted);
		
		TestUtils.deleteRowsForColumnRange(keyspace);
		Thread.sleep(1000);
		rowDeleted = true; 
		
		testRowKeysWithAllColumns(rowDeleted);
		testRowKeysWithColumnSet(rowDeleted);
		testRowKeysWithColumnRange(rowDeleted);
		
		testRowRangeWithAllColumns(rowDeleted);
		testRowRangeWithColumnSet(rowDeleted);
		testRowRangeWithColumnRange(rowDeleted);
	}
	
	private void testRowKeysWithAllColumns(boolean rowDeleted) throws Exception {
		
		Set<String> rowKeys = getRandomRowKeys();
		
		Rows<String, String> rows = keyspace.prepareQuery(CF_COLUMN_RANGE_TEST).getRowSlice(rowKeys).execute().getResult();
    	
		if (rowDeleted) {
			Assert.assertTrue(rows.isEmpty());
			return;
		}
		
    	Assert.assertFalse(rows.isEmpty());

    	int rowKeysSize = rowKeys.size();
    	for (Row<String, String> row : rows) {
    		
    		boolean isPresent = rowKeys.remove(row.getKey());
    		Assert.assertTrue("Extraneous row: " + row.getKey(), isPresent);
    		
        	ColumnList<String> colList = row.getColumns();
        	Assert.assertEquals(26, colList.size());
        	for(int index=0; index<26; index++) { 
        		Column<String> col = colList.getColumnByIndex(index);
        		Assert.assertTrue(String.valueOf((char)('a' + index)).equals(col.getName()));
        		Assert.assertEquals(index + 1, col.getIntegerValue());
        	}
    	}
    	Assert.assertEquals(rowKeysSize, rows.size());
	}
	

	private void testRowKeysWithColumnSet(boolean rowDeleted) throws Exception {
		
		Set<String> rowKeys = getRandomRowKeys();
		Set<String> columns = getRandomColumns();
		
		Rows<String, String> rows = keyspace.prepareQuery(CF_COLUMN_RANGE_TEST)
											.getRowSlice(rowKeys)
											.withColumnSlice(columns)
											.execute().getResult();
		
		if (rowDeleted) {
			Assert.assertTrue(rows.isEmpty());
			return;
		}
		
    	Assert.assertFalse(rows.isEmpty());

    	List<String> expected = new ArrayList<String>(columns);
    	Collections.sort(expected);
    
    	int rowKeysSize = rowKeys.size();
    	for (Row<String, String> row : rows) {
    		
    		boolean isPresent = rowKeys.remove(row.getKey());
    		Assert.assertTrue("Extraneous row: " + row.getKey(), isPresent);
    		
        	List<String> result = new ArrayList<String>();
        	ColumnList<String> colList = row.getColumns();
        	for (Column<String> col : colList) {
        		result.add(col.getName());
        	}
        	Collections.sort(result);
        	
        	Assert.assertEquals(expected, result);
    	}
    	Assert.assertEquals(rowKeysSize, rows.size());
	}
	
	@SuppressWarnings("unchecked")
	private void testRowKeysWithColumnRange(boolean rowDeleted) throws Exception {
		
		Set<String> rowKeys = getRandomRowKeys();
		
		// get random start and end column
		CqlRangeImpl<String> columns = (CqlRangeImpl<String>) getRandomColumnRange();
		
		Rows<String, String> rows = keyspace.prepareQuery(CF_COLUMN_RANGE_TEST)
											.getRowSlice(rowKeys)
											.withColumnRange(columns)
											.execute().getResult();
		if (rowDeleted) {
			Assert.assertTrue(rows.isEmpty());
			return;
		}
		
    	Assert.assertFalse(rows.isEmpty());

    	int rowKeysSize = rowKeys.size();
    	
    	for (Row<String, String> row : rows) {
    		
    		boolean isPresent = rowKeys.remove(row.getKey());
    		Assert.assertTrue("Extraneous row: " + row.getKey(), isPresent);
    		
    		int numExpectedCols = columns.getCqlEnd().charAt(0) - columns.getCqlStart().charAt(0) + 1;
    		
        	ColumnList<String> colList = row.getColumns();
        	Assert.assertEquals(numExpectedCols, colList.size());

        	for (Column<String> col : colList) {
        		Assert.assertTrue(col.getName().compareTo(columns.getCqlStart()) >= 0);
        		Assert.assertTrue(col.getName().compareTo(columns.getCqlEnd()) <= 0);
        	}
    	}
    	Assert.assertEquals(rowKeysSize, rows.size());
	}

	
	private void testRowRangeWithAllColumns(boolean rowDeleted) throws Exception {

		List<String> expectedColumns = new ArrayList<String>();
		for (char ch = 'a'; ch <= 'z'; ch++) {
			expectedColumns.add(String.valueOf(ch));
		}
		
		for (TestTokenRange testRange : getTestTokenRanges()) {
		
			Rows<String, String> rows = keyspace.prepareQuery(CF_COLUMN_RANGE_TEST)
					.getRowRange(null, null, testRange.startToken, testRange.endToken, -1)
					.execute().getResult();

			if (rowDeleted) {
				Assert.assertTrue(rows.isEmpty());
				continue;
			}

			Assert.assertFalse(rows.isEmpty());
			
			List<String> list = new ArrayList<String>();
			for (Row<String, String> row : rows) {
				String key = row.getKey();
				list.add(key);
				
				ColumnList<String> columns = row.getColumns();
				testRangeColumnsForRow(columns, expectedColumns);
			}
			
			Assert.assertEquals(testRange.expectedRowKeys, list);
		}
	}

	private void testRowRangeWithColumnSet(boolean rowDeleted) throws Exception {

		Set<String> randomColumns = getRandomColumns();
		
		List<String> expectedColumns = new ArrayList<String>(randomColumns);
		Collections.sort(expectedColumns);
		
		for (TestTokenRange testRange : getTestTokenRanges()) {
		
			Rows<String, String> rows = keyspace.prepareQuery(CF_COLUMN_RANGE_TEST)
					.getRowRange(null, null, testRange.startToken, testRange.endToken, -1)
					.withColumnSlice(randomColumns)
					.execute().getResult();

			if (rowDeleted) {
				Assert.assertTrue(rows.isEmpty());
				continue;
			}

			Assert.assertFalse(rows.isEmpty());
			
			List<String> list = new ArrayList<String>();
			for (Row<String, String> row : rows) {
				String key = row.getKey();
				list.add(key);
				
				ColumnList<String> columns = row.getColumns();
				testRangeColumnsForRow(columns, expectedColumns);
			}
			
			Assert.assertEquals(testRange.expectedRowKeys, list);
		}
	}
	
	private void testRowRangeWithColumnRange(boolean rowDeleted) throws Exception {

		CqlRangeImpl<String> columnRange = (CqlRangeImpl<String>) getRandomColumnRange();
		
		for (TestTokenRange testRange : getTestTokenRanges()) {
		
			Rows<String, String> rows = keyspace.prepareQuery(CF_COLUMN_RANGE_TEST)
					.getRowRange(null, null, testRange.startToken, testRange.endToken, -1)
					.withColumnRange(columnRange)
					.execute().getResult();

			if (rowDeleted) {
				Assert.assertTrue(rows.isEmpty());
				continue;
			}

			Assert.assertFalse(rows.isEmpty());
			
    		int numExpectedCols = columnRange.getCqlEnd().charAt(0) - columnRange.getCqlStart().charAt(0) + 1;
    		

			List<String> list = new ArrayList<String>();
			for (Row<String, String> row : rows) {
				String key = row.getKey();
				list.add(key);
				
	        	ColumnList<String> colList = row.getColumns();
	        	Assert.assertEquals(numExpectedCols, colList.size());

	        	for (Column<String> col : colList) {
	        		Assert.assertTrue(col.getName().compareTo(columnRange.getCqlStart()) >= 0);
	        		Assert.assertTrue(col.getName().compareTo(columnRange.getCqlEnd()) <= 0);
	        	}
			}
			
			Assert.assertEquals(testRange.expectedRowKeys, list);
		}
	}
	
	private Set<String> getRandomRowKeys() {
		
		Random random = new Random();
		int numRowKeys = random.nextInt(26) + 1;  // avoid 0 rows
		
		Set<String> set = new HashSet<String>();
		for (int i=0; i<numRowKeys; i++) {
			
			int no = random.nextInt(26);
			char ch = (char) ('A' + no);
			set.add(String.valueOf(ch));
		}
		System.out.println("Set: " + set);
		return set;
	}
	
	private Set<String> getRandomColumns() {
		
		Random random = new Random();
		int numRowKeys = random.nextInt(26) + 1;  // avoid 0 rows
		
		Set<String> set = new HashSet<String>();
		for (int i=0; i<numRowKeys; i++) {
			
			int no = random.nextInt(26);
			char ch = (char) ('a' + no);
			set.add(String.valueOf(ch));
		}
		
		return set;
	}
	
	private ByteBufferRange getRandomColumnRange() {
		
		Random random = new Random();
		Integer n1 = random.nextInt(26);
		Integer n2 = random.nextInt(26);
		
		String c1 = String.valueOf((char)('a' + n1));
		String c2 = String.valueOf((char)('a' + n2));
		
		if (n1 < n2) {
			return new CqlRangeBuilder<String>().setStart(c1).setEnd(c2).build();
		} else {
			return new CqlRangeBuilder<String>().setStart(c2).setEnd(c1).build();
		}
	}
	
	private List<TestTokenRange> getTestTokenRanges() {
		return TestUtils.getTestTokenRanges();
	}
	
	private void testRangeColumnsForRow(ColumnList<String> columns, List<String> expected) {
		
		Iterator<Column<String>> iter1 = columns.iterator();
		Iterator<String> iter2 = expected.iterator();
		while (iter2.hasNext()) {
			
			Column<String> column = iter1.next();
			String expectedName = iter2.next();
			
			Assert.assertEquals(expectedName, column.getName());
			int expectedValue = expectedName.charAt(0) - 'a' + 1;
			Assert.assertEquals(expectedValue, column.getIntegerValue());
		}
	}

	

}>>>>>>> YOURS
/home/ramdisk/experiment3/projects/astyanax/revisions/rev_2b15fd9_3de01e3/rev_2b15fd9-3de01e3/astyanax-test/src/main/java/com/netflix/astyanax/cql/test/TimeUUIDTests.java;<<<<<<< MINE
=======
package com.netflix.astyanax.cql.test;

import java.util.UUID;

import junit.framework.Assert;

import org.apache.log4j.Logger;
import org.junit.AfterClass;
import org.junit.BeforeClass;
import org.junit.Test;

import com.netflix.astyanax.MutationBatch;
import com.netflix.astyanax.connectionpool.OperationResult;
import com.netflix.astyanax.connectionpool.exceptions.ConnectionException;
import com.netflix.astyanax.cql.reads.model.CqlRangeBuilder;
import com.netflix.astyanax.model.Column;
import com.netflix.astyanax.model.ColumnFamily;
import com.netflix.astyanax.model.ColumnList;
import com.netflix.astyanax.query.RowQuery;
import com.netflix.astyanax.serializers.StringSerializer;
import com.netflix.astyanax.serializers.TimeUUIDSerializer;
import com.netflix.astyanax.util.RangeBuilder;
import com.netflix.astyanax.util.TimeUUIDUtils;

public class TimeUUIDTests extends KeyspaceTests {
	
	private static final Logger LOG = Logger.getLogger(TimeUUIDTests.class);
	
    public static ColumnFamily<String, UUID> CF_TIME_UUID = ColumnFamily
            .newColumnFamily(
                    "TimeUUID1", 
                    StringSerializer.get(),
                    TimeUUIDSerializer.get());

    @BeforeClass
	public static void init() throws Exception {
		initContext();
		keyspace.createColumnFamily(CF_TIME_UUID, null);
		CF_TIME_UUID.describe(keyspace);
	}
	
	@AfterClass
	public static void tearDown() throws Exception {
		keyspace.dropColumnFamily(CF_TIME_UUID);
	}

    @Test
    public void testTimeUUID() throws Exception {
    	
    	MutationBatch m = keyspace.prepareMutationBatch();

        UUID columnName = TimeUUIDUtils.getUniqueTimeUUIDinMillis();
        long columnTime = TimeUUIDUtils.getTimeFromUUID(columnName);
        String rowKey = "Key1";

        m.withRow(CF_TIME_UUID, rowKey).delete();
        m.execute();
        m.discardMutations();
        
        int startTime = 100;
        int endTime = 200;

        m.withRow(CF_TIME_UUID, rowKey).putColumn(columnName, 42, null);
        for (int i = startTime; i < endTime; i++) {
            // UUID c = TimeUUIDUtils.getTimeUUID(i);
            LOG.info(TimeUUIDUtils.getTimeUUID(columnTime + i).toString());

            m.withRow(CF_TIME_UUID, rowKey).putColumn(
                    TimeUUIDUtils.getTimeUUID(columnTime + i), i, null);
        }

        m.execute();

        OperationResult<Column<UUID>> result = keyspace
        		.prepareQuery(CF_TIME_UUID).getKey(rowKey)
        		.getColumn(columnName).execute();

        Assert.assertEquals(columnName, result.getResult().getName());
        Assert.assertTrue(result.getResult().getIntegerValue() == 42);

        OperationResult<ColumnList<UUID>> result2 = keyspace.prepareQuery(CF_TIME_UUID).getKey(rowKey).execute();
        Assert.assertTrue(result2.getResult().size() >= (endTime - startTime));

        result2 = keyspace
        		.prepareQuery(CF_TIME_UUID)
        		.getKey(rowKey)
        		.withColumnRange(
        				new RangeBuilder()
        				.setLimit(10)
        				.setStart(TimeUUIDUtils.getTimeUUID(0))
        				.setEnd(TimeUUIDUtils
        						.getTimeUUID(Long.MAX_VALUE >> 8))
        						.build()).execute();

        Assert.assertEquals(10, result2.getResult().size());

        // Test timeUUID pagination
        RowQuery<String, UUID> query = keyspace
                .prepareQuery(CF_TIME_UUID)
                .getKey(rowKey)
                .withColumnRange(
                        new CqlRangeBuilder<UUID>()
                                .setFetchSize(10)
                                .setStart(
                                        TimeUUIDUtils.getTimeUUID(columnTime
                                                + startTime))
                                .setEnd(TimeUUIDUtils.getTimeUUID(columnTime
                                        + endTime)).build()).autoPaginate(true);
        OperationResult<ColumnList<UUID>> result3;
        int pageCount = 0;
        int rowCount = 0;
        try {
            LOG.info("starting pagination");
            while (!(result3 = query.execute()).getResult().isEmpty()) {
                pageCount++;
                Assert.assertTrue(result3.getResult().size() <= 10);
                rowCount += result3.getResult().size();
                LOG.info("==== Block ====");
                for (Column<UUID> column : result3.getResult()) {
                    LOG.info("Column is " + column.getName());
                }
            }
            Assert.assertTrue("pagination complete:  " + pageCount, pageCount >= 10);
            Assert.assertTrue("pagination complete ", rowCount <= 100);
        } catch (ConnectionException e) {
            Assert.fail();
            LOG.info(e.getMessage());
            e.printStackTrace();
        }

    }

    @Test
    public void testTimeUUID2() throws Exception {
    	
    	CF_TIME_UUID.describe(keyspace);
    
        MutationBatch m = keyspace.prepareMutationBatch();
        String rowKey = "Key2";
        m.withRow(CF_TIME_UUID, rowKey).delete();
        m.execute();
        m.discardMutations();

        long now = System.currentTimeMillis();
        long msecPerDay = 86400000;
        for (int i = 0; i < 100; i++) {
            m.withRow(CF_TIME_UUID, rowKey).putColumn(
                    TimeUUIDUtils.getTimeUUID(now - i * msecPerDay), i, null);
        }
        m.execute();
        
        OperationResult<ColumnList<UUID>> result = keyspace
        		.prepareQuery(CF_TIME_UUID)
        		.getKey(rowKey)
        		.withColumnRange(
        				new RangeBuilder()
        				.setLimit(100)
        				.setStart(
        						TimeUUIDUtils.getTimeUUID(now - 20
        								* msecPerDay)).build())
        								.execute();
        Assert.assertTrue(result.getResult().size() >= 20);
    }
}>>>>>>> YOURS
/home/ramdisk/experiment3/projects/astyanax/revisions/rev_2b15fd9_3de01e3/rev_2b15fd9-3de01e3/astyanax-test/src/main/java/com/netflix/astyanax/cql/test/SingleColumnMutationTests.java;<<<<<<< MINE
=======
package com.netflix.astyanax.cql.test;

import org.junit.AfterClass;
import org.junit.Assert;
import org.junit.BeforeClass;
import org.junit.Test;

import com.netflix.astyanax.model.ColumnFamily;
import com.netflix.astyanax.model.ColumnList;
import com.netflix.astyanax.serializers.LongSerializer;
import com.netflix.astyanax.serializers.StringSerializer;

public class SingleColumnMutationTests extends KeyspaceTests {

	public static ColumnFamily<Long, String> CF_SINGLE_COLUMN = ColumnFamily
			.newColumnFamily(
					"cfsinglecolmutation", 
					LongSerializer.get(),
					StringSerializer.get());

	@BeforeClass
	public static void init() throws Exception {
		initContext();
		keyspace.createColumnFamily(CF_SINGLE_COLUMN, null);
		CF_SINGLE_COLUMN.describe(keyspace);
	}

	@AfterClass
	public static void tearDown() throws Exception {
		keyspace.dropColumnFamily(CF_SINGLE_COLUMN);
	}

	@Test
	public void testSingleColumnMutation() throws Exception {

		keyspace.prepareColumnMutation(CF_SINGLE_COLUMN, 1L, "1").putValue("11", null).execute();
		keyspace.prepareColumnMutation(CF_SINGLE_COLUMN, 1L, "2").putValue("22", null).execute();
		keyspace.prepareColumnMutation(CF_SINGLE_COLUMN, 1L, "3").putValue("33", null).execute();
		
		ColumnList<String> result = keyspace.prepareQuery(CF_SINGLE_COLUMN).getRow(1L).execute().getResult();
		Assert.assertTrue(3 == result.size());
		
		Assert.assertEquals("11", result.getColumnByName("1").getStringValue());
		Assert.assertEquals("22", result.getColumnByName("2").getStringValue());
		Assert.assertEquals("33", result.getColumnByName("3").getStringValue());
		
		keyspace.prepareColumnMutation(CF_SINGLE_COLUMN, 1L, "2").putEmptyColumn(null).execute();
		keyspace.prepareColumnMutation(CF_SINGLE_COLUMN, 1L, "3").deleteColumn().execute();

		result = keyspace.prepareQuery(CF_SINGLE_COLUMN).getRow(1L).execute().getResult();
		Assert.assertTrue(2 == result.size());
		
		Assert.assertEquals("11", result.getColumnByName("1").getStringValue());
		Assert.assertNull(result.getColumnByName("2").getStringValue());
	}
}>>>>>>> YOURS
/home/ramdisk/experiment3/projects/astyanax/revisions/rev_2b15fd9_3de01e3/rev_2b15fd9-3de01e3/astyanax-test/src/main/java/com/netflix/astyanax/cql/test/LongColumnPaginationTests.java;<<<<<<< MINE
=======
package com.netflix.astyanax.cql.test;

import junit.framework.Assert;

import org.junit.AfterClass;
import org.junit.BeforeClass;
import org.junit.Test;

import com.netflix.astyanax.ColumnListMutation;
import com.netflix.astyanax.MutationBatch;
import com.netflix.astyanax.cql.reads.model.CqlRangeBuilder;
import com.netflix.astyanax.model.Column;
import com.netflix.astyanax.model.ColumnFamily;
import com.netflix.astyanax.model.ColumnList;
import com.netflix.astyanax.query.RowQuery;
import com.netflix.astyanax.serializers.LongSerializer;
import com.netflix.astyanax.serializers.StringSerializer;

public class LongColumnPaginationTests extends KeyspaceTests {
	
	public static ColumnFamily<String, Long> CF_LONGCOLUMN = ColumnFamily
            .newColumnFamily(
                    "LongColumn1", 
                    StringSerializer.get(),
                    LongSerializer.get());

    @BeforeClass
	public static void init() throws Exception {
		initContext();
		keyspace.createColumnFamily(CF_LONGCOLUMN, null);
		CF_LONGCOLUMN.describe(keyspace);
	}

    @AfterClass
	public static void teardown() throws Exception {
    	keyspace.dropColumnFamily(CF_LONGCOLUMN);
	}

    @Test
    public void paginateLongColumns() throws Exception {

        String rowKey = "A";
        MutationBatch m = keyspace.prepareMutationBatch();
        ColumnListMutation<Long> cfmLong = m.withRow(CF_LONGCOLUMN, rowKey);
        for (Long l = -10L; l < 10L; l++) {
            cfmLong.putEmptyColumn(l, null);
        }
        cfmLong.putEmptyColumn(Long.MAX_VALUE, null);
        m.execute();
        
        // READ BACK WITH PAGINATION
        Long column = Long.MIN_VALUE;
        ColumnList<Long> columns;
        int pageSize = 10;
        RowQuery<String, Long> query = keyspace
        		.prepareQuery(CF_LONGCOLUMN)
        		.getKey("A")
        		.autoPaginate(true)
        		.withColumnRange(
        				new CqlRangeBuilder<Long>()
        				.setStart(column)
        				.setFetchSize(pageSize).build());

        int pageCount = 0;
        int colCount = 0;
        while (!(columns = query.execute().getResult()).isEmpty()) {
        	for (Column<Long> c : columns) {
        		colCount++;
        	}
        	pageCount++;
        }

        Assert.assertTrue("PageCount: " + pageCount, pageCount == 3);
        Assert.assertTrue("colCount = " + colCount,colCount == 21);

        query = keyspace
        		.prepareQuery(CF_LONGCOLUMN)
        		.getKey("A")
        		.autoPaginate(true)
        		.withColumnRange(
        				new CqlRangeBuilder<Long>()
        				.setStart(-5L)
        				.setEnd(11L)
        				.setFetchSize(pageSize).build());

        pageCount = 0;
        colCount = 0;
        while (!(columns = query.execute().getResult()).isEmpty()) {
        	for (Column<Long> c : columns) {
        		colCount++;
        	}
        	pageCount++;
        }

        Assert.assertTrue(pageCount == 2);
        Assert.assertTrue("colCount = " + colCount,colCount == 15);
    }
}>>>>>>> YOURS
/home/ramdisk/experiment3/projects/astyanax/revisions/rev_2b15fd9_3de01e3/rev_2b15fd9-3de01e3/astyanax-test/src/main/java/com/netflix/astyanax/cql/test/PreparedStatementTests.java;<<<<<<< MINE
=======
package com.netflix.astyanax.cql.test;

import java.util.Date;

import junit.framework.Assert;

import org.junit.AfterClass;
import org.junit.BeforeClass;
import org.junit.Test;

import com.netflix.astyanax.ColumnListMutation;
import com.netflix.astyanax.MutationBatch;
import com.netflix.astyanax.cql.CqlPreparedStatement;
import com.netflix.astyanax.cql.reads.model.CqlRangeBuilder;
import com.netflix.astyanax.cql.test.utils.ReadTests;
import com.netflix.astyanax.model.Column;
import com.netflix.astyanax.model.ColumnFamily;
import com.netflix.astyanax.model.ColumnList;
import com.netflix.astyanax.model.Row;
import com.netflix.astyanax.model.Rows;
import com.netflix.astyanax.partitioner.Murmur3Partitioner;
import com.netflix.astyanax.query.ColumnQuery;
import com.netflix.astyanax.query.RowQuery;
import com.netflix.astyanax.query.RowSliceQuery;
import com.netflix.astyanax.serializers.IntegerSerializer;
import com.netflix.astyanax.serializers.StringSerializer;

public class PreparedStatementTests extends ReadTests {

	private static int TestRowCount = 10;

	private static ColumnFamily<String, String> CF_ROW_RANGE = 
			new ColumnFamily<String, String>("rowrange", StringSerializer.get(), StringSerializer.get(), IntegerSerializer.get());
	
	@BeforeClass
	public static void init() throws Exception {
		initContext();
		keyspace.createColumnFamily(CF_USER_INFO, null);
		keyspace.createColumnFamily(CF_ROW_RANGE, null);
		CF_USER_INFO.describe(keyspace);
		CF_ROW_RANGE.describe(keyspace);
		populateRowsForUserInfo(TestRowCount); 
		populateRowsForColumnRange();
	}
	
	@AfterClass
	public static void tearDown() throws Exception {
		keyspace.dropColumnFamily(CF_USER_INFO);
		keyspace.dropColumnFamily(CF_ROW_RANGE);
	}

	@Test
	public void testSingleRowAllColumnsQuery() throws Exception {

		for (int i=0; i<TestRowCount; i++) {
			ColumnList<String> result = keyspace.prepareQuery(CF_USER_INFO)
					.withCaching(true)
					.getRow("acct_" + i)
					.execute()
					.getResult();
			super.testAllColumnsForRow(result, i);
		}
	}
	
	@Test
	public void testSingleRowSingleColumnQuery() throws Exception {

		for (int i=0; i<TestRowCount; i++) {
			Column<String> result = keyspace.prepareQuery(CF_USER_INFO)
					.withCaching(true)
					.getRow("acct_" + i)
					.getColumn("address")
					.execute()
					.getResult();
			
			Assert.assertNotNull(result);
			Assert.assertEquals("address", result.getName());
			Assert.assertNotNull(result.getStringValue());
			Assert.assertEquals("john smith address " + i, result.getStringValue());
		}
	}
	
	@Test
	public void testSingleRowColumnSliceQueryWithCollection() throws Exception {

		for (int i=0; i<TestRowCount; i++) {
			ColumnList<String> result = keyspace.prepareQuery(CF_USER_INFO)
					.withCaching(true)
					.getRow("acct_" + i)
					.withColumnSlice("firstname", "lastname", "address", "age")
					.execute()
					.getResult();
			
			Assert.assertNotNull(result);
			Assert.assertTrue(4 == result.size());
			Assert.assertEquals("john_" + i, result.getColumnByName("firstname").getStringValue());
			Assert.assertEquals("smith_" + i, result.getColumnByName("lastname").getStringValue());
			Assert.assertEquals("john smith address " + i, result.getColumnByName("address").getStringValue());
			Assert.assertTrue(30 + i == result.getColumnByName("age").getIntegerValue());
		}
	}

	@Test
	public void testRowKeysAllColumnsQuery() throws Exception {

		keyspace.prepareQuery(CF_USER_INFO)
				.withCaching(true)
				.getRowSlice("acct_0", "acct_1", "acct_2", "acct_3")
				.execute();
		
		Rows<String, String> result = keyspace.prepareQuery(CF_USER_INFO)
				.withCaching(true)
				.getRowSlice("acct_4", "acct_5", "acct_6", "acct_7")
				.execute()
				.getResult();

		Assert.assertNotNull(result);
		Assert.assertTrue(4 == result.size());
		for (Row<String, String> row : result) {
			int rowNo = Integer.parseInt(row.getKey().substring("acct_".length()));
			Assert.assertTrue(rowNo >= 4);
			Assert.assertTrue(rowNo <= 7);
			Assert.assertTrue(13 == row.getColumns().size());
		}
	}

	@Test
	public void testRowKeysColumnSetQuery() throws Exception {

		keyspace.prepareQuery(CF_USER_INFO)
				.withCaching(true)
				.getRowSlice("acct_0", "acct_1", "acct_2", "acct_3")
				.withColumnSlice("firstname", "lastname", "age")
				.execute();

		Rows<String, String> result = keyspace.prepareQuery(CF_USER_INFO)
				.withCaching(true)
				.getRowSlice("acct_4", "acct_5", "acct_6", "acct_7")
				.withColumnSlice("firstname", "lastname", "age")
				.execute()
				.getResult();

		Assert.assertNotNull(result);
		Assert.assertTrue(4 == result.size());
		for (Row<String, String> row : result) {
			int rowNo = Integer.parseInt(row.getKey().substring("acct_".length()));
			Assert.assertTrue(rowNo >= 4);
			Assert.assertTrue(rowNo <= 7);
			Assert.assertTrue(3 == row.getColumns().size());
		}
	}

	@Test
	public void testRowKeysColumnRangeQuery() throws Exception {

		keyspace.prepareQuery(CF_ROW_RANGE)
				.withCaching(true)
				.getRowSlice("A", "B", "C", "D")
				.withColumnRange(new CqlRangeBuilder<String>()
						.setStart("a")
						.setEnd("c")
						.build())
						.execute();
		
		Rows<String, String> result = keyspace.prepareQuery(CF_ROW_RANGE)
				.withCaching(true)
				.getRowSlice("E", "F", "G", "H")
				.withColumnRange(new CqlRangeBuilder<String>()
						.setStart("d")
						.setEnd("h")
						.build())
				.execute()
				.getResult();

		Assert.assertNotNull(result);
		Assert.assertTrue(4 == result.size());
		for (Row<String, String> row : result) {
			int rowNo = row.getKey().charAt(0) - 'A';
			Assert.assertTrue(rowNo >= 4);
			Assert.assertTrue(rowNo <= 7);
			Assert.assertTrue(5 == row.getColumns().size());
		}
	}
	
	@Test
	public void testRowRangeAllColumnsQuery() throws Exception {

		String startToken = Murmur3Partitioner.get().getTokenForKey(StringSerializer.get().fromString("A"));
		String endToken = Murmur3Partitioner.get().getTokenForKey(StringSerializer.get().fromString("G"));
		
		keyspace.prepareQuery(CF_ROW_RANGE)
				.withCaching(true)
				.getRowRange(null, null, startToken, endToken, 10)
				.execute();

		Rows<String, String> result = keyspace.prepareQuery(CF_ROW_RANGE)
				.withCaching(true)
				.getRowRange(null, null, startToken, endToken, 10)
				.execute()
				.getResult();

		Assert.assertNotNull(result);
		Assert.assertTrue(3 == result.size());
		for (Row<String, String> row : result) {
			Assert.assertTrue(26 == row.getColumns().size());
		}
	}

	@Test
	public void testRowRangeColumnSetQuery() throws Exception {

		String startToken = Murmur3Partitioner.get().getTokenForKey(StringSerializer.get().fromString("A"));
		String endToken = Murmur3Partitioner.get().getTokenForKey(StringSerializer.get().fromString("G"));
		
		keyspace.prepareQuery(CF_ROW_RANGE)
				.withCaching(true)
				.getRowRange(null, null, startToken, endToken, 10)
				.withColumnSlice("a", "s", "d", "f")
				.execute();
		
		Rows<String, String> result = keyspace.prepareQuery(CF_ROW_RANGE)
				.withCaching(true)
				.getRowRange(null, null, startToken, endToken, 10)
				.withColumnSlice("a", "s", "d", "f")
				.execute()
				.getResult();

		Assert.assertNotNull(result);
		Assert.assertTrue(3 == result.size());
		for (Row<String, String> row : result) {
			Assert.assertTrue(4 == row.getColumns().size());
		}
	}

	@Test
	public void testRowRangeColumnRangeQuery() throws Exception {

		String startToken = Murmur3Partitioner.get().getTokenForKey(StringSerializer.get().fromString("A"));
		String endToken = Murmur3Partitioner.get().getTokenForKey(StringSerializer.get().fromString("G"));
		
		keyspace.prepareQuery(CF_ROW_RANGE)
				.withCaching(true)
				.getRowRange(null, null, startToken, endToken, 10)
				.withColumnRange(new CqlRangeBuilder<String>()
						.setStart("d")
						.setEnd("h")
						.build())
						.execute();

		Rows<String, String> result = keyspace.prepareQuery(CF_ROW_RANGE)
				.withCaching(true)
				.getRowRange(null, null, startToken, endToken, 10)
				.withColumnRange(new CqlRangeBuilder<String>()
						.setStart("d")
						.setEnd("h")
						.build())
				.execute()
				.getResult();

		Assert.assertNotNull(result);
		Assert.assertTrue(3 == result.size());
		for (Row<String, String> row : result) {
			Assert.assertTrue(5 == row.getColumns().size());
		}
	}

	private static void populateRowsForUserInfo(int numRows) throws Exception {

		MutationBatch mb = keyspace.prepareMutationBatch();

		for (int i=0; i<numRows; i++) {

			Date date = OriginalDate.plusMinutes(i).toDate();
			mb.withRow(CF_USER_INFO, "acct_" + i)
			.putColumn("firstname", "john_" + i, null)
			.putColumn("lastname", "smith_" + i, null)
			.putColumn("address", "john smith address " + i, null)
			.putColumn("age", 30+i, null)
			.putColumn("ageShort", new Integer(30+i).shortValue(), null)
			.putColumn("ageLong", new Integer(30+i).longValue(), null)
			.putColumn("percentile", 30.1)
			.putColumn("married", true)
			.putColumn("single", false)
			.putColumn("birthdate", date)
			.putColumn("bytes", TestBytes)
			.putColumn("uuid", TestUUID)
			.putEmptyColumn("empty");

			mb.execute();
			mb.discardMutations();
		}
	}

	private static void populateRowsForColumnRange() throws Exception {
		
        MutationBatch m = keyspace.prepareMutationBatch();

        for (char keyName = 'A'; keyName <= 'Z'; keyName++) {
        	String rowKey = Character.toString(keyName);
        	ColumnListMutation<String> colMutation = m.withRow(CF_ROW_RANGE, rowKey);
              for (char cName = 'a'; cName <= 'z'; cName++) {
            	  colMutation.putColumn(Character.toString(cName), (int) (cName - 'a') + 1, null);
              }
              m.withCaching(true);
              m.execute();
              m.discardMutations();
        }
	}

}>>>>>>> YOURS
/home/ramdisk/experiment3/projects/astyanax/revisions/rev_2b15fd9_3de01e3/rev_2b15fd9-3de01e3/astyanax-test/src/main/java/com/netflix/astyanax/cql/test/ColumnTimestampAndTTLTests.java;<<<<<<< MINE
=======
package com.netflix.astyanax.cql.test;

import org.junit.AfterClass;
import org.junit.Assert;
import org.junit.BeforeClass;
import org.junit.Test;

import com.netflix.astyanax.MutationBatch;
import com.netflix.astyanax.model.ColumnFamily;
import com.netflix.astyanax.model.ColumnList;
import com.netflix.astyanax.serializers.LongSerializer;
import com.netflix.astyanax.serializers.StringSerializer;

public class ColumnTimestampAndTTLTests extends KeyspaceTests {

    private static ColumnFamily<Long, Long> CF_COL_TIMESTAMP = ColumnFamily
            .newColumnFamily(
                    "columntimestamps", 
                    LongSerializer.get(),
                    LongSerializer.get(),
                    LongSerializer.get());
	
    private static ColumnFamily<String, String> CF_TTL = ColumnFamily
            .newColumnFamily(
                    "columnttls", 
                    StringSerializer.get(),
                    StringSerializer.get());

	@BeforeClass
	public static void init() throws Exception {
		initContext();
		keyspace.createColumnFamily(CF_COL_TIMESTAMP,     null);
		keyspace.createColumnFamily(CF_TTL,     null);
		
		CF_COL_TIMESTAMP.describe(keyspace);
		CF_TTL.describe(keyspace);
	}
	
	@AfterClass
	public static void teardown() throws Exception {
		keyspace.dropColumnFamily(CF_COL_TIMESTAMP);
		keyspace.dropColumnFamily(CF_TTL);
	}

	@Test
	public void testColumnTimestamps() throws Exception {
		
		CF_COL_TIMESTAMP.describe(keyspace);

        MutationBatch mb = keyspace.prepareMutationBatch();
        mb.withRow(CF_COL_TIMESTAMP, 1L)
            .setTimestamp(1).putColumn(1L, 1L)
            .setTimestamp(10).putColumn(2L, 2L)
            ;
        mb.execute();
        
        ColumnList<Long> result1 = keyspace.prepareQuery(CF_COL_TIMESTAMP).getRow(1L).execute().getResult();
        Assert.assertEquals(2, result1.size());
        Assert.assertNotNull(result1.getColumnByName(1L));
        Assert.assertNotNull(result1.getColumnByName(2L));
        
        mb = keyspace.prepareMutationBatch();
        mb.withRow(CF_COL_TIMESTAMP,  1L)
            .setTimestamp(result1.getColumnByName(1L).getTimestamp()-1)
            .deleteColumn(1L)
            .setTimestamp(result1.getColumnByName(2L).getTimestamp()-1)
            .deleteColumn(2L)
            .putEmptyColumn(3L, null);
        
        mb.execute();
        
        result1 = keyspace.prepareQuery(CF_COL_TIMESTAMP).getRow(1L).execute().getResult();
        Assert.assertEquals(3, result1.size());
        
        mb = keyspace.prepareMutationBatch();
        mb.withRow(CF_COL_TIMESTAMP,  1L)
            .setTimestamp(result1.getColumnByName(1L).getTimestamp()+1)
            .deleteColumn(1L)
            .setTimestamp(result1.getColumnByName(2L).getTimestamp()+1)
            .deleteColumn(2L);
        mb.execute();
        
        result1 = keyspace.prepareQuery(CF_COL_TIMESTAMP).getRow(1L).execute().getResult();
        Assert.assertEquals(1, result1.size());
    }
	

    @Test
    public void testTtlValues() throws Exception {
        MutationBatch mb = keyspace.prepareMutationBatch();
        mb.withRow(CF_TTL, "row")
          .putColumn("TTL0", "TTL0", 0)
          .putColumn("TTLNULL", "TTLNULL", null)
          .putColumn("TTL1", "TTL1", 1);
        
        mb.execute();
        
        Thread.sleep(2000);
        
        ColumnList<String> result = keyspace.prepareQuery(CF_TTL)
            .getRow("row")
            .execute().getResult();
       
        Assert.assertEquals(2,  result.size());
        Assert.assertNotNull(result.getColumnByName("TTL0"));
        Assert.assertNotNull(result.getColumnByName("TTLNULL"));
    }
}>>>>>>> YOURS
/home/ramdisk/experiment3/projects/astyanax/revisions/rev_2b15fd9_3de01e3/rev_2b15fd9-3de01e3/astyanax-test/src/main/java/com/netflix/astyanax/cql/test/ColumnCountQueryTests.java;<<<<<<< MINE
=======
package com.netflix.astyanax.cql.test;

import java.util.Collection;
import java.util.HashMap;
import java.util.HashSet;
import java.util.List;
import java.util.Map;
import java.util.Random;
import java.util.Set;

import junit.framework.Assert;

import org.junit.AfterClass;
import org.junit.BeforeClass;
import org.junit.Test;

import com.netflix.astyanax.ColumnListMutation;
import com.netflix.astyanax.MutationBatch;
import com.netflix.astyanax.cql.test.utils.ReadTests;
import com.netflix.astyanax.cql.test.utils.TestUtils;
import com.netflix.astyanax.cql.test.utils.TestUtils.TestTokenRange;
import com.netflix.astyanax.model.ColumnFamily;

public class ColumnCountQueryTests extends ReadTests {

	private static ColumnFamily<String, String> CF_COLUMN_RANGE_TEST = TestUtils.CF_COLUMN_RANGE_TEST;

	@BeforeClass
	public static void init() throws Exception {
		initContext();
		keyspace.createColumnFamily(CF_COLUMN_RANGE_TEST, null);
		CF_COLUMN_RANGE_TEST.describe(keyspace);
	}
	
	@AfterClass
	public static void tearDown() throws Exception {
		keyspace.dropColumnFamily(CF_COLUMN_RANGE_TEST);
	}

	@Test
	public void runAllTests() throws Exception {
		
		CF_COLUMN_RANGE_TEST.describe(keyspace);
		
		boolean rowDeleted = false; 
		
		/** INSERT ROWS FOR COLUMN COUNT READ TESTS */
		populateRowsForColumnRange();
		Thread.sleep(1000);
		
		/** PERFORM READS AND CHECK FOR COLUMN COUNTS */
		testColumnCountSingleRowAndAllColumns(rowDeleted); 
		testColumnCountSingleRowAndColumnSet(rowDeleted); 
		testColumnCountSingleRowAndColumnRange(rowDeleted);
		
		testColumnCountMultipleRowKeysAndAllColumns(rowDeleted); 
		testColumnCountMultipleRowKeysAndColumnSet(rowDeleted);
		testColumnCountMultipleRowKeysAndColumnRange(rowDeleted);
		
		testColumnCountRowRangeAndAllColumns(rowDeleted); 
		testColumnCountRowRangeAndColumnSet(rowDeleted); 
		testColumnCountRowRangeAndColumnRange(rowDeleted); 

		/** DELETE ROWS */
		deleteRowsForColumnRange(); 
		Thread.sleep(1000);
		rowDeleted = true; 
		
		/** PERFORM READS AND CHECK FOR COLUMN COUNTS  = 0 */
		testColumnCountSingleRowAndAllColumns(rowDeleted); 
		testColumnCountSingleRowAndColumnSet(rowDeleted); 
		testColumnCountSingleRowAndColumnRange(rowDeleted);
		
		testColumnCountMultipleRowKeysAndAllColumns(rowDeleted); 
		testColumnCountMultipleRowKeysAndColumnSet(rowDeleted);
		testColumnCountMultipleRowKeysAndColumnRange(rowDeleted); 

		testColumnCountRowRangeAndAllColumns(rowDeleted); 
		testColumnCountRowRangeAndColumnSet(rowDeleted); 
		testColumnCountRowRangeAndColumnRange(rowDeleted); 
	}
	
	private void testColumnCountSingleRowAndAllColumns(boolean rowDeleted) throws Exception {

		char ch = 'A';
		while (ch <= 'Z') {
			String rowKey = String.valueOf(ch);

			Integer columnCount = keyspace
					.prepareQuery(CF_COLUMN_RANGE_TEST)
					.getKey(rowKey)
					.getCount()
					.execute().getResult();

			int expected = rowDeleted ? 0 : 26;
			Assert.assertTrue("expected: " + expected + " colCount: " + columnCount, expected == columnCount);
			ch++;
		}
	}
	
	private void testColumnCountSingleRowAndColumnSet(boolean rowDeleted) throws Exception {

		Random random = new Random();
		
		char ch = 'A';
		while (ch <= 'Z') {
			
			String rowKey = String.valueOf(ch);
			int numColumns = random.nextInt(26) + 1; // avoid 0
			
			Integer columnCount = keyspace
					.prepareQuery(CF_COLUMN_RANGE_TEST)
					.getKey(rowKey)
					.withColumnSlice(getRandomColumns(numColumns))
					.getCount()
					.execute().getResult();

			int expected = rowDeleted ? 0 : numColumns;
			Assert.assertTrue("expected: " + expected + " colCount: " + columnCount, expected == columnCount);
			ch++;
		}
	}
	
	private void testColumnCountSingleRowAndColumnRange(boolean rowDeleted) throws Exception {
		
		Random random = new Random();
		
		char ch = 'A';
		while (ch <= 'Z') {

			String rowKey = String.valueOf(ch);
			
			// Get a random start column
			int rand = random.nextInt(26);
			char randCH = (char) ('a' + rand);
			String startCol = String.valueOf(randCH);
			
			Integer columnCount = keyspace
					.prepareQuery(CF_COLUMN_RANGE_TEST)
					.getKey(rowKey)
					.withColumnRange(startCol, "z", false, -1)
					.getCount()
					.execute().getResult();

			int charOffset = startCol.charAt(0) - 'a' + 1;
			int expected = rowDeleted ? 0 : 26 - charOffset + 1;
			
			/**
			 * e.g  if start col = 'b' 
			 *      then range = 'b' -> 'z' both inclusive 
			 *      then colCount = 25 
			 *      where
			 *      'a' - 'b' + 1 = 2 which is offset for 'b'
			 *      25 = 26 ('z') - 2('b') + 1 
			 */
	 		
			Assert.assertTrue("expected: " + expected + " colCount: " + columnCount, expected == columnCount);
			ch++;
		}
	}
	
	private void testColumnCountMultipleRowKeysAndAllColumns(boolean rowDeleted) throws Exception {

		Collection<String> rowKeys = getRandomRowKeys();
		
		Map<String, Integer> columnCountsPerRowKey = keyspace
				.prepareQuery(CF_COLUMN_RANGE_TEST)
				.getRowSlice(rowKeys)
				.getColumnCounts()
				.execute().getResult();

		Map<String, Integer> expected = new HashMap<String, Integer>();
		if (!rowDeleted) {
			for (String key : rowKeys) {
				expected.put(key, 26);
			}
		}
		Assert.assertEquals("expected: " + expected + " colCount: " + columnCountsPerRowKey, expected, columnCountsPerRowKey);
	}

	private void testColumnCountMultipleRowKeysAndColumnSet(boolean rowDeleted) throws Exception {

		Collection<String> rowKeys = getRandomRowKeys();
		Collection<String> columns = getRandomColumns(new Random().nextInt(26) + 1);
		
		
		Map<String, Integer> columnCountsPerRowKey = keyspace
				.prepareQuery(CF_COLUMN_RANGE_TEST)
				.getRowSlice(rowKeys)
				.withColumnSlice(columns)
				.getColumnCounts()
				.execute().getResult();

		Map<String, Integer> expected = new HashMap<String, Integer>();
		if (!rowDeleted) {
			for (String key : rowKeys) {
				expected.put(key, columns.size());
			}
		}
		Assert.assertEquals("expected: " + expected + " colCount: " + columnCountsPerRowKey, expected, columnCountsPerRowKey);
	}
	
	private void testColumnCountMultipleRowKeysAndColumnRange(boolean rowDeleted) throws Exception {

		Collection<String> rowKeys = getRandomRowKeys();

		// Get a random start column
		int rand = new Random().nextInt(26);
		char randCH = (char) ('a' + rand);
		String startCol = String.valueOf(randCH);
		
		Map<String, Integer> columnCountsPerRowKey = keyspace
				.prepareQuery(CF_COLUMN_RANGE_TEST)
				.getRowSlice(rowKeys)
				.withColumnRange(startCol, "z", false, -1)
				.getColumnCounts()
				.execute().getResult();

		int charOffset = startCol.charAt(0) - 'a' + 1;
		int expectedColCount = 26 - charOffset + 1;

		Map<String, Integer> expected = new HashMap<String, Integer>();
		if (!rowDeleted) {
			for (String key : rowKeys) {
				expected.put(key, expectedColCount);
			}
		}
		Assert.assertEquals("expected: " + expected + " colCount: " + columnCountsPerRowKey, expected, columnCountsPerRowKey);
	}
	
	private void testColumnCountRowRangeAndAllColumns(boolean rowDeleted) throws Exception {

		List<TestTokenRange> testRanges = TestUtils.getTestTokenRanges();
		
		Map<String, Integer> expectedRowCounts = rowDeleted ? new HashMap<String, Integer>() : getExpectedRowCountsForTokenRanges(testRanges, 26);
		Map<String, Integer> resultRowCounts = new HashMap<String, Integer>();
		
		for (TestTokenRange testRange : testRanges) {
			
			Map<String, Integer> rowCounts = keyspace.prepareQuery(CF_COLUMN_RANGE_TEST)
					.getRowRange(null, null, testRange.startToken, testRange.endToken, -1)
					.getColumnCounts()
					.execute().getResult();

			resultRowCounts.putAll(rowCounts);
		}
		
		Assert.assertEquals(expectedRowCounts, resultRowCounts);
	}
	
	private void testColumnCountRowRangeAndColumnSet(boolean rowDeleted) throws Exception {

		Collection<String> columns = getRandomColumns(new Random().nextInt(26) + 1);

		List<TestTokenRange> testRanges = TestUtils.getTestTokenRanges();
		
		Map<String, Integer> expectedRowCounts = rowDeleted ? new HashMap<String, Integer>() : getExpectedRowCountsForTokenRanges(testRanges, columns.size());
		Map<String, Integer> resultRowCounts = new HashMap<String, Integer>();
		
		for (TestTokenRange testRange : testRanges) {
			
			Map<String, Integer> rowCounts = keyspace.prepareQuery(CF_COLUMN_RANGE_TEST)
					.getRowRange(null, null, testRange.startToken, testRange.endToken, -1)
					.withColumnSlice(columns)
					.getColumnCounts()
					.execute().getResult();

			resultRowCounts.putAll(rowCounts);
		}
		
		Assert.assertEquals(expectedRowCounts, resultRowCounts);
	}
	
	private void testColumnCountRowRangeAndColumnRange(boolean rowDeleted) throws Exception {

		// Get the random start column
		int rand = new Random().nextInt(26);
		char randCH = (char) ('a' + rand);
		String startColumn = String.valueOf(randCH);

		int expectedColCount = 'z' - startColumn.charAt(0) + 1; 
		
		List<TestTokenRange> testRanges = TestUtils.getTestTokenRanges();
		
		Map<String, Integer> expectedRowCounts = rowDeleted ? new HashMap<String, Integer>() : getExpectedRowCountsForTokenRanges(testRanges, expectedColCount);
		Map<String, Integer> resultRowCounts = new HashMap<String, Integer>();
		
		for (TestTokenRange testRange : testRanges) {
			
			Map<String, Integer> rowCounts = keyspace.prepareQuery(CF_COLUMN_RANGE_TEST)
					.getRowRange(null, null, testRange.startToken, testRange.endToken, -1)
					.withColumnRange(startColumn, "z", false, -1)
					.getColumnCounts()
					.execute().getResult();

			resultRowCounts.putAll(rowCounts);
		}
		
		Assert.assertEquals(expectedRowCounts, resultRowCounts);
	}

	private void populateRowsForColumnRange() throws Exception {
		
        MutationBatch m = keyspace.prepareMutationBatch();

        for (char keyName = 'A'; keyName <= 'Z'; keyName++) {
        	String rowKey = Character.toString(keyName);
        	ColumnListMutation<String> colMutation = m.withRow(CF_COLUMN_RANGE_TEST, rowKey);
              for (char cName = 'a'; cName <= 'z'; cName++) {
            	  colMutation.putColumn(Character.toString(cName), (int) (cName - 'a') + 1, null);
              }
              m.withCaching(true);
              m.execute();
              m.discardMutations();
        }
	}

	private void deleteRowsForColumnRange() throws Exception {
		
        for (char keyName = 'A'; keyName <= 'Z'; keyName++) {
            MutationBatch m = keyspace.prepareMutationBatch();
        	String rowKey = Character.toString(keyName);
        	m.withRow(CF_COLUMN_RANGE_TEST, rowKey).delete();
        	m.execute();
        	m.discardMutations();
        }
	}
	
	private Collection<String> getRandomRowKeys() {

		Random random = new Random();
		
		int numRowKeys = random.nextInt(26) + 1;
		
		Set<String> hashSet = new HashSet<String>();

		while(hashSet.size() < numRowKeys) {
			int pick = random.nextInt(26);
			char ch = (char) ('A' + pick);
			hashSet.add(String.valueOf(ch));
		}
		return hashSet;
	}
	
	private Map<String, Integer> getExpectedRowCountsForTokenRanges(List<TestTokenRange> testRanges, int expectedColumnCountForEachRow) {
		
		Map<String, Integer> expectedRowCounts = new HashMap<String, Integer>();

		for (TestTokenRange range : testRanges) {
			for (String rowKey : range.expectedRowKeys) {
				expectedRowCounts.put(rowKey, expectedColumnCountForEachRow);
			}
		}
		
		return expectedRowCounts;
	}
}>>>>>>> YOURS
/home/ramdisk/experiment3/projects/astyanax/revisions/rev_2b15fd9_3de01e3/rev_2b15fd9-3de01e3/astyanax-test/src/main/java/com/netflix/astyanax/cql/test/CounterColumnTests.java;<<<<<<< MINE
=======
package com.netflix.astyanax.cql.test;

import junit.framework.Assert;

import org.junit.AfterClass;
import org.junit.BeforeClass;
import org.junit.Test;

import com.google.common.collect.ImmutableMap;
import com.netflix.astyanax.MutationBatch;
import com.netflix.astyanax.model.Column;
import com.netflix.astyanax.model.ColumnFamily;
import com.netflix.astyanax.serializers.StringSerializer;

public class CounterColumnTests extends KeyspaceTests {
	
    public static ColumnFamily<String, String> CF_COUNTER1 = ColumnFamily
            .newColumnFamily(
                    "Counter1", 
                    StringSerializer.get(),
                    StringSerializer.get());

    @BeforeClass
	public static void init() throws Exception {
		initContext();
    	keyspace.createColumnFamily(CF_COUNTER1,ImmutableMap.<String, Object>builder()
                .put("default_validation_class", "CounterColumnType")
                .build());
		
		CF_COUNTER1.describe(keyspace);
	}

    @AfterClass
	public static void tearDown() throws Exception {
		initContext();
		keyspace.dropColumnFamily(CF_COUNTER1);
	}

    @Test
    public void testIncrementCounter() throws Exception {
        long baseAmount, incrAmount = 100;
        Column<String> column;

        column = keyspace.prepareQuery(CF_COUNTER1).getRow("CounterRow1").getColumn("MyCounter").execute().getResult();
        //Assert.assertNull(column);

        baseAmount = 0; 
        
        MutationBatch m = keyspace.prepareMutationBatch();
        m.withRow(CF_COUNTER1, "CounterRow1").incrementCounterColumn("MyCounter", incrAmount);
        m.execute();
//
//        column = keyspace.prepareQuery(CF_COUNTER1).getRow("CounterRow1").getColumn("MyCounter").execute().getResult();
//        Assert.assertNotNull(column);
//        Assert.assertEquals(baseAmount + incrAmount, column.getLongValue());
//
//        m = keyspace.prepareMutationBatch();
//        m.withRow(CF_COUNTER1, "CounterRow1").incrementCounterColumn("MyCounter", incrAmount);
//        m.execute();
//
//        column = keyspace.prepareQuery(CF_COUNTER1).getRow("CounterRow1").getColumn("MyCounter").execute().getResult();
//        Assert.assertNotNull(column);
//        Assert.assertEquals(column.getLongValue(), baseAmount + 2 * incrAmount);
    }

    @Test
    public void testDeleteCounter() throws Exception {
        Column<String> column;
        String rowKey = "CounterRowDelete1";
        String counterName = "MyCounter";

        // Increment the column
        MutationBatch m = keyspace.prepareMutationBatch();
        m.withRow(CF_COUNTER1, rowKey).incrementCounterColumn(counterName, 1);
        m.execute();

//        // Read back the value
//        column = keyspace.prepareQuery(CF_COUNTER1).getRow(rowKey).getColumn(counterName).execute().getResult();
//        Assert.assertNotNull(column);
//        Assert.assertEquals(column.getLongValue(), 1);
//
//        // Delete the column
//        keyspace.prepareColumnMutation(CF_COUNTER1, rowKey, counterName).deleteCounterColumn().execute();
//
//        // Try to read back
//        // This should be non-existent
//        column = keyspace.prepareQuery(CF_COUNTER1).getRow(rowKey).getColumn(counterName).execute().getResult();
//        Assert.assertNull(column);
    }


}>>>>>>> YOURS
/home/ramdisk/experiment3/projects/astyanax/revisions/rev_2b15fd9_3de01e3/rev_2b15fd9-3de01e3/astyanax-test/src/main/java/com/netflix/astyanax/cql/test/AllRowsQueryTest.java;<<<<<<< MINE
=======
package com.netflix.astyanax.cql.test;

import java.util.concurrent.atomic.AtomicLong;

import org.junit.AfterClass;
import org.junit.Assert;
import org.junit.BeforeClass;
import org.junit.Test;
import org.slf4j.Logger;
import org.slf4j.LoggerFactory;

import com.netflix.astyanax.ColumnListMutation;
import com.netflix.astyanax.ExceptionCallback;
import com.netflix.astyanax.MutationBatch;
import com.netflix.astyanax.RowCallback;
import com.netflix.astyanax.connectionpool.OperationResult;
import com.netflix.astyanax.connectionpool.exceptions.ConnectionException;
import com.netflix.astyanax.model.ColumnFamily;
import com.netflix.astyanax.model.Row;
import com.netflix.astyanax.model.Rows;
import com.netflix.astyanax.serializers.StringSerializer;

public class AllRowsQueryTest extends KeyspaceTests {
    
	private static final Logger LOG = LoggerFactory.getLogger(AllRowsQueryTest.class);
	
	public static ColumnFamily<String, String> CF_ALL_ROWS = ColumnFamily
            .newColumnFamily(
                    "allrows", 
                    StringSerializer.get(),
                    StringSerializer.get());

    @BeforeClass
	public static void init() throws Exception {
		initContext();
		
		keyspace.createColumnFamily(CF_ALL_ROWS, null);
		CF_ALL_ROWS.describe(keyspace);
		
		/** POPULATE ROWS FOR TESTS */
    	MutationBatch m = keyspace.prepareMutationBatch();

        for (char keyName = 'A'; keyName <= 'Z'; keyName++) {
            String rowKey = Character.toString(keyName);
            ColumnListMutation<String> cfmStandard = m.withRow(CF_ALL_ROWS, rowKey);
            for (char cName = 'a'; cName <= 'z'; cName++) {
                cfmStandard.putColumn(Character.toString(cName), (int) (cName - 'a') + 1, null);
            }
            m.withCaching(true);
            m.execute();
            m.discardMutations();
        }
    }

    @AfterClass
	public static void tearDown() throws Exception {
    	keyspace.dropColumnFamily(CF_ALL_ROWS);
    }
    
	@Test
	public void getAllWithCallback() throws Exception {
		
		final AtomicLong counter = new AtomicLong();

		keyspace.prepareQuery(CF_ALL_ROWS).getAllRows()
		.setRowLimit(30)
		.setRepeatLastToken(false)
		.setConcurrencyLevel(2)
		//.withColumnRange(new RangeBuilder().setLimit(2).build())
		.executeWithCallback(new RowCallback<String, String>() {
			@Override
			public void success(Rows<String, String> rows) {
				for (Row<String, String> row : rows) {
					LOG.info("ROW: " + row.getKey() + " "
							+ row.getColumns().size());
					counter.incrementAndGet();
				}
			}

			@Override
			public boolean failure(ConnectionException e) {
				LOG.error(e.getMessage(), e);
				return false;
			}
		});
		LOG.info("Read " + counter.get() + " keys");
		Assert.assertEquals(26,  counter.get());
	}
	
	@Test
	public void getAll() throws Exception {
		AtomicLong counter = new AtomicLong(0);

		OperationResult<Rows<String, String>> rows = keyspace
				.prepareQuery(CF_ALL_ROWS).getAllRows().setConcurrencyLevel(2).setRowLimit(30)
				//.withColumnRange(new RangeBuilder().setLimit(0).build())
				.setExceptionCallback(new ExceptionCallback() {
					@Override
					public boolean onException(ConnectionException e) {
						Assert.fail(e.getMessage());
						return true;
					}
				}).execute();
		for (Row<String, String> row : rows.getResult()) {
			counter.incrementAndGet();
			LOG.info("ROW: " + row.getKey() + " " + row.getColumns().size());
		}
		LOG.info("Read " + counter.get() + " keys");
		Assert.assertEquals(26, counter.get());
	}
}>>>>>>> YOURS
/home/ramdisk/experiment3/projects/astyanax/revisions/rev_2b15fd9_3de01e3/rev_2b15fd9-3de01e3/astyanax-test/src/main/java/com/netflix/astyanax/cql/test/SingleRowColumnPaginationTests.java;<<<<<<< MINE
=======
package com.netflix.astyanax.cql.test;

import java.util.Random;

import junit.framework.Assert;

import org.junit.AfterClass;
import org.junit.BeforeClass;
import org.junit.Test;

import com.netflix.astyanax.cql.reads.model.CqlRangeBuilder;
import com.netflix.astyanax.cql.test.utils.ReadTests;
import com.netflix.astyanax.cql.test.utils.TestUtils;
import com.netflix.astyanax.model.Column;
import com.netflix.astyanax.model.ColumnFamily;
import com.netflix.astyanax.model.ColumnList;
import com.netflix.astyanax.query.RowQuery;

public class SingleRowColumnPaginationTests extends ReadTests {
	
	private static ColumnFamily<String, String> CF_COLUMN_RANGE_TEST = TestUtils.CF_COLUMN_RANGE_TEST;

	@BeforeClass
	public static void init() throws Exception {
		initContext();
		keyspace.createColumnFamily(CF_COLUMN_RANGE_TEST, null);
		CF_COLUMN_RANGE_TEST.describe(keyspace);
	}
	
	@AfterClass
	public static void tearDown() throws Exception {
		keyspace.dropColumnFamily(CF_COLUMN_RANGE_TEST);
	}

	@Test
	public void runAllTests() throws Exception {
		
		boolean rowDeleted = false;
		
		TestUtils.populateRowsForColumnRange(keyspace);
		Thread.sleep(1000);
		
		paginateColumnsForAllRows(rowDeleted);

		TestUtils.deleteRowsForColumnRange(keyspace);
		Thread.sleep(1000);
		rowDeleted = true;

		paginateColumnsForAllRows(rowDeleted);
	}
	
	private void paginateColumnsForAllRows(boolean rowDeleted) throws Exception {
		
		Random random = new Random();
		
		char ch = 'A';
		while (ch <= 'Z') {
			int pageSize = random.nextInt(26) % 10;
			if (pageSize <= 0) {
				pageSize = 10;
			}
			paginateColumnsForRowKey(String.valueOf(ch), rowDeleted, pageSize);
			ch++;
		}
	}

	private void paginateColumnsForRowKey(String rowKey, boolean rowDeleted, int pageSize) throws Exception {
		ColumnList<String> columns;
		
		RowQuery<String, String> query = keyspace
				.prepareQuery(TestUtils.CF_COLUMN_RANGE_TEST)
				.getKey(rowKey)
				.autoPaginate(true)
				.withColumnRange(
						new CqlRangeBuilder<String>().setStart("a")
						.setFetchSize(pageSize).build());

		int count = 1; 
		while (!(columns = query.execute().getResult()).isEmpty()) {
			Assert.assertTrue(columns.size() <= pageSize);
			
			for (Column<String> col : columns) {
				int value = col.getName().charAt(0) - 'a' + 1;
				Assert.assertEquals(count, value);
				count++;
			}
		}
		
		if (rowDeleted) {
			Assert.assertTrue(count == 1);
		}
	}

}>>>>>>> YOURS
/home/ramdisk/experiment3/projects/astyanax/revisions/rev_2b15fd9_3de01e3/rev_2b15fd9-3de01e3/astyanax-test/src/main/java/com/netflix/astyanax/cql/test/MockCompositeTypeTests.java;<<<<<<< MINE
=======
package com.netflix.astyanax.cql.test;

import junit.framework.Assert;

import org.apache.log4j.Logger;
import org.junit.AfterClass;
import org.junit.BeforeClass;
import org.junit.Test;

import com.google.common.collect.ImmutableMap;
import com.netflix.astyanax.ColumnListMutation;
import com.netflix.astyanax.MutationBatch;
import com.netflix.astyanax.annotations.Component;
import com.netflix.astyanax.connectionpool.OperationResult;
import com.netflix.astyanax.model.Column;
import com.netflix.astyanax.model.ColumnFamily;
import com.netflix.astyanax.model.ColumnList;
import com.netflix.astyanax.serializers.AnnotatedCompositeSerializer;
import com.netflix.astyanax.serializers.StringSerializer;

public class MockCompositeTypeTests extends KeyspaceTests {

	private static final Logger LOG = Logger.getLogger(MockCompositeTypeTests.class);
	
	@BeforeClass
	public static void init() throws Exception {
		initContext();
		
        keyspace.createColumnFamily(CF_COMPOSITE, ImmutableMap.<String, Object>builder()
                .put("comparator_type", "CompositeType(AsciiType, IntegerType, IntegerType, BytesType, UTF8Type)")
                .build());
		
		CF_COMPOSITE.describe(keyspace);
	}
	
	@AfterClass
	public static void tearDown() throws Exception {
		keyspace.dropColumnFamily(CF_COMPOSITE);
	}

	private static AnnotatedCompositeSerializer<MockCompositeType> M_SERIALIZER 
    	= new AnnotatedCompositeSerializer<MockCompositeType>(MockCompositeType.class);

    private static ColumnFamily<String, MockCompositeType> CF_COMPOSITE 
    	= ColumnFamily.newColumnFamily("mockcompositetype", StringSerializer.get(), M_SERIALIZER);
	
    
    @Test
    public void testComposite() throws Exception {
        String rowKey = "Composite1";

        boolean bool = false;
        MutationBatch m = keyspace.prepareMutationBatch();
        ColumnListMutation<MockCompositeType> mRow = m.withRow(CF_COMPOSITE, rowKey);
        int columnCount = 0;
        for (char part1 = 'a'; part1 <= 'b'; part1++) {
            for (int part2 = 0; part2 < 10; part2++) {
                for (int part3 = 10; part3 < 11; part3++) {
                    bool = !bool;
                    columnCount++;
                    mRow.putEmptyColumn(
                            new MockCompositeType(Character.toString(part1),
                                    part2, part3, bool, "UTF"), null);
                }
            }
        }
        m.execute();
        LOG.info("Created " + columnCount + " columns");

        OperationResult<ColumnList<MockCompositeType>> result;
        
        result = keyspace.prepareQuery(CF_COMPOSITE).getKey(rowKey).execute();
        Assert.assertEquals(columnCount,  result.getResult().size());
        for (Column<MockCompositeType> col : result.getResult()) {
        	LOG.info("COLUMN: " + col.getName().toString());
        }

        Column<MockCompositeType> column = keyspace.prepareQuery(CF_COMPOSITE).getKey(rowKey)
        		.getColumn(new MockCompositeType("a", 0, 10, true, "UTF"))
        		.execute().getResult();
        LOG.info("Got single column: " + column.getName().toString());
        Assert.assertNotNull(column);
        Assert.assertEquals("MockCompositeType[a,0,10,true,UTF]", column.getName().toString());

        LOG.info("Range builder");
        result = keyspace
        		.prepareQuery(CF_COMPOSITE)
        		.getKey(rowKey)
        		.withColumnRange(
        				M_SERIALIZER
        				.buildRange()
        				.withPrefix("a")
        				.greaterThanEquals(1)
        				.lessThanEquals(1)
        				.build()).execute();
        Assert.assertTrue(1 == result.getResult().size());
        for (Column<MockCompositeType> col : result.getResult()) {
        	LOG.info("COLUMN: " + col.getName().toString());
        	Assert.assertEquals("MockCompositeType[a,1,10,false,UTF]", col.getName().toString());
        }
    }
    
	public static class MockCompositeType {
	    @Component
	    private String stringPart;

	    @Component
	    private Integer intPart;

	    @Component
	    private Integer intPart2;

	    @Component
	    private boolean boolPart;

	    @Component
	    private String utf8StringPart;

	    public MockCompositeType() {

	    }

	    public MockCompositeType(String part1, Integer part2, Integer part3,
	            boolean boolPart, String utf8StringPart) {
	        this.stringPart = part1;
	        this.intPart = part2;
	        this.intPart2 = part3;
	        this.boolPart = boolPart;
	        this.utf8StringPart = utf8StringPart;
	    }

	    public MockCompositeType setStringPart(String part) {
	        this.stringPart = part;
	        return this;
	    }

	    public String getStringPart() {
	        return this.stringPart;
	    }

	    public MockCompositeType setIntPart1(int value) {
	        this.intPart = value;
	        return this;
	    }

	    public int getIntPart1() {
	        return this.intPart;
	    }

	    public MockCompositeType setIntPart2(int value) {
	        this.intPart2 = value;
	        return this;
	    }

	    public int getIntPart2() {
	        return this.intPart2;
	    }

	    public MockCompositeType setBoolPart(boolean boolPart) {
	        this.boolPart = boolPart;
	        return this;
	    }

	    public boolean getBoolPart() {
	        return this.boolPart;
	    }

	    public MockCompositeType setUtf8StringPart(String str) {
	        this.utf8StringPart = str;
	        return this;
	    }

	    public String getUtf8StringPart() {
	        return this.utf8StringPart;
	    }

	    public String toString() {
	        return new StringBuilder().append("MockCompositeType[")
	                .append(stringPart).append(',').append(intPart).append(',')
	                .append(intPart2).append(',').append(boolPart).append(',')
	                .append(utf8StringPart).append(']').toString();
	    }
	}
}>>>>>>> YOURS
/home/ramdisk/experiment3/projects/astyanax/revisions/rev_2b15fd9_3de01e3/rev_2b15fd9-3de01e3/astyanax-test/src/main/java/com/netflix/astyanax/cql/test/SerializerPackageTests.java;<<<<<<< MINE
=======
package com.netflix.astyanax.cql.test;

import java.nio.ByteBuffer;
import java.util.ArrayList;
import java.util.List;

import org.apache.cassandra.db.marshal.AbstractType;
import org.apache.cassandra.db.marshal.CompositeType;
import org.apache.cassandra.db.marshal.UTF8Type;
import org.apache.log4j.Logger;
import org.junit.AfterClass;
import org.junit.Assert;
import org.junit.BeforeClass;
import org.junit.Test;

import com.netflix.astyanax.SerializerPackage;
import com.netflix.astyanax.model.ColumnFamily;
import com.netflix.astyanax.model.Composite;
import com.netflix.astyanax.serializers.LongSerializer;
import com.netflix.astyanax.serializers.SpecificCompositeSerializer;
import com.netflix.astyanax.serializers.StringSerializer;

public class SerializerPackageTests extends KeyspaceTests {

	private static final Logger LOG = Logger.getLogger(SerializerPackageTests.class);

	public static ColumnFamily<String, Long> CF_SERIALIZER1 = ColumnFamily
			.newColumnFamily(
					"Serializer1", 
					StringSerializer.get(),
					LongSerializer.get());

	@BeforeClass
	public static void init() throws Exception {
		initContext();

		keyspace.prepareQuery(CF_SERIALIZER1)
		.withCql("CREATE TABLE astyanaxunittests.serializer1 (key text, column1 bigint, value text, PRIMARY KEY (key))")
		.execute();

		CF_SERIALIZER1.describe(keyspace);
	}

	@AfterClass
	public static void tearDown() throws Exception {
		keyspace.dropColumnFamily(CF_SERIALIZER1);
	}

	@Test
	public void testSerializer() throws Exception {

		keyspace.prepareQuery(CF_SERIALIZER1)
		.withCql("select * from astyanaxunittests.serializer1")
		.execute();

		SerializerPackage serializer = keyspace.getSerializerPackage("Serializer1", false);

		System.out.println("");
		System.out.println("KeySerializer: " + serializer.getKeySerializer());
		System.out.println("ColumnNameSerializer: " + serializer.getColumnNameSerializer());
		System.out.println("ColumnSerializer: " + serializer.getColumnSerializer());
		System.out.println("DefaultValueSerializer: " + serializer.getDefaultValueSerializer());
		System.out.println("ValueSerializer: " + serializer.getValueSerializer());

		String ss1 = "ss1"; 
		ByteBuffer bb1 = StringSerializer.get().fromString(ss1);
		String ss1Result = serializer.getKeySerializer().getString(bb1);

		System.out.println("ss1Result: " + ss1Result);
		Assert.assertEquals(ss1, ss1Result);

		SpecificCompositeSerializer comp = (SpecificCompositeSerializer) serializer.getColumnNameSerializer();
		System.out.println(comp.getComparators().toString());

		Composite dc = new Composite(ss1);

		List<AbstractType<?>> types = new ArrayList<AbstractType<?>>();
		types.add(UTF8Type.instance);

		CompositeType c1 = CompositeType.getInstance(types);

		SpecificCompositeSerializer ccSerializer = new SpecificCompositeSerializer(c1);
		ByteBuffer bb2 = ccSerializer.toByteBuffer(dc);

		Composite c2 = (Composite) serializer.getColumnNameSerializer().fromByteBuffer(bb2);
		ss1Result =  (String) c2.get(0);

		Assert.assertEquals(ss1, ss1Result);
	}
}>>>>>>> YOURS
/home/ramdisk/experiment3/projects/astyanax/revisions/rev_2b15fd9_3de01e3/rev_2b15fd9-3de01e3/astyanax-test/src/main/java/com/netflix/astyanax/cql/test/KeyspaceTests.java;<<<<<<< MINE
=======
package com.netflix.astyanax.cql.test;

import org.apache.log4j.PropertyConfigurator;

import com.netflix.astyanax.AstyanaxContext;
import com.netflix.astyanax.Keyspace;
import com.netflix.astyanax.cql.test.utils.AstyanaxContextFactory;

public class KeyspaceTests {
	
	public static AstyanaxContext<Keyspace> context;
	public static Keyspace keyspace;
    
	public KeyspaceTests() {
	}
	
    public static void initContext() throws Exception {
    	PropertyConfigurator.configure("./src/main/java/test-log4j.properties");
    	keyspace = AstyanaxContextFactory.getCachedKeyspace();
    }
}>>>>>>> YOURS
/home/ramdisk/experiment3/projects/astyanax/revisions/rev_2b15fd9_3de01e3/rev_2b15fd9-3de01e3/astyanax-test/src/main/java/com/netflix/astyanax/cql/test/SingleRowQueryTests.java;<<<<<<< MINE
=======
package com.netflix.astyanax.cql.test;

import java.nio.ByteBuffer;
import java.util.ArrayList;
import java.util.Arrays;
import java.util.Collections;
import java.util.Date;
import java.util.Iterator;
import java.util.List;
import java.util.UUID;

import junit.framework.Assert;

import org.junit.AfterClass;
import org.junit.BeforeClass;
import org.junit.Test;

import com.netflix.astyanax.cql.test.utils.ReadTests;
import com.netflix.astyanax.model.Column;
import com.netflix.astyanax.model.ColumnList;
import com.netflix.astyanax.serializers.BytesArraySerializer;

public class SingleRowQueryTests extends ReadTests {

	private int TestRowCount = 10;

	@BeforeClass
	public static void init() throws Exception {
		initContext();
		keyspace.createColumnFamily(CF_USER_INFO, null);
		CF_USER_INFO.describe(keyspace);
	}
	
	@AfterClass
	public static void tearDown() throws Exception {
		keyspace.dropColumnFamily(CF_USER_INFO);
	}

	@Test
	public void runAllTests() throws Exception {

		/** POPULATE ROWS FOR READ TESTS */

		populateRows(TestRowCount);  // NOTE THAT WE ARE UING USER_INFO CF
		Thread.sleep(1000);
		boolean rowDeleted = false; 

		/** NOW READ BACK THE COLUMNS FOR EACH ROW */

		testSingleRowAllColumnsQuery(rowDeleted); 
		testSingleRowSingleColumnQuery(rowDeleted);
		testSingleRowColumnSliceQueryWithCollection(rowDeleted);
		testSingleRowColumnSliceQueryVarArgs(rowDeleted);
		testSingleRowAllColumnsColumnCountQuery(rowDeleted);
		testSingleRowColumnSliceCollectionColumnCountQuery(rowDeleted);
		testSingleRowColumnSliceVarArgsColumnCountQuery(rowDeleted);

		/** NOW DELETE THE ROWS */ 

		deleteRows(TestRowCount);
		Thread.sleep(1000);
		rowDeleted = true;

		/** NOW ISSUE THE SAME QUERY BUT VERIFY THAT THE RESULTS ARE EMPTY */

		testSingleRowAllColumnsQuery(rowDeleted); 
		testSingleRowSingleColumnQuery(rowDeleted);
		testSingleRowColumnSliceQueryWithCollection(rowDeleted);
		testSingleRowColumnSliceQueryVarArgs(rowDeleted);
		testSingleRowAllColumnsColumnCountQuery(rowDeleted);
		testSingleRowColumnSliceCollectionColumnCountQuery(rowDeleted);
		testSingleRowColumnSliceVarArgsColumnCountQuery(rowDeleted);
	}

    private void testSingleRowAllColumnsQuery(boolean rowDeleted) throws Exception {
    	
    	String[] arr = {"firstname", "lastname", "address","age","ageShort", "ageLong","percentile", "married","single", "birthdate", "bytes", "uuid", "empty"};
    	List<String> columnNames = new ArrayList<String>(Arrays.asList(arr));
    	Collections.sort(columnNames);
    	
        /** NOW READ 1000 ROWS BACK */
    	
    	/**
    	 * READ BY COLUMN NAME
    	 */
        for (int i=0; i<TestRowCount; i++) {

        	ColumnList<String> response = keyspace.prepareQuery(CF_USER_INFO).getRow("acct_" + i).execute().getResult();

        	if (rowDeleted) {
            	Assert.assertTrue(response.isEmpty());
            	continue;
        	}
        	
        	Assert.assertFalse(response.isEmpty());
        	
        	List<String> columnNameList = new ArrayList<String>(response.getColumnNames());
        	Collections.sort(columnNameList);
        	
        	Assert.assertEquals(columnNames, columnNameList);
        	Date date = OriginalDate.plusMinutes(i).toDate();

        	testColumnValue(response, "firstname", columnNames, "john_" + i);
        	testColumnValue(response, "lastname", columnNames, "smith_" + i);
        	testColumnValue(response, "address", columnNames, "john smith address " + i);
        	testColumnValue(response, "age", columnNames, 30 + i);
        	testColumnValue(response, "ageShort", columnNames, new Integer(30+i).shortValue());
        	testColumnValue(response, "ageLong", columnNames, new Integer(30+i).longValue());
        	testColumnValue(response, "percentile", columnNames, 30.1);
        	testColumnValue(response, "married", columnNames, true);
        	testColumnValue(response, "single", columnNames, false);
        	testColumnValue(response, "birthdate", columnNames, date);
        	testColumnValue(response, "bytes", columnNames, TestBytes);
        	testColumnValue(response, "uuid", columnNames, TestUUID);
        	testColumnValue(response, "empty", columnNames, null);
        	
        	/** TEST THE ITERATOR INTERFACE */
        	Iterator<Column<String>> iter = response.iterator();
        	Iterator<String> columnNameIter = columnNames.iterator();
        	while (iter.hasNext()) {
        		Column<String> col = iter.next();
        		String columnName = columnNameIter.next();
        		Assert.assertEquals(columnName, col.getName());
        	}
        }
    }

    private void testSingleRowSingleColumnQuery(boolean rowDeleted) throws Exception {
    	
        /** NOW READ 1000 ROWS BACK */
    	
    	/**
    	 * READ BY COLUMN NAME
    	 */
        for (int i=0; i<TestRowCount; i++) {

        	Date date = OriginalDate.plusMinutes(i).toDate();

        	Column<String> column = keyspace.prepareQuery(CF_USER_INFO).getRow("acct_" + i) .getColumn("firstname").execute().getResult();
        	
        	if (rowDeleted) {
        		Assert.assertNull(column);
        		continue;
        	} else {
        		Assert.assertTrue(column.hasValue());
        	}
        	
        	testColumnValue(column, "john_" + i);
        	column = keyspace.prepareQuery(CF_USER_INFO).getRow("acct_" + i) .getColumn("lastname").execute().getResult();
        	testColumnValue(column, "smith_" + i);
        	column = keyspace.prepareQuery(CF_USER_INFO).getRow("acct_" + i) .getColumn("address").execute().getResult();
        	testColumnValue(column, "john smith address " + i);
        	column = keyspace.prepareQuery(CF_USER_INFO).getRow("acct_" + i) .getColumn("age").execute().getResult();
        	testColumnValue(column, 30 + i);
        	column = keyspace.prepareQuery(CF_USER_INFO).getRow("acct_" + i) .getColumn("ageShort").execute().getResult();
        	testColumnValue(column, new Integer(30+i).shortValue());
        	column = keyspace.prepareQuery(CF_USER_INFO).getRow("acct_" + i) .getColumn("ageLong").execute().getResult();
        	testColumnValue(column, new Integer(30+i).longValue());
        	column = keyspace.prepareQuery(CF_USER_INFO).getRow("acct_" + i) .getColumn("percentile").execute().getResult();
        	testColumnValue(column, 30.1);
        	column = keyspace.prepareQuery(CF_USER_INFO).getRow("acct_" + i) .getColumn("married").execute().getResult();
        	testColumnValue(column, true);
        	column = keyspace.prepareQuery(CF_USER_INFO).getRow("acct_" + i) .getColumn("single").execute().getResult();
        	testColumnValue(column, false);
        	column = keyspace.prepareQuery(CF_USER_INFO).getRow("acct_" + i) .getColumn("birthdate").execute().getResult();
        	testColumnValue(column, date);
        	column = keyspace.prepareQuery(CF_USER_INFO).getRow("acct_" + i) .getColumn("bytes").execute().getResult();
        	testColumnValue(column, TestBytes);
        	column = keyspace.prepareQuery(CF_USER_INFO).getRow("acct_" + i) .getColumn("uuid").execute().getResult();
        	testColumnValue(column, TestUUID);
        	column = keyspace.prepareQuery(CF_USER_INFO).getRow("acct_" + i) .getColumn("empty").execute().getResult();
        	testColumnValue(column, null);
        }
    }
    



	private void testSingleRowColumnSliceQueryWithCollection(boolean rowDeleted) throws Exception {
    	
    	/**
    	 * READ BY COLUMN SLICE COLLECTION
    	 */
        for (int i=0; i<TestRowCount; i++) {

        	Date date = OriginalDate.plusMinutes(i).toDate();

        	ColumnList<String> response = keyspace.prepareQuery(CF_USER_INFO).getRow("acct_" + i).withColumnSlice(columnNames).execute().getResult();
        	
        	if (rowDeleted) {
        		Assert.assertTrue(response.isEmpty());
        		continue;
        	} else {
        		Assert.assertFalse(response.isEmpty());
        	}
        	
        	testColumnValue(response, "firstname", columnNames, "john_" + i);
        	testColumnValue(response, "lastname", columnNames, "smith_" + i);
        	testColumnValue(response, "address", columnNames, "john smith address " + i);
        	testColumnValue(response, "age", columnNames, 30 + i);
        	testColumnValue(response, "ageShort", columnNames, new Integer(30+i).shortValue());
        	testColumnValue(response, "ageLong", columnNames, new Integer(30+i).longValue());
        	testColumnValue(response, "percentile", columnNames, 30.1);
        	testColumnValue(response, "married", columnNames, true);
        	testColumnValue(response, "single", columnNames, false);
        	testColumnValue(response, "birthdate", columnNames, date);
        	testColumnValue(response, "bytes", columnNames, TestBytes);
        	testColumnValue(response, "uuid", columnNames, TestUUID);
        	testColumnValue(response, "empty", columnNames, null);
        }
    }

    private void testSingleRowColumnSliceQueryVarArgs(boolean rowDeleted) throws Exception {
    	
    	/**
    	 * READ BY COLUMN SLICE COLLECTION
    	 */
        for (int i=0; i<TestRowCount; i++) {

        	Date date = OriginalDate.plusMinutes(i).toDate();

        	ColumnList<String> response = keyspace.prepareQuery(CF_USER_INFO).getRow("acct_" + i)
        			.withColumnSlice("firstname", "lastname", "address","age","ageShort", "ageLong","percentile", "married","single", "birthdate", "bytes", "uuid", "empty").execute().getResult();
        	
        	if (rowDeleted) {
        		Assert.assertTrue(response.isEmpty());
        		continue;
        	} else {
        		Assert.assertFalse(response.isEmpty());
        	}

        	testColumnValue(response, "firstname", columnNames, "john_" + i);
        	testColumnValue(response, "lastname", columnNames, "smith_" + i);
        	testColumnValue(response, "address", columnNames, "john smith address " + i);
        	testColumnValue(response, "age", columnNames, 30 + i);
        	testColumnValue(response, "ageShort", columnNames, new Integer(30+i).shortValue());
        	testColumnValue(response, "ageLong", columnNames, new Integer(30+i).longValue());
        	testColumnValue(response, "percentile", columnNames, 30.1);
        	testColumnValue(response, "married", columnNames, true);
        	testColumnValue(response, "single", columnNames, false);
        	testColumnValue(response, "birthdate", columnNames, date);
        	testColumnValue(response, "bytes", columnNames, TestBytes);
        	testColumnValue(response, "uuid", columnNames, TestUUID);
        	testColumnValue(response, "empty", columnNames, null);
        }
    }

	private void testSingleRowAllColumnsColumnCountQuery(boolean rowDeleted) throws Exception {
		int expected = rowDeleted ? 0 : columnNames.size();
        for (int i=0; i<TestRowCount; i++) {
        	int count = keyspace.prepareQuery(CF_USER_INFO).getRow("acct_" + i).getCount().execute().getResult().intValue();
        	Assert.assertEquals(expected, count);
        }
	}
	
	private void testSingleRowColumnSliceCollectionColumnCountQuery(boolean rowDeleted) throws Exception {
		int expected = rowDeleted ? 0 : columnNames.size();
        for (int i=0; i<TestRowCount; i++) {
        	int count = keyspace.prepareQuery(CF_USER_INFO).getRow("acct_" + i).withColumnSlice(columnNames).getCount().execute().getResult();
        	Assert.assertEquals(expected, count);
        }
	}

	private void testSingleRowColumnSliceVarArgsColumnCountQuery(boolean rowDeleted) throws Exception {
		int expected = rowDeleted ? 0 : columnNames.size();
        for (int i=0; i<TestRowCount; i++) {
        	int count = keyspace.prepareQuery(CF_USER_INFO).getRow("acct_" + i)
        			.withColumnSlice("firstname", "lastname", "address","age","ageShort", "ageLong","percentile", "married","single", "birthdate", "bytes", "uuid", "empty")
        			.getCount().execute().getResult();
        	Assert.assertEquals(expected, count);
        }
	}

    private <T> void testColumnValue(ColumnList<String> response, String columnName, List<String> columnNames, T value) {
    	
    	// by column name
    	Column<String> column = response.getColumnByName(columnName);
    	Assert.assertEquals(columnName, column.getName());
    	testColumnValue(column, value);
    }
    
    private <T> void testColumnValue(Column<String> column, T value) {

    	// Check the column name
    	// check if value exists
    	if (value != null) {
    		Assert.assertTrue(column.hasValue());
    		if (value instanceof String) {
        		Assert.assertEquals(value, column.getStringValue());
    		} else if (value instanceof Integer) {
        		Assert.assertEquals(value, column.getIntegerValue());
    		} else if (value instanceof Short) {
        		Assert.assertEquals(value, column.getShortValue());
    		} else if (value instanceof Long) {
        		Assert.assertEquals(value, column.getLongValue());
    		} else if (value instanceof Double) {
        		Assert.assertEquals(value, column.getDoubleValue());
    		} else if (value instanceof Boolean) {
        		Assert.assertEquals(value, column.getBooleanValue());
    		} else if (value instanceof Date) {
        		Assert.assertEquals(value, column.getDateValue());
    		} else if (value instanceof byte[]) {
    			ByteBuffer bbuf = column.getByteBufferValue();
    			String result = new String(BytesArraySerializer.get().fromByteBuffer(bbuf));
    			Assert.assertEquals(new String((byte[])value), result);
    		} else if (value instanceof UUID) {
        		Assert.assertEquals(value, column.getUUIDValue());
    		} else {
    			Assert.fail("Value not recognized for column: " + column.getName()); 
    		}
    	} else {
    		// check that value does not exist
    		Assert.assertFalse(column.hasValue());
    	}
    }
}>>>>>>> YOURS
/home/ramdisk/experiment3/projects/astyanax/revisions/rev_2b15fd9_3de01e3/rev_2b15fd9-3de01e3/astyanax-test/src/main/java/com/netflix/astyanax/cql/test/RingDescribeTests.java;<<<<<<< MINE
=======
package com.netflix.astyanax.cql.test;

import java.util.List;

import org.apache.log4j.Logger;
import org.junit.BeforeClass;
import org.junit.Test;

import com.netflix.astyanax.connectionpool.TokenRange;

public class RingDescribeTests extends KeyspaceTests {
	
	private static final Logger LOG = Logger.getLogger(RingDescribeTests.class);
	
    @BeforeClass
	public static void init() throws Exception {
		initContext();
	}
	
    @Test
    public void testDescribeRing() throws Exception {
    	// [TokenRangeImpl [startToken=0, endToken=0, endpoints=[127.0.0.1]]]
    	List<TokenRange> ring = keyspace.describeRing();
    	LOG.info(ring.toString());
    }
}>>>>>>> YOURS
/home/ramdisk/experiment3/projects/astyanax/revisions/rev_2b15fd9_3de01e3/rev_2b15fd9-3de01e3/astyanax-test/src/main/java/com/netflix/astyanax/cql/test/CompositeColumnTests.java;<<<<<<< MINE
=======
package com.netflix.astyanax.cql.test;

import java.util.ArrayList;
import java.util.Arrays;
import java.util.HashMap;
import java.util.List;
import java.util.Map;
import java.util.Random;

import junit.framework.Assert;

import org.junit.AfterClass;
import org.junit.BeforeClass;
import org.junit.Test;

import com.netflix.astyanax.MutationBatch;
import com.netflix.astyanax.annotations.Component;
import com.netflix.astyanax.model.Column;
import com.netflix.astyanax.model.ColumnFamily;
import com.netflix.astyanax.model.ColumnList;
import com.netflix.astyanax.model.Row;
import com.netflix.astyanax.model.Rows;
import com.netflix.astyanax.serializers.AnnotatedCompositeSerializer;
import com.netflix.astyanax.serializers.IntegerSerializer;

public class CompositeColumnTests extends KeyspaceTests {

	private static AnnotatedCompositeSerializer<Population> compSerializer = new AnnotatedCompositeSerializer<Population>(Population.class);

	private static ColumnFamily<Integer, Population> CF_POPULATION = 
			new ColumnFamily<Integer, Population>("population", IntegerSerializer.get(), compSerializer, IntegerSerializer.get());

	@BeforeClass
	public static void init() throws Exception {
		initContext();
		keyspace.createColumnFamily(CF_POPULATION,     null);
		CF_POPULATION.describe(keyspace);
	}
	
	@AfterClass
	public static void teardown() throws Exception {
		keyspace.dropColumnFamily(CF_POPULATION);
	}

	@Test
	public void runAllTests() throws Exception {
		
		boolean rowDeleted = false;
		
		populateRowsForCFPopulation();
		Thread.sleep(1000);
		
		/** READ SINGLE ROW QUERIES */
		testReadSingleRowAllColumns(rowDeleted);
		testReadSingleRowSingleColumn(rowDeleted);
		testReadSingleRowColumnRange(rowDeleted);
		
		/** READ ROW SLICE WITH ROW KEYS */
		testReadMultipleRowKeysWithAllColumns(rowDeleted);
		testReadMultipleRowKeysWithColumnRange(rowDeleted);
		
		/** READ ROW SLICE WITH ROWS RANGE */
		testReadRowRangeWithAllColumns(rowDeleted);
		testReadRowRangeWithColumnRange(rowDeleted);
		
		/** ALL ROW COUNT QUERIES */
		testReadSingleRowAllColumnsWithColumnCount(rowDeleted);
		testReadSingleRowColumnRangeWithColumnCount(rowDeleted);
		testReadMultipleRowKeysAllColumnsWithColumnCount(rowDeleted);
		testReadMultipleRowKeysColumnRangeWithColumnCount(rowDeleted);
		testReadRowRangeAllColumnsWithColumnCount(rowDeleted);
		testReadRowRangeColumnRangeWithColumnCount(rowDeleted);

		deleteRowsForCFPopulation(); 
		Thread.sleep(1000);
		rowDeleted = true; 
		
		/** READ SINGLE ROW QUERIES */
		testReadSingleRowAllColumns(rowDeleted);
		testReadSingleRowSingleColumn(rowDeleted);
		testReadSingleRowColumnRange(rowDeleted);
		
		/** READ ROW SLICE WITH ROW KEYS */
		testReadMultipleRowKeysWithAllColumns(rowDeleted);
		testReadMultipleRowKeysWithColumnRange(rowDeleted);
		
		/** READ ROW SLICE WITH ROWS RANGE */
		testReadRowRangeWithAllColumns(rowDeleted);
		testReadRowRangeWithColumnRange(rowDeleted);
		
		/** ALL ROW COUNT QUERIES */
		testReadSingleRowAllColumnsWithColumnCount(rowDeleted);
		testReadSingleRowColumnRangeWithColumnCount(rowDeleted);
		testReadMultipleRowKeysAllColumnsWithColumnCount(rowDeleted);
		testReadMultipleRowKeysColumnRangeWithColumnCount(rowDeleted);
		testReadRowRangeAllColumnsWithColumnCount(rowDeleted);
		testReadRowRangeColumnRangeWithColumnCount(rowDeleted);
	}
	
	private void populateRowsForCFPopulation() throws Exception {
		
		MutationBatch m = keyspace.prepareMutationBatch(); 
		
		Random random = new Random();
		
		for (int year = 2001; year <= 2014; year++) {
			
			m.withRow(CF_POPULATION, year)
				.putColumn(NewYork.clone(), random.nextInt(25000))
				.putColumn(SanDiego.clone(), random.nextInt(25000))
				.putColumn(SanFrancisco.clone(), random.nextInt(25000))
				.putColumn(Seattle.clone(), random.nextInt(25000));
		}
		
		m.execute();
	}

	private void deleteRowsForCFPopulation() throws Exception {
		
		MutationBatch m = keyspace.prepareMutationBatch(); 
		
		for (int year = 2001; year <= 2014; year ++) {
			m.withRow(CF_POPULATION, year).delete();
		}
		
		m.execute();
	}

	private void testReadSingleRowAllColumns(boolean rowDeleted) throws Exception {
		
		for (int year = 2001; year <= 2014; year++) {
			ColumnList<Population> result = keyspace.prepareQuery(CF_POPULATION)
													.getRow(year)
													.execute().getResult();
			if (rowDeleted) {
				Assert.assertTrue(result.isEmpty());
				continue;
			} else {
				checkResult(result, SanDiego, SanFrancisco, NewYork, Seattle);
			}
		}
	}
	
	private void testReadSingleRowSingleColumn(boolean rowDeleted) throws Exception {
		
		for (int year = 2001; year <= 2014; year++) {

			Column<Population> result = keyspace.prepareQuery(CF_POPULATION)
					.getRow(year)
					.getColumn(SanFrancisco.clone())
					.execute().getResult();
		
			if (rowDeleted) {
				Assert.assertNull(result);
				continue;
			} else {
				Assert.assertTrue(result.hasValue());
			}
		
			Assert.assertEquals(SanFrancisco, result.getName());
		}
	}
	
	private void testReadSingleRowColumnRange(boolean rowDeleted) throws Exception {
		
		AnnotatedCompositeSerializer<Population> compSerializer = new AnnotatedCompositeSerializer<Population>(Population.class);
		
		for (int year = 2001; year <= 2001; year++) {

			ColumnList<Population> result = keyspace.prepareQuery(CF_POPULATION)
					.getRow(year)
					.withColumnRange(compSerializer.buildRange()
									.withPrefix("CA")
									.build())
					.execute().getResult();
		
			if (rowDeleted) {
				Assert.assertTrue(result.isEmpty());
				continue;
			} else {
				checkResult(result, SanDiego, SanFrancisco);
			}
		
			result = keyspace.prepareQuery(CF_POPULATION)
					.getRow(year)
					.withColumnRange(compSerializer.buildRange()
									.withPrefix("CA")
									.greaterThan("San Diego")
									.build())
					.execute().getResult();
		
			if (rowDeleted) {
				Assert.assertTrue(result.isEmpty());
				continue;
			} else {
				checkResult(result, SanFrancisco);
			}
			
			result = keyspace.prepareQuery(CF_POPULATION)
					.getRow(year)
					.withColumnRange(compSerializer.buildRange()
									.withPrefix("WA")
									.withPrefix("Seattle")
									.withPrefix(40000)
									.build())
					.execute().getResult();
		
			if (rowDeleted) {
				Assert.assertTrue(result.isEmpty());
				continue;
			} else {
				checkResult(result, Seattle);
			}
		}
	}
	
	private void testReadMultipleRowKeysWithAllColumns(boolean rowDeleted) throws Exception {
		
		Rows<Integer, Population> result = keyspace.prepareQuery(CF_POPULATION)
				.getKeySlice(2001, 2002, 2003, 2004, 2005)
				.execute().getResult();
		if (rowDeleted) {
			Assert.assertTrue(result.isEmpty());
		} else {
			checkRowResult(result, 2001, 5, SanDiego, SanFrancisco, NewYork, Seattle);
		}
	}
	
	private void testReadMultipleRowKeysWithColumnRange(boolean rowDeleted) throws Exception {
		
		Rows<Integer, Population> result = keyspace.prepareQuery(CF_POPULATION)
													.getKeySlice(2001, 2002, 2003, 2004, 2005)
													.withColumnRange(compSerializer.buildRange()
															.withPrefix("CA")
															.build())
											.execute().getResult();
								
		if (rowDeleted) {
			Assert.assertTrue(result.isEmpty());
		} else {
			checkRowResult(result, 2001, 5, SanDiego, SanFrancisco);
		}
								
		result = keyspace.prepareQuery(CF_POPULATION)
						 .getKeySlice(2001, 2002, 2003, 2004, 2005)
						 .withColumnRange(compSerializer.buildRange()
								 		  .withPrefix("CA")
								 		  .greaterThan("San Diego")
								 		  .build())
								 		  .execute().getResult();
								
		if (rowDeleted) {
			Assert.assertTrue(result.isEmpty());
		} else {
			checkRowResult(result, 2001, 5, SanFrancisco);
		}
									
		result = keyspace.prepareQuery(CF_POPULATION)
				 .getKeySlice(2001, 2002, 2003, 2004, 2005)
				 .withColumnRange(compSerializer.buildRange()
						 		  .withPrefix("WA")
						 		  .withPrefix("Seattle")
						 		  .withPrefix(40000)
						 		  .build())
						 		  .execute().getResult();
								
		if (rowDeleted) {
			Assert.assertTrue(result.isEmpty());
		} else {
			checkRowResult(result, 2001, 5, Seattle);
		}
	}
	
	private void testReadRowRangeWithAllColumns(boolean rowDeleted) throws Exception {

		List<TestRange> testRanges = getTestRanges();

		for (TestRange testRange : testRanges) {
			Rows<Integer, Population> result = keyspace.prepareQuery(CF_POPULATION)
					.getKeyRange(null, null, testRange.start, testRange.end, 100)
					.execute().getResult();

			if (rowDeleted) {
				Assert.assertTrue(result.isEmpty());
			} else {
				checkRowResult(result, testRange.expectedRowKeys, SanDiego, SanFrancisco, NewYork, Seattle);
			}
		}
	}

	private void testReadRowRangeWithColumnRange(boolean rowDeleted) throws Exception {

		List<TestRange> testRanges = getTestRanges();
		for (TestRange testRange : testRanges) {

			Rows<Integer, Population> result = keyspace.prepareQuery(CF_POPULATION)
					.getKeyRange(null, null, testRange.start, testRange.end, 100)
					.withColumnRange(compSerializer.buildRange()
							.withPrefix("CA")
							.build())
							.execute().getResult();

			if (rowDeleted) {
				Assert.assertTrue(result.isEmpty());
			} else {
				checkRowResult(result, testRange.expectedRowKeys, SanDiego, SanFrancisco);
			}

			result = keyspace.prepareQuery(CF_POPULATION)
					.getKeyRange(null, null, testRange.start, testRange.end, 100)
					.withColumnRange(compSerializer.buildRange()
							.withPrefix("CA")
							.greaterThan("San Diego")
							.build())
							.execute().getResult();

			if (rowDeleted) {
				Assert.assertTrue(result.isEmpty());
			} else {
				checkRowResult(result, testRange.expectedRowKeys, SanFrancisco);
			}

			result = keyspace.prepareQuery(CF_POPULATION)
					.getKeyRange(null, null, testRange.start, testRange.end, 100)
					.withColumnRange(compSerializer.buildRange()
							.withPrefix("WA")
							.withPrefix("Seattle")
							.withPrefix(40000)
							.build())
							.execute().getResult();

			if (rowDeleted) {
				Assert.assertTrue(result.isEmpty());
			} else {
				checkRowResult(result, testRange.expectedRowKeys, Seattle);
			}
		}
	}
	
	/** ALL COLUMN COUNT QUERIES */
	
	private void testReadSingleRowAllColumnsWithColumnCount(boolean rowDeleted) throws Exception {
		
		for (int year = 2001; year <= 2014; year++) {
			Integer result = keyspace.prepareQuery(CF_POPULATION)
													.getRow(year)
													.getCount()
													.execute().getResult();
			int expected = rowDeleted ? 0 : 4;
			Assert.assertTrue(expected == result.intValue());
		}
	}

	private void testReadSingleRowColumnRangeWithColumnCount(boolean rowDeleted) throws Exception {
		
		for (int year = 2001; year <= 2014; year++) {
			
			Integer result = keyspace.prepareQuery(CF_POPULATION)
												   .getRow(year)
												   .withColumnRange(compSerializer.buildRange()
														   			.withPrefix("CA")
														   			.build())
														   			.getCount()
														   			.execute().getResult();
			int expected = rowDeleted ? 0 : 2;
			Assert.assertTrue(expected == result.intValue());

			result = keyspace.prepareQuery(CF_POPULATION)
										  .getRow(year)
										  .withColumnRange(compSerializer.buildRange()
												  		   .withPrefix("CA")
												  		   .greaterThan("San Diego")
												  		   .build())
												  		   .getCount()
												  		   .execute().getResult();
			expected = rowDeleted ? 0 : 1;
			Assert.assertTrue(expected == result.intValue());

			result = keyspace.prepareQuery(CF_POPULATION)
					   						.getRow(year)
					   						.withColumnRange(compSerializer.buildRange()
					   								.withPrefix("WA")
					   								.withPrefix("Seattle")
					   								.withPrefix(40000)
					   								.build())
					   								.getCount()
					   								.execute().getResult();
			expected = rowDeleted ? 0 : 1;
			Assert.assertTrue(expected == result.intValue());
		}
	}

	private void testReadMultipleRowKeysAllColumnsWithColumnCount(boolean rowDeleted) throws Exception {

		Map<Integer, Integer> result = keyspace.prepareQuery(CF_POPULATION)
				.getKeySlice(2001, 2002, 2003, 2004, 2005)
				.getColumnCounts()
				.execute().getResult();
		Map<Integer, Integer> expected = new HashMap<Integer, Integer>();
		if (!rowDeleted) {
			for (int year = 2001; year<= 2005; year++) {
				expected.put(year, 4);
			}
		}
		Assert.assertEquals(expected, result);
	}

	private void testReadMultipleRowKeysColumnRangeWithColumnCount(boolean rowDeleted) throws Exception {

			Map<Integer, Integer> result = keyspace.prepareQuery(CF_POPULATION)
					 .getKeySlice(2001, 2002, 2003, 2004, 2005)
					 .withColumnRange(compSerializer.buildRange()
							.withPrefix("CA")
							.build())
							.getColumnCounts()
							.execute().getResult();

			Map<Integer, Integer> expected = new HashMap<Integer, Integer>();
			if (!rowDeleted) {
				for (Integer rowKey = 2001; rowKey<=2005; rowKey++) {
					expected.put(rowKey, 2);
				}
			}
			
			Assert.assertEquals(expected, result);
			
			result = keyspace.prepareQuery(CF_POPULATION)
					 .getKeySlice(2001, 2002, 2003, 2004, 2005)
					 .withColumnRange(compSerializer.buildRange()
							.withPrefix("CA")
							.greaterThan("San Diego")
							.build())
							.getColumnCounts()
							.execute().getResult();

			expected = new HashMap<Integer, Integer>();
			if (!rowDeleted) {
				for (Integer rowKey = 2001; rowKey<=2005; rowKey++) {
					expected.put(rowKey, 1);
				}
			}
			
			Assert.assertEquals(expected, result);
			
			result = keyspace.prepareQuery(CF_POPULATION)
					 .getKeySlice(2001, 2002, 2003, 2004, 2005)
					 .withColumnRange(compSerializer.buildRange()
							.withPrefix("WA")
							.withPrefix("Seattle")
							.withPrefix(40000)
							.build())
							.getColumnCounts()
							.execute().getResult();

			expected = new HashMap<Integer, Integer>();
			if (!rowDeleted) {
				for (Integer rowKey = 2001; rowKey<=2005; rowKey++) {
					expected.put(rowKey, 1);
				}
			}
			
			Assert.assertEquals(expected, result);
	}
	
	private void testReadRowRangeAllColumnsWithColumnCount(boolean rowDeleted) throws Exception {

		List<TestRange> testRanges = getTestRanges();
		
		TestRange range = testRanges.get(0);
		
		Map<Integer, Integer> result = keyspace.prepareQuery(CF_POPULATION)
				.getKeyRange(null, null, range.start, range.end, 100)
				.getColumnCounts()
				.execute().getResult();
		
		Map<Integer, Integer> expected = new HashMap<Integer, Integer>();
		if (!rowDeleted) {
			for (Integer year : range.expectedRowKeys) {
				expected.put(year, 4);
			}
		}
		Assert.assertEquals(expected, result);
	}

	private void testReadRowRangeColumnRangeWithColumnCount(boolean rowDeleted) throws Exception {

		List<TestRange> testRanges = getTestRanges();
		for (TestRange testRange : testRanges) {

			Map<Integer, Integer> result = keyspace.prepareQuery(CF_POPULATION)
					.getKeyRange(null, null, testRange.start, testRange.end, 100)
					.withColumnRange(compSerializer.buildRange()
							.withPrefix("CA")
							.build())
							.getColumnCounts()
							.execute().getResult();

			Map<Integer, Integer> expected = new HashMap<Integer, Integer>();
			if (!rowDeleted) {
				for (Integer rowKey : testRange.expectedRowKeys) {
					expected.put(rowKey, 2);
				}
			}
			
			Assert.assertEquals(expected, result);
			
			result = keyspace.prepareQuery(CF_POPULATION)
					.getKeyRange(null, null, testRange.start, testRange.end, 100)
					.withColumnRange(compSerializer.buildRange()
							.withPrefix("CA")
							.greaterThan("San Diego")
							.build())
							.getColumnCounts()
							.execute().getResult();

			expected = new HashMap<Integer, Integer>();
			if (!rowDeleted) {
				for (Integer rowKey : testRange.expectedRowKeys) {
					expected.put(rowKey, 1);
				}
			}
			
			Assert.assertEquals(expected, result);
			
			result = keyspace.prepareQuery(CF_POPULATION)
					.getKeyRange(null, null, testRange.start, testRange.end, 100)
					.withColumnRange(compSerializer.buildRange()
							.withPrefix("WA")
							.withPrefix("Seattle")
							.withPrefix(40000)
							.build())
							.getColumnCounts()
							.execute().getResult();

			expected = new HashMap<Integer, Integer>();
			if (!rowDeleted) {
				for (Integer rowKey : testRange.expectedRowKeys) {
					expected.put(rowKey, 1);
				}
			}
			
			Assert.assertEquals(expected, result);
		}
	}
	
	private void checkResult(ColumnList<Population> result,  Population ... expected) throws Exception {
		
		Assert.assertFalse(result.isEmpty());
		Assert.assertEquals(expected.length, result.size());
		int index = 0;
		for (Population p : expected) {
			Assert.assertEquals(p, result.getColumnByIndex(index++).getName());
		}
	}

	private void checkRowResult(Rows<Integer, Population> result, Integer startKey, Integer size, Population ... expected) throws Exception {
		
		int rowKey = startKey;
		for (Row<Integer, Population> row : result) {
			Assert.assertTrue(rowKey == row.getKey());
			checkResult(row.getColumns(), expected);
			rowKey++;
		}
		Assert.assertTrue("Result: " + result.size() + ", size: " + size, size == result.size());
	}
	
	private void checkRowResult(Rows<Integer, Population> result, List<Integer> rowKeys, Population ... expected) throws Exception {
		
		int index = 0;
		for (Row<Integer, Population> row : result) {
			Assert.assertEquals(rowKeys.toString() + " " + row.getKey(), rowKeys.get(index++), row.getKey());
			checkResult(row.getColumns(), expected);
		}
		Assert.assertTrue(rowKeys.size() == result.size());
	}
	
	/** TEST CITIES */
	public static Population NewYork = new Population("NY", "New York", 10000);
	public static Population SanDiego = new Population("CA", "San Diego", 20000);
	public static Population SanFrancisco = new Population("CA", "San Francisco", 30000);
	public static Population Seattle = new Population("WA", "Seattle", 40000);

	public static class Population {
		
		@Component(ordinal=0) String state;
		@Component(ordinal=1) String city;
		@Component(ordinal=2) Integer zipcode;
		
		public Population() {
		}

		public Population(String state, String city, Integer zipcode) {
			this.state = state;
			this.city = city;
			this.zipcode = zipcode;
		}

		public String toString() {
			return "Population [" + state + ", " + city + ", " + zipcode + "]";
		}

		@Override
		public int hashCode() {
			final int prime = 31;
			int result = 1;
			result = prime * result + ((state == null) ? 0 : state.hashCode());
			result = prime * result + ((city == null) ? 0 : city.hashCode());
			result = prime * result + ((zipcode == null) ? 0 : zipcode.hashCode());
			return result;
		}

		@Override
		public boolean equals(Object obj) {
			if (this == obj) return true;
			if (obj == null)return false;
			if (getClass() != obj.getClass()) return false;
			Population other = (Population) obj;
			boolean equal = true;
			equal &= (state != null) ? (state.equals(other.state)) : other.state == null; 
			equal &= (city != null) ? (city.equals(other.city)) : other.city == null; 
			equal &= (zipcode != null) ? (zipcode.equals(other.zipcode)) : other.zipcode == null;
			return equal;
		}
		
		public Population clone() {
			return new Population(state, city, zipcode);
		}
	}	
	
	/**
	 *   2014 -->  -6625834866172541556    2003 -->  -5952676706262623311    2009 -->  -4850296245464368619
	 *   2010 -->  -4012971246572234480    2005 -->  -3904377230599730913    2006 -->  -3604768136712843506 
	 *   2012 -->  -3193851331505022123    2007 -->  -797272529921810676     2001 -->   267648259961407629 
	 *   2002 -->   313927025611477591     2011 -->   2700799408278278395    2004 -->   5455601112738248795  
	 *   2013 -->   8821734684824899422    2008 -->   9033513988054576353
	*/
	
	private static class TestRange {
		
		private String start; 
		private String end; 
		private List<Integer> expectedRowKeys = new ArrayList<Integer>(); 
		
		private TestRange(String start, String end, Integer ... rows) {
			this.start = start;
			this.end = end;
			this.expectedRowKeys.addAll(Arrays.asList(rows));
		}
	}
	
	private List<TestRange> getTestRanges() {
		
		List<TestRange> list = new ArrayList<TestRange>();
		list.add(new TestRange("-6625834866172541556", "-4850296245464368619", 2014, 2003, 2009));
		list.add(new TestRange("-4012971246572234480", "-3604768136712843506", 2010, 2005, 2006));
		list.add(new TestRange("-3193851331505022123", "267648259961407629", 2012, 2007, 2001));
		list.add(new TestRange("313927025611477591", "5455601112738248795", 2002, 2011, 2004));
		list.add(new TestRange("8821734684824899422", "9033513988054576353", 2013, 2008));
		return list;
	}
}>>>>>>> YOURS
/home/ramdisk/experiment3/projects/astyanax/revisions/rev_2b15fd9_3de01e3/rev_2b15fd9-3de01e3/astyanax-test/src/main/java/com/netflix/astyanax/cql/test/RowUniquenessConstraintTest.java;<<<<<<< MINE
=======
package com.netflix.astyanax.cql.test;

import java.util.UUID;

import junit.framework.Assert;

import org.junit.AfterClass;
import org.junit.BeforeClass;
import org.junit.Test;

import com.google.common.base.Function;
import com.google.common.base.Supplier;
import com.netflix.astyanax.MutationBatch;
import com.netflix.astyanax.model.ColumnFamily;
import com.netflix.astyanax.model.ColumnList;
import com.netflix.astyanax.model.ConsistencyLevel;
import com.netflix.astyanax.recipes.uniqueness.NotUniqueException;
import com.netflix.astyanax.recipes.uniqueness.RowUniquenessConstraint;
import com.netflix.astyanax.serializers.LongSerializer;
import com.netflix.astyanax.serializers.StringSerializer;

public class RowUniquenessConstraintTest extends KeyspaceTests {

	public static ColumnFamily<Long, String> CF_UNIQUE_CONSTRAINT = ColumnFamily
			.newColumnFamily(
					"cfunique", 
					LongSerializer.get(),
					StringSerializer.get());

	@BeforeClass
	public static void init() throws Exception {
		initContext();
		keyspace.createColumnFamily(CF_UNIQUE_CONSTRAINT, null);
		CF_UNIQUE_CONSTRAINT.describe(keyspace);
	}

	@AfterClass
	public static void tearDown() throws Exception {
		keyspace.dropColumnFamily(CF_UNIQUE_CONSTRAINT);
	}


	Supplier<String> UniqueColumnSupplier = new Supplier<String>() {

		@Override
		public String get() {
			return UUID.randomUUID().toString();
		}
	};

	@Test
	public void testUnique() throws Exception {

		RowUniquenessConstraint<Long, String> unique = 
				new RowUniquenessConstraint<Long, String>(keyspace, CF_UNIQUE_CONSTRAINT, 1L, UniqueColumnSupplier)
				.withConsistencyLevel(ConsistencyLevel.CL_ONE);

		unique.acquire();

		try { 
			unique = new RowUniquenessConstraint<Long, String>(keyspace, CF_UNIQUE_CONSTRAINT, 1L, UniqueColumnSupplier)
					.withConsistencyLevel(ConsistencyLevel.CL_ONE);
			unique.acquire();
			Assert.fail("Should have gotten a non-unique ex");
		} catch (NotUniqueException e) {
			System.out.println(e.getMessage());
		}
	}

	@Test
	public void testUniqueAndRelease() throws Exception {

		RowUniquenessConstraint<Long, String> unique = 
				new RowUniquenessConstraint<Long, String>(keyspace, CF_UNIQUE_CONSTRAINT, 2L, UniqueColumnSupplier)
				.withConsistencyLevel(ConsistencyLevel.CL_ONE);

		unique.acquire();
		unique.release();

		unique = new RowUniquenessConstraint<Long, String>(keyspace, CF_UNIQUE_CONSTRAINT, 2L, UniqueColumnSupplier)
				.withConsistencyLevel(ConsistencyLevel.CL_ONE);
		unique.acquire();
	}

	@Test
	public void testUniquenessWithCustomMutation() throws Exception {

		ColumnList<String> result = keyspace.prepareQuery(CF_UNIQUE_CONSTRAINT).getRow(10L).execute().getResult();
		Assert.assertTrue(result.isEmpty());

		RowUniquenessConstraint<Long, String> unique = 
				new RowUniquenessConstraint<Long, String>(keyspace, CF_UNIQUE_CONSTRAINT, 3L, UniqueColumnSupplier)
				.withConsistencyLevel(ConsistencyLevel.CL_ONE);

		unique.acquireAndApplyMutation(new Function<MutationBatch, Boolean>() {
			public Boolean apply(MutationBatch input) {

				input.withRow(CF_UNIQUE_CONSTRAINT, 10L).putEmptyColumn("MyCustomColumn", null);
				return true;
			}
		});

		result = keyspace.prepareQuery(CF_UNIQUE_CONSTRAINT).getRow(10L).execute().getResult();
		Assert.assertFalse(result.isEmpty());
	}
}>>>>>>> YOURS
/home/ramdisk/experiment3/projects/astyanax/revisions/rev_2b15fd9_3de01e3/rev_2b15fd9-3de01e3/astyanax-test/src/main/java/com/netflix/astyanax/cql/test/RowCopierTests.java;<<<<<<< MINE
=======
package com.netflix.astyanax.cql.test;

import junit.framework.Assert;

import org.junit.AfterClass;
import org.junit.BeforeClass;
import org.junit.Test;

import com.netflix.astyanax.MutationBatch;
import com.netflix.astyanax.model.Column;
import com.netflix.astyanax.model.ColumnFamily;
import com.netflix.astyanax.model.ColumnList;
import com.netflix.astyanax.serializers.IntegerSerializer;
import com.netflix.astyanax.serializers.StringSerializer;

public class RowCopierTests extends KeyspaceTests {

	private static final ColumnFamily<Integer, String> CF_ROW_COPY = 
			new ColumnFamily<Integer, String>("testrowcopy", IntegerSerializer.get(), StringSerializer.get(), IntegerSerializer.get());
	private static final ColumnFamily<Integer, String> CF_ROW_COPY2 = 
			new ColumnFamily<Integer, String>("testrowcopy2", IntegerSerializer.get(), StringSerializer.get(), IntegerSerializer.get());

	@BeforeClass
	public static void init() throws Exception {

		initContext();
		
		keyspace.createColumnFamily(CF_ROW_COPY, null);
		keyspace.createColumnFamily(CF_ROW_COPY2, null);
		
		CF_ROW_COPY.describe(keyspace);
		CF_ROW_COPY2.describe(keyspace);
	}
	
	@AfterClass
	public static void tearDown() throws Exception {
		keyspace.dropColumnFamily(CF_ROW_COPY);
		keyspace.dropColumnFamily(CF_ROW_COPY2);
	}
	
	@Test
	public void runRowCopyTest() throws Exception {
		
		MutationBatch m = keyspace.prepareMutationBatch();
		m.withRow(CF_ROW_COPY, 10).putColumn("c1", 1).putColumn("c2", 2);
		m.execute();
		
		ColumnList<String> result = keyspace.prepareQuery(CF_ROW_COPY).getRow(10).execute().getResult();
		
		Column<String> column = result.getColumnByIndex(0);
		Assert.assertEquals("c1", column.getName());
		Assert.assertEquals(1, column.getIntegerValue());
		column = result.getColumnByIndex(1);
		Assert.assertEquals("c2", column.getName());
		Assert.assertEquals(2, column.getIntegerValue());
		
		keyspace.prepareQuery(CF_ROW_COPY).getRow(10).copyTo(CF_ROW_COPY2, 11).execute();
		
		ColumnList<String> result2 = keyspace.prepareQuery(CF_ROW_COPY2).getRow(11).execute().getResult();
		
		column = result2.getColumnByIndex(0);
		Assert.assertEquals("c1", column.getName());
		Assert.assertEquals(1, column.getIntegerValue());
		column = result2.getColumnByIndex(1);
		Assert.assertEquals("c2", column.getName());
		Assert.assertEquals(2, column.getIntegerValue());
	}
}>>>>>>> YOURS
/home/ramdisk/experiment3/projects/astyanax/revisions/rev_2b15fd9_3de01e3/rev_2b15fd9-3de01e3/astyanax-test/src/main/java/com/netflix/astyanax/cql/test/DirectCqlTests.java;<<<<<<< MINE
=======
package com.netflix.astyanax.cql.test;

import junit.framework.Assert;

import org.junit.AfterClass;
import org.junit.BeforeClass;
import org.junit.Test;

import com.netflix.astyanax.connectionpool.OperationResult;
import com.netflix.astyanax.model.ColumnFamily;
import com.netflix.astyanax.model.CqlResult;
import com.netflix.astyanax.model.Row;
import com.netflix.astyanax.query.CqlQuery;
import com.netflix.astyanax.query.PreparedCqlQuery;
import com.netflix.astyanax.serializers.IntegerSerializer;
import com.netflix.astyanax.serializers.StringSerializer;

public class DirectCqlTests extends KeyspaceTests {
	
    public static ColumnFamily<Integer, String> CF_DIRECT = ColumnFamily
            .newColumnFamily(
                    "cfdirect", 
                    IntegerSerializer.get(),
                    StringSerializer.get());
    
    public static ColumnFamily<String, String> CF_EMPTY_TABLE = ColumnFamily
            .newColumnFamily(
                    "empty_table", 
                    StringSerializer.get(),
                    StringSerializer.get(),
                    StringSerializer.get());


    @BeforeClass
	public static void init() throws Exception {
		initContext();
		
		keyspace.prepareQuery(CF_DIRECT)
        .withCql("CREATE TABLE astyanaxunittests.cfdirect ( key int, column1 text, value bigint, PRIMARY KEY (key) )")
        .execute();
		keyspace.prepareQuery(CF_EMPTY_TABLE)
        .withCql("CREATE TABLE astyanaxunittests.empty_table ( key text, column1 text, value text, PRIMARY KEY (key) )")
        .execute();
	}

    @AfterClass
	public static void tearDown() throws Exception {
		keyspace.prepareQuery(CF_DIRECT)
        .withCql("DROP TABLE astyanaxunittests.cfdirect")
        .execute();
		keyspace.prepareQuery(CF_EMPTY_TABLE)
        .withCql("DROP TABLE astyanaxunittests.empty_table")
        .execute();
	}

    @Test
    public void testCql() throws Exception {
    	
    	// INSERT VALUES 
    	CqlQuery<Integer, String> cqlQuery = keyspace
    	.prepareQuery(CF_DIRECT)
    	.withCql("INSERT INTO astyanaxunittests.cfdirect (key, column1, value) VALUES (?,?,?)");
    	
    	for (int i=0; i<10; i++) {
    		PreparedCqlQuery<Integer, String> pStatement = cqlQuery.asPreparedStatement();
    		pStatement.withIntegerValue(i).withStringValue(""+i).withLongValue(Long.valueOf(""+i)).execute();
    	}
    	
    	// TEST REGULAR CQL
    	OperationResult<CqlResult<Integer, String>> result = keyspace
    			.prepareQuery(CF_DIRECT)
    			.withCql("SELECT * FROM astyanaxunittests.cfdirect;").execute();
    	Assert.assertTrue(result.getResult().hasRows());

    	Assert.assertEquals(10, result.getResult().getRows().size());
    	Assert.assertFalse(result.getResult().hasNumber());
    	
    	for (int i=0; i<10; i++) {
    		
    		Row<Integer, String> row = result.getResult().getRows().getRow(i);
        	Assert.assertTrue(i == row.getKey());
        	Assert.assertEquals(3, row.getColumns().size());
        	
        	Integer key = row.getColumns().getIntegerValue("key", null);
        	Assert.assertTrue(Integer.valueOf(""+i) == key);

        	String column1 = row.getColumns().getStringValue("column1", null);
        	Assert.assertEquals(""+i, column1);
        	
        	Long value = row.getColumns().getLongValue("value", null);
        	Assert.assertTrue(Long.valueOf(""+i) == value);
    	}
    	
    	//  TEST CQL COUNT

    	result = keyspace
    			.prepareQuery(CF_DIRECT)
    			.withCql("SELECT count(*) FROM astyanaxunittests.cfdirect;").execute();
    	Assert.assertFalse(result.getResult().hasRows());
    	Assert.assertTrue(result.getResult().hasNumber());

    	Assert.assertTrue(10 == result.getResult().getNumber());
    }
    
    
    @Test 
    public void testEmptyTable() throws Exception {

    	CqlResult<String, String> result = keyspace.prepareQuery(CF_EMPTY_TABLE)
    	.withCql("select * from astyanaxunittests.empty_table where  key = 'blah'")
    	.execute()
    	.getResult();
    	
    	Assert.assertFalse(result.hasRows());
    	Assert.assertFalse(result.hasNumber());
    	Assert.assertTrue(0 == result.getRows().size());
    }
}>>>>>>> YOURS
/home/ramdisk/experiment3/projects/astyanax/revisions/rev_2b15fd9_3de01e3/rev_2b15fd9-3de01e3/astyanax-test/src/main/java/com/netflix/astyanax/cql/test/StaticColumnFamilyTests.java;<<<<<<< MINE
=======
package com.netflix.astyanax.cql.test;

import java.util.ArrayList;
import java.util.List;

import junit.framework.Assert;

import org.junit.AfterClass;
import org.junit.BeforeClass;
import org.junit.Test;

import com.netflix.astyanax.ColumnListMutation;
import com.netflix.astyanax.Keyspace;
import com.netflix.astyanax.MutationBatch;
import com.netflix.astyanax.model.Column;
import com.netflix.astyanax.model.ColumnFamily;
import com.netflix.astyanax.model.ColumnList;
import com.netflix.astyanax.model.Rows;
import com.netflix.astyanax.serializers.StringSerializer;

public class StaticColumnFamilyTests extends KeyspaceTests {

	private static ColumnFamily<String, String> CF_ACCOUNTS = new ColumnFamily<String, String>("accounts", StringSerializer.get(), StringSerializer.get());

	@BeforeClass
	public static void init() throws Exception {
		initContext();
		keyspace.prepareQuery(CF_ACCOUNTS)
				.withCql("CREATE TABLE astyanaxunittests.accounts (userid text PRIMARY KEY, user text, pswd text)")
				.execute();
		CF_ACCOUNTS.describe(keyspace);
	}

	@AfterClass
	public static void tearDown() throws Exception {
		keyspace.dropColumnFamily(CF_ACCOUNTS);
	}

	@Test
	public void testReadWriteOpsWithStaticNamedColumns() throws Exception {

		populateRowsForAccountsTable(keyspace);
		Thread.sleep(200);
		boolean rowDeleted = false; 

		performSimpleRowQuery(rowDeleted);
		performSimpleRowQueryWithColumnCollection(rowDeleted);
		performSimpleRowSingleColumnQuery(rowDeleted);
		performRowSliceQueryWithAllColumns(rowDeleted);
		performRowSliceQueryWithColumnSlice(rowDeleted);

		deleteRowsForAccountsTable(keyspace);
		Thread.sleep(200);
		rowDeleted = true; 

		performSimpleRowQuery(rowDeleted);
		performSimpleRowQueryWithColumnCollection(rowDeleted);
		performSimpleRowSingleColumnQuery(rowDeleted);
		performRowSliceQueryWithAllColumns(rowDeleted);
		performRowSliceQueryWithColumnSlice(rowDeleted);
	}


	private void performSimpleRowQuery(boolean rowDeleted) throws Exception {
		for (char keyName = 'A'; keyName <= 'Z'; keyName++) {
			String key = Character.toString(keyName);
			performSimpleRowQueryForRow(key, rowDeleted, key);
		}
	}

	private void performSimpleRowQueryForRow(String rowKey, boolean rowDeleted, String expectedChar) throws Exception {

		ColumnList<String> result =  keyspace.prepareQuery(CF_ACCOUNTS).getRow(rowKey).execute().getResult();

		if (rowDeleted) {
			Assert.assertTrue(result.isEmpty());
		} else {
			Assert.assertFalse(result.isEmpty());
			Column<String> col = result.getColumnByName("user");
			Assert.assertEquals("user" + expectedChar, col.getStringValue());
			col = result.getColumnByName("pswd");
			Assert.assertEquals("pswd" + expectedChar, col.getStringValue());
		}
	}

	private void performSimpleRowQueryWithColumnCollection(boolean rowDeleted) throws Exception {
		for (char keyName = 'A'; keyName <= 'Z'; keyName++) {
			String key = Character.toString(keyName);
			performSimpleRowQueryWithColumnCollectionForRow(key, rowDeleted, key);
		}
	}

	private void performSimpleRowQueryWithColumnCollectionForRow(String rowKey, boolean rowDeleted, String expectedChar) throws Exception {

		ColumnList<String> result =  keyspace.prepareQuery(CF_ACCOUNTS).getRow(rowKey).withColumnSlice("user", "pswd").execute().getResult();

		if (rowDeleted) {
			Assert.assertTrue(result.isEmpty());
		} else {
			Assert.assertFalse(result.isEmpty());
			Column<String> col = result.getColumnByName("user");
			Assert.assertEquals("user" + expectedChar, col.getStringValue());
			col = result.getColumnByName("pswd");
			Assert.assertEquals("pswd" + expectedChar, col.getStringValue());
		}

		result =  keyspace.prepareQuery(CF_ACCOUNTS).getRow(rowKey).withColumnSlice("user").execute().getResult();

		if (rowDeleted) {
			Assert.assertTrue(result.isEmpty());
		} else {
			Assert.assertFalse(result.isEmpty());
			Column<String> col = result.getColumnByName("user");
			Assert.assertEquals("user" + expectedChar, col.getStringValue());
		}

		result =  keyspace.prepareQuery(CF_ACCOUNTS).getRow(rowKey).withColumnSlice("pswd").execute().getResult();

		if (rowDeleted) {
			Assert.assertTrue(result.isEmpty());
		} else {
			Assert.assertFalse(result.isEmpty());
			Column<String> col = result.getColumnByName("pswd");
			Assert.assertEquals("pswd" + expectedChar, col.getStringValue());
		}

		List<String> cols = new ArrayList<String>();
		cols.add("user"); cols.add("pswd");

		result =  keyspace.prepareQuery(CF_ACCOUNTS).getRow(rowKey).withColumnSlice(cols).execute().getResult();

		if (rowDeleted) {
			Assert.assertTrue(result.isEmpty());
		} else {
			Assert.assertFalse(result.isEmpty());
			Column<String> col = result.getColumnByName("user");
			Assert.assertEquals("user" + expectedChar, col.getStringValue());
			col = result.getColumnByName("pswd");
			Assert.assertEquals("pswd" + expectedChar, col.getStringValue());
		}

		cols.remove("user");

		result =  keyspace.prepareQuery(CF_ACCOUNTS).getRow(rowKey).withColumnSlice(cols).execute().getResult();

		if (rowDeleted) {
			Assert.assertTrue(result.isEmpty());
		} else {
			Assert.assertFalse(result.isEmpty());
			Column<String> col = result.getColumnByName("pswd");
			Assert.assertEquals("pswd" + expectedChar, col.getStringValue());
		}
	}

	private void performSimpleRowSingleColumnQuery(boolean rowDeleted) throws Exception {
		for (char keyName = 'A'; keyName <= 'Z'; keyName++) {
			String key = Character.toString(keyName);
			performSimpleRowSingleColumnQueryForRow(key, rowDeleted, key);
		}
	}

	private void performSimpleRowSingleColumnQueryForRow(String rowKey, boolean rowDeleted, String expectedChar) throws Exception {

		Column<String> col =  keyspace.prepareQuery(CF_ACCOUNTS).getRow(rowKey).getColumn("user").execute().getResult();
		if (rowDeleted) {
			Assert.assertNull(col);
		} else {
			Assert.assertTrue(col.hasValue());
			Assert.assertEquals("user" + expectedChar, col.getStringValue());
		}

		col =  keyspace.prepareQuery(CF_ACCOUNTS).getRow(rowKey).getColumn("pswd").execute().getResult();
		if (rowDeleted) {
			Assert.assertNull(col);
		} else {
			Assert.assertTrue(col.hasValue());
			Assert.assertEquals("pswd" + expectedChar, col.getStringValue());
		}
	}

	private void performRowSliceQueryWithAllColumns(boolean rowDeleted) throws Exception {

		List<String> keys = new ArrayList<String>();
		for (char keyName = 'A'; keyName <= 'Z'; keyName++) {
			keys.add(Character.toString(keyName));
		}

		int index = 0;
		Rows<String, String> rows =  keyspace.prepareQuery(CF_ACCOUNTS).getRowSlice(keys).execute().getResult();
		if (rowDeleted) {
			Assert.assertTrue(rows.isEmpty());
		} else {
			Assert.assertFalse(rows.isEmpty());
			for (com.netflix.astyanax.model.Row<String, String> row : rows) {

				Assert.assertEquals(keys.get(index),row.getKey());

				ColumnList<String> cols = row.getColumns();
				Assert.assertFalse(cols.isEmpty());
				Column<String> col = cols.getColumnByName("user");
				Assert.assertEquals("user" + keys.get(index), col.getStringValue());
				col = cols.getColumnByName("pswd");
				Assert.assertEquals("pswd" + keys.get(index), col.getStringValue());

				index++;
			}
		}
	}

	private void performRowSliceQueryWithColumnSlice(boolean rowDeleted) throws Exception {

		List<String> keys = new ArrayList<String>();
		for (char keyName = 'A'; keyName <= 'Z'; keyName++) {
			keys.add(Character.toString(keyName));
		}

		int index = 0;
		Rows<String, String> rows =  keyspace.prepareQuery(CF_ACCOUNTS).getRowSlice(keys).withColumnSlice("user", "pswd").execute().getResult();
		if (rowDeleted) {
			Assert.assertTrue(rows.isEmpty());
		} else {
			Assert.assertFalse(rows.isEmpty());
			for (com.netflix.astyanax.model.Row<String, String> row : rows) {

				Assert.assertEquals(keys.get(index),row.getKey());

				ColumnList<String> cols = row.getColumns();
				Assert.assertFalse(cols.isEmpty());
				Column<String> col = cols.getColumnByName("user");
				Assert.assertEquals("user" + keys.get(index), col.getStringValue());
				col = cols.getColumnByName("pswd");
				Assert.assertEquals("pswd" + keys.get(index), col.getStringValue());

				index++;
			}
		}

		index=0;
		rows =  keyspace.prepareQuery(CF_ACCOUNTS).getRowSlice(keys).withColumnSlice("user").execute().getResult();
		if (rowDeleted) {
			Assert.assertTrue(rows.isEmpty());
		} else {
			Assert.assertFalse(rows.isEmpty());
			for (com.netflix.astyanax.model.Row<String, String> row : rows) {

				Assert.assertEquals(keys.get(index),row.getKey());

				ColumnList<String> cols = row.getColumns();
				Assert.assertFalse(cols.isEmpty());
				Column<String> col = cols.getColumnByName("user");
				Assert.assertEquals("user" + keys.get(index), col.getStringValue());

				index++;
			}
		}

		index=0;
		rows =  keyspace.prepareQuery(CF_ACCOUNTS).getRowSlice(keys).withColumnSlice("pswd").execute().getResult();
		if (rowDeleted) {
			Assert.assertTrue(rows.isEmpty());
		} else {
			Assert.assertFalse(rows.isEmpty());
			for (com.netflix.astyanax.model.Row<String, String> row : rows) {

				Assert.assertEquals(keys.get(index),row.getKey());

				ColumnList<String> cols = row.getColumns();
				Assert.assertFalse(cols.isEmpty());
				Column<String> col = cols.getColumnByName("pswd");
				Assert.assertEquals("pswd" + keys.get(index), col.getStringValue());

				index++;
			}
		}
	}


	public static void populateRowsForAccountsTable(Keyspace keyspace) throws Exception {

		MutationBatch m = keyspace.prepareMutationBatch();

		for (char keyName = 'A'; keyName <= 'Z'; keyName++) {
			String character = Character.toString(keyName);
			ColumnListMutation<String> colMutation = m.withRow(CF_ACCOUNTS, character);
			colMutation.putColumn("user", "user" + character).putColumn("pswd", "pswd" + character);
			m.execute();
			m.discardMutations();
		}
	}

	public static void deleteRowsForAccountsTable(Keyspace keyspace) throws Exception {

		for (char keyName = 'A'; keyName <= 'Z'; keyName++) {
			MutationBatch m = keyspace.prepareMutationBatch();
			String rowKey = Character.toString(keyName);
			m.withRow(CF_ACCOUNTS, rowKey).delete();
			m.execute();
			m.discardMutations();
		}
	}
}>>>>>>> YOURS
/home/ramdisk/experiment3/projects/astyanax/revisions/rev_2b15fd9_3de01e3/rev_2b15fd9-3de01e3/astyanax-test/src/main/java/com/netflix/astyanax/cql/test/ClickStreamTests.java;<<<<<<< MINE
=======
package com.netflix.astyanax.cql.test;

import java.util.ArrayList;
import java.util.List;
import java.util.UUID;

import org.junit.AfterClass;
import org.junit.Assert;
import org.junit.BeforeClass;
import org.junit.Test;

import com.netflix.astyanax.MutationBatch;
import com.netflix.astyanax.connectionpool.OperationResult;
import com.netflix.astyanax.model.ColumnFamily;
import com.netflix.astyanax.model.ColumnList;
import com.netflix.astyanax.serializers.AnnotatedCompositeSerializer;
import com.netflix.astyanax.serializers.StringSerializer;
import com.netflix.astyanax.test.SessionEvent;
import com.netflix.astyanax.util.TimeUUIDUtils;

public class ClickStreamTests extends KeyspaceTests {

	public static AnnotatedCompositeSerializer<SessionEvent> SE_SERIALIZER 
	= new AnnotatedCompositeSerializer<SessionEvent>(SessionEvent.class);

	public static ColumnFamily<String, SessionEvent> CF_CLICK_STREAM = 
			ColumnFamily.newColumnFamily("ClickStream", StringSerializer.get(), SE_SERIALIZER);

	@BeforeClass
	public static void init() throws Exception {
		initContext();

		keyspace.createColumnFamily(CF_CLICK_STREAM, null);
		CF_CLICK_STREAM.describe(keyspace);
	}

	@AfterClass
	public static void tearDown() throws Exception {
		keyspace.dropColumnFamily(CF_CLICK_STREAM);
	}

	@Test
	public void testClickStream() throws Exception {

		MutationBatch m = keyspace.prepareMutationBatch();
		String userId = "UserId";

		List<UUID> uuids = new ArrayList<UUID>();
		for (int j = 0; j < 10; j++) {
			uuids.add(TimeUUIDUtils.getTimeUUID(j));
		}

		long timeCounter = 0;
		for (int i = 0; i < 10; i++) {
			String sessionId = "Session" + i;

			for (int j = 0; j < 10; j++) {
				m.withRow(CF_CLICK_STREAM, userId).putColumn(
						new SessionEvent(sessionId, uuids.get(j)),
						Long.toString(timeCounter), null);
				timeCounter++;
			}
		}

		m.execute();


		OperationResult<ColumnList<SessionEvent>> result;

		result = keyspace
				.prepareQuery(CF_CLICK_STREAM)
				.getKey(userId)
				.withColumnRange(
						SE_SERIALIZER.buildRange()
						.greaterThanEquals("Session3")
						.lessThanEquals("Session5").build())
						.execute();

		Assert.assertEquals(30, result.getResult().size());

		result = keyspace
				.prepareQuery(CF_CLICK_STREAM)
				.getKey(userId)
				.withColumnRange(
						SE_SERIALIZER.buildRange()
						.greaterThanEquals("Session3")
						.lessThan("Session5").build()).execute();
		Assert.assertEquals(20, result.getResult().size());

		result = keyspace
				.prepareQuery(CF_CLICK_STREAM)
				.getKey(userId)
				.withColumnRange(
						SE_SERIALIZER.buildRange().greaterThan("Session3")
						.lessThanEquals("Session5").build())
						.execute();
		Assert.assertEquals(20, result.getResult().size());

		result = keyspace
				.prepareQuery(CF_CLICK_STREAM)
				.getKey(userId)
				.withColumnRange(
						SE_SERIALIZER.buildRange().greaterThan("Session3")
						.lessThan("Session5").build()).execute();
		Assert.assertEquals(10, result.getResult().size());

		result = keyspace
				.prepareQuery(CF_CLICK_STREAM)
				.getKey(userId)
				.withColumnRange(
						SE_SERIALIZER
						.buildRange()
						.withPrefix("Session3")
						.greaterThanEquals(uuids.get(2))
						.lessThanEquals(uuids.get(8))
						.build()).execute();

		Assert.assertEquals(7, result.getResult().size());

		result = keyspace
				.prepareQuery(CF_CLICK_STREAM)
				.getKey(userId)
				.withColumnRange(
						SE_SERIALIZER
						.buildRange()
						.withPrefix("Session3")
						.greaterThanEquals(
								TimeUUIDUtils.getTimeUUID(2))
								.lessThan(
										TimeUUIDUtils.getTimeUUID(8))
										.build()).execute();

		Assert.assertEquals(6, result.getResult().size());


		result = keyspace
				.prepareQuery(CF_CLICK_STREAM)
				.getKey(userId)
				.withColumnRange(
						SE_SERIALIZER
						.buildRange()
						.withPrefix("Session3")
						.greaterThan(
								TimeUUIDUtils.getTimeUUID(2))
								.lessThanEquals(
										TimeUUIDUtils.getTimeUUID(8))
										.build()).execute();

		Assert.assertEquals(6, result.getResult().size());

		result = keyspace
				.prepareQuery(CF_CLICK_STREAM)
				.getKey(userId)
				.withColumnRange(
						SE_SERIALIZER
						.buildRange()
						.withPrefix("Session3")
						.greaterThan(
								TimeUUIDUtils.getTimeUUID(2))
								.lessThan(
										TimeUUIDUtils.getTimeUUID(8))
										.build()).execute();

		Assert.assertEquals(5, result.getResult().size());
	}
}>>>>>>> YOURS
/home/ramdisk/experiment3/projects/astyanax/revisions/rev_2b15fd9_3de01e3/rev_2b15fd9-3de01e3/astyanax-test/src/main/java/com/netflix/astyanax/cql/test/SingleRowColumnRangeQueryTests.java;<<<<<<< MINE
=======
package com.netflix.astyanax.cql.test;

import junit.framework.Assert;

import org.junit.AfterClass;
import org.junit.BeforeClass;
import org.junit.Test;

import com.netflix.astyanax.cql.test.utils.ReadTests;
import com.netflix.astyanax.cql.test.utils.TestUtils;
import com.netflix.astyanax.model.Column;
import com.netflix.astyanax.model.ColumnFamily;
import com.netflix.astyanax.model.ColumnList;

public class SingleRowColumnRangeQueryTests extends ReadTests {

	private static ColumnFamily<String, String> CF_COLUMN_RANGE_TEST = TestUtils.CF_COLUMN_RANGE_TEST;
	
	@BeforeClass
	public static void init() throws Exception {
		initContext();
		keyspace.createColumnFamily(CF_COLUMN_RANGE_TEST, null);
		CF_COLUMN_RANGE_TEST.describe(keyspace);
	}
	
	@AfterClass
	public static void tearDown() throws Exception {
		keyspace.dropColumnFamily(CF_COLUMN_RANGE_TEST);
	}
	
	@Test
	public void testColumnRangeQuery() throws Exception {
		
		/** POPULATE DATA FOR TESTING */ 
		TestUtils.populateRowsForColumnRange(keyspace);
		Thread.sleep(1000);
		boolean rowDeleted = false;
		
		/** PERFORM READ TESTS */
		readColumnRangeForAllRows(rowDeleted);
		getColumnCountForAllRows(rowDeleted); 
		
		/** DELETE ALL ROWS */ 
		TestUtils.deleteRowsForColumnRange(keyspace);
		rowDeleted = true;

		/** PERFORM READ TESTS FOR MISSING DATA */
		readColumnRangeForAllRows(rowDeleted);
		getColumnCountForAllRows(rowDeleted); 
	}
	
	public void readColumnRangeForAllRows(boolean rowDeleted) throws Exception {
		
		char ch = 'A';
		while (ch <= 'Z') {
			readColumnRangeForRowKey(String.valueOf(ch), rowDeleted);
			ch++;
		}
	}

	private void readColumnRangeForRowKey(String rowKey, boolean rowDeleted) throws Exception {
		
		ColumnList<String> columns = keyspace
				.prepareQuery(CF_COLUMN_RANGE_TEST)
				.getKey(rowKey)
				.withColumnRange("a", "z", false, -1)
				.execute().getResult();

		if (rowDeleted) {
			Assert.assertTrue(columns.isEmpty());
			return;
		}
		
		Assert.assertFalse(columns.isEmpty());
		
		char ch = 'a';
		for (Column<String> c : columns) {
			Assert.assertEquals(String.valueOf(ch), c.getName());
			Assert.assertTrue( ch-'a'+1 == c.getIntegerValue());
			ch++;
		}
	}
	
	public void getColumnCountForAllRows(boolean rowDeleted) throws Exception {
		
		char ch = 'A';
		while (ch <= 'Z') {
			getColumnCountForRowKey(String.valueOf(ch), rowDeleted);
			ch++;
		}
	}

	private void getColumnCountForRowKey(String rowKey, boolean rowDeleted) throws Exception {
		
		Integer count = keyspace
				.prepareQuery(CF_COLUMN_RANGE_TEST)
				.getKey(rowKey)
				.withColumnRange("a", "z", false, -1)
				.getCount()
				.execute().getResult();

		int expectedCount = rowDeleted ? 0 : 26; 
		Assert.assertTrue(count.intValue() == expectedCount);
	}
}>>>>>>> YOURS
/home/ramdisk/experiment3/projects/astyanax/revisions/rev_2b15fd9_3de01e3/rev_2b15fd9-3de01e3/astyanax-test/src/main/java/com/netflix/astyanax/cql/test/SchemaTests.java;<<<<<<< MINE
=======
package com.netflix.astyanax.cql.test;

import java.util.Date;
import java.util.HashMap;
import java.util.List;
import java.util.Map;
import java.util.Properties;

import junit.framework.Assert;

import org.junit.BeforeClass;
import org.junit.Test;

import com.google.common.collect.ImmutableMap;
import com.netflix.astyanax.AstyanaxContext;
import com.netflix.astyanax.Keyspace;
import com.netflix.astyanax.annotations.Component;
import com.netflix.astyanax.cql.test.utils.AstyanaxContextFactory;
import com.netflix.astyanax.cql.test.utils.ClusterConfiguration;
import com.netflix.astyanax.cql.test.utils.ClusterConfiguration.Driver;
import com.netflix.astyanax.ddl.ColumnDefinition;
import com.netflix.astyanax.ddl.ColumnFamilyDefinition;
import com.netflix.astyanax.ddl.KeyspaceDefinition;
import com.netflix.astyanax.model.ColumnFamily;
import com.netflix.astyanax.serializers.AnnotatedCompositeSerializer;
import com.netflix.astyanax.serializers.LongSerializer;
import com.netflix.astyanax.serializers.StringSerializer;

public class SchemaTests extends KeyspaceTests {

	@BeforeClass
	public static void init() throws Exception {
		initContext();
	}
	
	@Test
	public void createKeyspaceUsingOptions() throws Exception {
		
		String keyspaceName = "AstyanaxTestKeyspaceUsingOptions".toLowerCase();
				
		AstyanaxContext<Keyspace> context = AstyanaxContextFactory.getKeyspace(keyspaceName);
		
    	context.start();
        keyspace = context.getClient();
        
		Map<String, Object> options = ImmutableMap.<String, Object>builder()
										.put("strategy_options", ImmutableMap.<String, Object>builder()
																	.put("replication_factor", "1")
																	.build())
									    .put("strategy_class",     "SimpleStrategy")
									    .build();
        
		keyspace.createKeyspace(options);
		
		Thread.sleep(1000);
		
		KeyspaceDefinition ksDef = keyspace.describeKeyspace();
		
		verifyKeyspacePropertiesForSimpleStrategy(keyspaceName, ksDef);
		
		keyspace.dropKeyspace();
		
		/** NETWORK TOPOLOGY */ 
		keyspaceName = "AstyanaxTestKeyspaceUsingOptions2".toLowerCase();
		
		context = AstyanaxContextFactory.getKeyspace(keyspaceName);
		context.start();
        keyspace = context.getClient();

		options = ImmutableMap.<String, Object>builder()
			    						.put("strategy_options", ImmutableMap.<String, Object>builder()
			    												.put("us-east", "3")
			    												.put("eu-west", "3")
			    												.build())
			    						.put("strategy_class",     "NetworkTopologyStrategy")
			    						.build();
        
		keyspace.createKeyspace(options);
		
		Thread.sleep(1000);
		
		ksDef = keyspace.describeKeyspace();
		verifyKeyspacePropertiesForNetworkTopology(keyspaceName, ksDef);
		
		keyspace.dropKeyspace();
	}
	
	@Test
	public void createKeyspaceUsingProperties() throws Exception {
		
		/** SIMPLE STRATEGY */
		String keyspaceName = "AstyanaxTestKeyspaceUsingProperties".toLowerCase();
				
		AstyanaxContext<Keyspace> context = AstyanaxContextFactory.getKeyspace(keyspaceName);
		
    	context.start();
        keyspace = context.getClient();

        Properties props = new Properties();
        props.setProperty("strategy_options.replication_factor", "1");
        props.setProperty("strategy_class", "SimpleStrategy");

        keyspace.createKeyspace(props);
		Thread.sleep(1000);
		
		KeyspaceDefinition ksDef = keyspace.describeKeyspace();
		verifyKeyspacePropertiesForSimpleStrategy(keyspaceName, ksDef);
		
		keyspace.dropKeyspace();
		
		/** NETWORK TOPOLOGY STRATEGY */
		keyspaceName = "AstyanaxTestKeyspaceUsingProperties2".toLowerCase();
		
		context = AstyanaxContextFactory.getKeyspace(keyspaceName);
		
    	context.start();
        keyspace = context.getClient();

        props = new Properties();

        props.setProperty("strategy_options.us-east", "3");
        props.setProperty("strategy_options.eu-west", "3");
        props.setProperty("strategy_class", "NetworkTopologyStrategy");

        keyspace.createKeyspace(props);
		Thread.sleep(1000);
		
		ksDef = keyspace.describeKeyspace();
		verifyKeyspacePropertiesForNetworkTopology(keyspaceName, ksDef);
		
		keyspace.dropKeyspace();
	}
	
	private void verifyKeyspacePropertiesForSimpleStrategy(String keyspaceName, KeyspaceDefinition ksDef) throws Exception {
		
		Assert.assertEquals(keyspaceName, ksDef.getName());
		Assert.assertTrue(ksDef.getStrategyClass().contains("SimpleStrategy"));

		Properties properties = ksDef.getProperties();
		Assert.assertEquals(keyspaceName, properties.getProperty("name"));
		
		Assert.assertEquals("true", properties.get("durable_writes"));
		
		String strategyClass = properties.getProperty("strategy_class");
		if (strategyClass == null) {
			strategyClass = properties.getProperty("replication.class");
		}
		Assert.assertTrue(ksDef.getStrategyClass().contains("SimpleStrategy"));
		
		Map<String, String> strategyOptions = ksDef.getStrategyOptions();
		Assert.assertEquals("1", strategyOptions.get("replication_factor"));
	}
	

	private void verifyKeyspacePropertiesForNetworkTopology(String keyspaceName, KeyspaceDefinition ksDef) throws Exception {
		
		Assert.assertEquals(keyspaceName, ksDef.getName());
		Assert.assertTrue(ksDef.getStrategyClass().contains("NetworkTopologyStrategy"));

		Properties properties = ksDef.getProperties();
		Assert.assertEquals(keyspaceName, properties.getProperty("name"));
		
		Assert.assertEquals("true", properties.get("durable_writes"));
		
		String strategyClass = properties.getProperty("strategy_class");
		if (strategyClass == null) {
			strategyClass = properties.getProperty("replication.class");
		}
		Assert.assertTrue(ksDef.getStrategyClass().contains("NetworkTopologyStrategy"));
		
		Map<String, String> strategyOptions = ksDef.getStrategyOptions();
		Assert.assertEquals("3", strategyOptions.get("us-east"));
		Assert.assertEquals("3", strategyOptions.get("eu-west"));
	}
	
	@Test
	public void createKeyspaceAndCFsUsingUsingOptions() throws Exception {
		
		String keyspaceName = "AstyanaxTestKeyspaceAndCFsUsingOptions".toLowerCase();
				
		AstyanaxContext<Keyspace> context = AstyanaxContextFactory.getKeyspace(keyspaceName);
		
    	context.start();
        keyspace = context.getClient();

		Map<String, Object> options = ImmutableMap.<String, Object>builder()
				.put("strategy_options", ImmutableMap.<String, Object>builder()
											.put("replication_factor", "1")
											.build())
			    .put("strategy_class",     "SimpleStrategy")
			    .build();
        
		
		Map<ColumnFamily, Map<String, Object>> cfs = ImmutableMap.<ColumnFamily, Map<String, Object>>builder()
				.put(new ColumnFamily<String, String>("testcf1", StringSerializer.get(), StringSerializer.get()), 
						ImmutableMap.<String, Object>builder()
						.put("bloom_filter_fp_chance", 0.01)
						.build())
				.put(new ColumnFamily<Long, String>("testcf2", LongSerializer.get(), StringSerializer.get()), 
						ImmutableMap.<String, Object>builder()
						.put("read_repair_chance", 0.2)
						.put("bloom_filter_fp_chance", 0.01)
						.build())
				.build();
		
		keyspace.createKeyspace(options, cfs);
		
		Thread.sleep(1000);
		
		KeyspaceDefinition ksDef = keyspace.describeKeyspace();
		verifyKeyspacePropertiesForSimpleStrategy(keyspaceName, ksDef);
		
		Map<String, String> strategyOptions = ksDef.getStrategyOptions();
		Assert.assertEquals("1", strategyOptions.get("replication_factor"));

		Properties cfProps = keyspace.getColumnFamilyProperties("testcf1");

		Assert.assertEquals("0.1", String.valueOf(cfProps.get("read_repair_chance")));
		Assert.assertEquals("0.01", String.valueOf(cfProps.get("bloom_filter_fp_chance")));
		Assert.assertEquals("KEYS_ONLY", String.valueOf(cfProps.get("caching")));
		Assert.assertEquals("4", String.valueOf(cfProps.get("min_compaction_threshold")));
		Assert.assertEquals("32", String.valueOf(cfProps.get("max_compaction_threshold")));

		cfProps = keyspace.getColumnFamilyProperties("testcf2");

		Assert.assertEquals("0.2", String.valueOf(cfProps.get("read_repair_chance")));
		Assert.assertEquals("0.01", String.valueOf(cfProps.get("bloom_filter_fp_chance")));
		Assert.assertEquals("KEYS_ONLY", String.valueOf(cfProps.get("caching")));
		Assert.assertEquals("4", String.valueOf(cfProps.get("min_compaction_threshold")));
		Assert.assertEquals("32", String.valueOf(cfProps.get("max_compaction_threshold")));

		keyspace.dropKeyspace();
	}

	@Test
	public void createKeyspaceAndCFsDirectly() throws Exception {
		
		String keyspaceName = "AstyanaxTestKeyspaceAndCFsDirect".toLowerCase();
				
		AstyanaxContext<Keyspace> context = AstyanaxContextFactory.getKeyspace(keyspaceName);
		
    	context.start();
        keyspace = context.getClient();

		Map<String, Object> ksOptions = ImmutableMap.<String, Object>builder()
				.put("strategy_options", ImmutableMap.<String, Object>builder()
											.put("replication_factor", "1")
											.build())
			    .put("strategy_class",     "SimpleStrategy")
			    .build();
        
		
		keyspace.createKeyspace(ksOptions);
		
		ColumnFamily<String, String> cf1 = new ColumnFamily<String, String>("testcf1", StringSerializer.get(), StringSerializer.get());
		Map<String, Object> options1 = ImmutableMap.<String, Object>builder()
				.put("read_repair_chance", 0.2)
				.put("bloom_filter_fp_chance", 0.01)
				.build();
		
		keyspace.createColumnFamily(cf1, options1);

		Map<String, Object> options2 = new HashMap<String, Object>();
		options2.put("name", "testcf2");
		options2.put("read_repair_chance", 0.4);
		options2.put("bloom_filter_fp_chance", 0.01);

		keyspace.createColumnFamily(options2);
		
		Thread.sleep(1000);
		
		KeyspaceDefinition ksDef = keyspace.describeKeyspace();
		verifyKeyspacePropertiesForSimpleStrategy(keyspaceName, ksDef);
		
		Properties cfProps = keyspace.getColumnFamilyProperties("testcf1");

		Assert.assertEquals("0.2", String.valueOf(cfProps.get("read_repair_chance")));
		Assert.assertEquals("0.01", String.valueOf(cfProps.get("bloom_filter_fp_chance")));
		Assert.assertEquals("KEYS_ONLY", String.valueOf(cfProps.get("caching")));
		Assert.assertEquals("4", String.valueOf(cfProps.get("min_compaction_threshold")));
		Assert.assertEquals("32", String.valueOf(cfProps.get("max_compaction_threshold")));

		cfProps = keyspace.getColumnFamilyProperties("testcf2");

		Assert.assertEquals("0.4", String.valueOf(cfProps.get("read_repair_chance")));
		Assert.assertEquals("0.01", String.valueOf(cfProps.get("bloom_filter_fp_chance")));
		Assert.assertEquals("KEYS_ONLY", String.valueOf(cfProps.get("caching")));
		Assert.assertEquals("4", String.valueOf(cfProps.get("min_compaction_threshold")));
		Assert.assertEquals("32", String.valueOf(cfProps.get("max_compaction_threshold")));
		
		ColumnFamilyDefinition cfDef = ksDef.getColumnFamily("testcf1");
		Assert.assertEquals("testcf1", cfDef.getName());
		Assert.assertEquals(0.2, cfDef.getReadRepairChance());
		Assert.assertEquals("KEYS_ONLY", cfDef.getCaching());
		Assert.assertTrue(32 == cfDef.getMaxCompactionThreshold());
		Assert.assertTrue(4 == cfDef.getMinCompactionThreshold());
		Assert.assertEquals(0.01, cfDef.getBloomFilterFpChance());

		cfDef = ksDef.getColumnFamily("testcf2");
		Assert.assertEquals("testcf2", cfDef.getName());
		Assert.assertEquals(0.4, cfDef.getReadRepairChance());
		Assert.assertEquals("KEYS_ONLY", cfDef.getCaching());
		Assert.assertTrue(32 == cfDef.getMaxCompactionThreshold());
		Assert.assertTrue(4 == cfDef.getMinCompactionThreshold());
		Assert.assertEquals(0.01, cfDef.getBloomFilterFpChance());

		List<ColumnFamilyDefinition> cfDefs = ksDef.getColumnFamilyList();
		Assert.assertTrue(2 == cfDefs.size());
		
		cfDef = cfDefs.get(0);
		Assert.assertEquals("testcf1", cfDef.getName());
		Assert.assertEquals(0.2, cfDef.getReadRepairChance());
		Assert.assertEquals("KEYS_ONLY", cfDef.getCaching());
		Assert.assertTrue(32 == cfDef.getMaxCompactionThreshold());
		Assert.assertTrue(4 == cfDef.getMinCompactionThreshold());
		Assert.assertEquals(0.01, cfDef.getBloomFilterFpChance());

		cfDef = cfDefs.get(1);
		Assert.assertEquals("testcf2", cfDef.getName());
		Assert.assertEquals(0.4, cfDef.getReadRepairChance());
		Assert.assertEquals("KEYS_ONLY", cfDef.getCaching());
		Assert.assertTrue(32 == cfDef.getMaxCompactionThreshold());
		Assert.assertTrue(4 == cfDef.getMinCompactionThreshold());
		Assert.assertEquals(0.01, cfDef.getBloomFilterFpChance());

		keyspace.dropKeyspace();
	}
	
	@Test
	public void createKeyspaceWithCompositeCF() throws Exception {
		
		// Annotated composite class
		class Population {
		  @Component(ordinal=0) String country;
		  @Component(ordinal=1) String state;
		  @Component(ordinal=2) String city;
		  @Component(ordinal=3) Integer zip;
		  @Component(ordinal=3) Date district;
		  // Must have public default constructor
		  public Population() {
		  }
		}

		AnnotatedCompositeSerializer<Population> compSerializer = new AnnotatedCompositeSerializer<Population>(Population.class);
		ColumnFamily<String, Population> CF_POPULATION = 
				new ColumnFamily<String, Population>("population", StringSerializer.get(), compSerializer);
		
		String keyspaceName = "AstyanaxTestKeyspaceCompositeCFs".toLowerCase();
		
		AstyanaxContext<Keyspace> context = AstyanaxContextFactory.getKeyspace(keyspaceName);
    	context.start();
        keyspace = context.getClient();

		Map<String, Object> ksOptions = ImmutableMap.<String, Object>builder()
				.put("strategy_options", ImmutableMap.<String, Object>builder()
											.put("replication_factor", "1")
											.build())
			    .put("strategy_class",     "SimpleStrategy")
			    .build();
        
		
		keyspace.createKeyspace(ksOptions);

		KeyspaceDefinition ksDef = keyspace.describeKeyspace();
		Assert.assertEquals(keyspaceName, ksDef.getName());

		keyspace.createColumnFamily(CF_POPULATION, ImmutableMap.<String, Object>builder()
        .put("default_validation_class", "UTF8Type")
        .put("key_validation_class",     "UTF8Type")
        .put("comparator_type",          "CompositeType(UTF8Type, UTF8Type, UTF8Type, Int32Type, DateType)")
        .build());

		if (ClusterConfiguration.getDriver().equals(Driver.JAVA_DRIVER)) {
			List<ColumnFamilyDefinition> list = ksDef.getColumnFamilyList();
			Assert.assertTrue(1 == list.size());
		
			ColumnFamilyDefinition cfDef = list.get(0);
			Assert.assertEquals("population", cfDef.getName());
		
			List<ColumnDefinition> colDefs = cfDef.getColumnDefinitionList();
			Assert.assertTrue(7 == colDefs.size());
		
			for (int i=1; i<=5; i++) {
				ColumnDefinition colDef = colDefs.get(i-1);
				Assert.assertEquals("column" + i, colDef.getName());
				Assert.assertNotNull(colDef.getValidationClass());
			}
			ColumnDefinition colDef = colDefs.get(6);
			Assert.assertEquals("value", colDef.getName());
			Assert.assertNotNull(colDef.getValidationClass());
			
			cfDef = ksDef.getColumnFamily("population");
			Assert.assertEquals("population", cfDef.getName());
			
			colDefs = cfDef.getColumnDefinitionList();
			Assert.assertTrue(7 == colDefs.size());
		
			for (int i=1; i<=5; i++) {
				colDef = colDefs.get(i-1);
				Assert.assertEquals("column" + i, colDef.getName());
				Assert.assertNotNull(colDef.getValidationClass());
			}
			colDef = colDefs.get(6);
			Assert.assertEquals("value", colDef.getName());
			Assert.assertNotNull(colDef.getValidationClass());
		}

		keyspace.dropKeyspace();
	}
	
	@Test
	public void alterKeyspaceOptions() throws Exception {
		
		String keyspaceName = "AstyanaxTestKeyspaceAlterOptions".toLowerCase();

		AstyanaxContext<Keyspace> context = AstyanaxContextFactory.getKeyspace(keyspaceName);
    	context.start();
        keyspace = context.getClient();

		Map<String, Object> options = ImmutableMap.<String, Object>builder()
										.put("strategy_options", ImmutableMap.<String, Object>builder()
																	.put("replication_factor", "1")
																	.build())
									    .put("strategy_class",     "SimpleStrategy")
									    .build();
        
		keyspace.createKeyspace(options);
		Thread.sleep(1000);
		
		KeyspaceDefinition ksDef = keyspace.describeKeyspace();
		verifyKeyspacePropertiesForSimpleStrategy(keyspaceName, ksDef);
		
		keyspace.updateKeyspace(ImmutableMap.<String, Object>builder()
										.put("strategy_options", ImmutableMap.<String, Object>builder()
																	.put("replication_factor", "2")
																	.build())
									    .put("strategy_class",     "SimpleStrategy")
									    .build());
		ksDef = keyspace.describeKeyspace();
		Assert.assertEquals("2", ksDef.getStrategyOptions().get("replication_factor"));
		
		keyspace.dropKeyspace();
		
		/** NETWORK TOPOLOGY */ 
		keyspaceName = "AstyanaxTestKeyspaceAlterOptions2".toLowerCase();
		
		context = AstyanaxContextFactory.getKeyspace(keyspaceName);
		context.start();
        keyspace = context.getClient();

		options = ImmutableMap.<String, Object>builder()
			    						.put("strategy_options", ImmutableMap.<String, Object>builder()
			    												.put("us-east", "3")
			    												.put("eu-west", "3")
			    												.build())
			    						.put("strategy_class",     "NetworkTopologyStrategy")
			    						.build();
        
		keyspace.createKeyspace(options);
		Thread.sleep(1000);
		
		KeyspaceDefinition ksDef2 = keyspace.describeKeyspace();
		verifyKeyspacePropertiesForNetworkTopology(keyspaceName, ksDef2);

		options = ImmutableMap.<String, Object>builder()
				.put("strategy_options", ImmutableMap.<String, Object>builder()
										.put("us-east", "2")
										.put("eu-west", "2")
										.build())
				.put("strategy_class",     "NetworkTopologyStrategy")
				.build();

		keyspace.updateKeyspace(options);
		ksDef2 = keyspace.describeKeyspace();
		
		System.out.println(ksDef2.getStrategyOptions());
		Assert.assertEquals("2", ksDef2.getStrategyOptions().get("us-east"));
		Assert.assertEquals("2", ksDef2.getStrategyOptions().get("eu-west"));

		keyspace.dropKeyspace();
	}
	
	@Test
	public void alterCFOptions() throws Exception {
		
		String keyspaceName = "AstyanaxTestKeyspaceAlterCFOptions".toLowerCase();
		
		AstyanaxContext<Keyspace> context = AstyanaxContextFactory.getKeyspace(keyspaceName);
    	context.start();
        keyspace = context.getClient();

		Map<String, Object> options = ImmutableMap.<String, Object>builder()
										.put("strategy_options", ImmutableMap.<String, Object>builder()
																	.put("replication_factor", "1")
																	.build())
									    .put("strategy_class",     "SimpleStrategy")
									    .build();
        
		keyspace.createKeyspace(options);
		Thread.sleep(1000);
		
		ColumnFamily<String, String> cf = new ColumnFamily<String, String>("testaltercf1", StringSerializer.get(), StringSerializer.get());
		keyspace.createColumnFamily(cf, null);
		
		Assert.assertEquals(0.1, keyspace.getColumnFamilyProperties("testaltercf1").get("read_repair_chance"));
		
		keyspace.updateColumnFamily(cf, ImmutableMap.<String, Object>builder()
										.put("read_repair_chance", 0.2)
										.build());
		Assert.assertEquals(0.2, keyspace.getColumnFamilyProperties("testaltercf1").get("read_repair_chance"));

		keyspace.dropKeyspace();
	}
	
	@Test
	public void createAndDeleteCF() throws Exception {
		
		String keyspaceName = "AstyanaxTestKeyspaceCreateDeleteCF".toLowerCase();
		
		AstyanaxContext<Keyspace> context = AstyanaxContextFactory.getKeyspace(keyspaceName);
    	context.start();
        keyspace = context.getClient();

		Map<String, Object> options = ImmutableMap.<String, Object>builder()
										.put("strategy_options", ImmutableMap.<String, Object>builder()
																	.put("replication_factor", "1")
																	.build())
									    .put("strategy_class",     "SimpleStrategy")
									    .build();
        
		keyspace.createKeyspace(options);
		Thread.sleep(1000);
		
		ColumnFamily<String, String> cf = new ColumnFamily<String, String>("testcreatedeletecf1", StringSerializer.get(), StringSerializer.get());
		keyspace.createColumnFamily(cf, null);
		Assert.assertEquals(0.1, keyspace.getColumnFamilyProperties("testcreatedeletecf1").get("read_repair_chance"));
		
		keyspace.dropColumnFamily(cf);
		try {
			keyspace.getColumnFamilyProperties("testaltercf1");
			Assert.fail("Should have gotten CF not found ex");
		} catch(RuntimeException e) {
			
		} finally {
			keyspace.dropKeyspace();
		}
	}
	
	@Test
	public void createAndDeleteKeyspace() throws Exception {
		
		String keyspaceName = "AstyanaxTestKeyspaceCreateDeleteKS".toLowerCase();

		AstyanaxContext<Keyspace> context = AstyanaxContextFactory.getKeyspace(keyspaceName);
    	context.start();
        keyspace = context.getClient();

		Map<String, Object> options = ImmutableMap.<String, Object>builder()
										.put("strategy_options", ImmutableMap.<String, Object>builder()
																	.put("replication_factor", "1")
																	.build())
									    .put("strategy_class",     "SimpleStrategy")
									    .build();
        
		keyspace.createKeyspace(options);
		Thread.sleep(1000);
		
		KeyspaceDefinition ksDef = keyspace.describeKeyspace();
		Assert.assertTrue(ksDef.getStrategyClass().contains("SimpleStrategy"));

		keyspace.dropKeyspace();
		try {
			keyspace.describeKeyspace();
			Assert.fail("Should have gotten KS not found ex");
		} catch(RuntimeException e) {
			
		}
	}

	@Test
	public void keyspaceDescribePartitioner() throws Exception {
		
		String keyspaceName = "AstyanaxTestKeyspaceDescribeRing".toLowerCase();

		AstyanaxContext<Keyspace> context = AstyanaxContextFactory.getKeyspace(keyspaceName);
    	context.start();
        keyspace = context.getClient();

		Map<String, Object> options = ImmutableMap.<String, Object>builder()
										.put("strategy_options", ImmutableMap.<String, Object>builder()
																	.put("replication_factor", "1")
																	.build())
									    .put("strategy_class",     "SimpleStrategy")
									    .build();
        
		keyspace.createKeyspace(options);
		Thread.sleep(1000);

		String partitioner = keyspace.describePartitioner();
		Assert.assertNotNull(partitioner);
		
		keyspace.dropKeyspace();
	}
}>>>>>>> YOURS
/home/ramdisk/experiment3/projects/astyanax/revisions/rev_a20aab8_5afcbce/rev_a20aab8-5afcbce/src/main/java/com/netflix/astyanax/recipes/locks/ColumnPrefixDistributedRowLock.java;<<<<<<< MINE
=======
/*******************************************************************************
 * Copyright 2011 Netflix
 * 
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 * 
 *   http://www.apache.org/licenses/LICENSE-2.0
 * 
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 ******************************************************************************/
package com.netflix.astyanax.recipes.locks;

import java.nio.ByteBuffer;
import java.util.Map;
import java.util.Map.Entry;
import java.util.Set;
import java.util.concurrent.TimeUnit;

import com.google.common.base.Preconditions;
import com.google.common.collect.Maps;
import com.google.common.collect.Sets;
import com.netflix.astyanax.ColumnListMutation;
import com.netflix.astyanax.Keyspace;
import com.netflix.astyanax.MutationBatch;
import com.netflix.astyanax.model.Column;
import com.netflix.astyanax.model.ColumnFamily;
import com.netflix.astyanax.model.ColumnList;
import com.netflix.astyanax.model.ColumnMap;
import com.netflix.astyanax.model.ConsistencyLevel;
import com.netflix.astyanax.model.OrderedColumnMap;
import com.netflix.astyanax.retry.RetryPolicy;
import com.netflix.astyanax.retry.RunOnce;
import com.netflix.astyanax.serializers.ByteBufferSerializer;
import com.netflix.astyanax.serializers.LongSerializer;
import com.netflix.astyanax.util.RangeBuilder;
import com.netflix.astyanax.util.TimeUUIDUtils;

/**
 * Takes a distributed row lock for a single row.  The row lock is accomplished using
 * a sequence of read/write events to Cassandra without the need for something like
 * zookeeper.  
 * 
 * Algorithm 
 * 1. Write a column with name <prefix>_<uuid>. Value is an expiration time. 
 * 2. Read back all columns with <prefix> 
 *      case 1) count==1 Got the lock 
 *      case 2) count> 1 No lock
 * 3. Do something in your code assuming the row is locked
 * 4. Release the lock by deleting the lock columns
 * 
 * Usage considerations
 * 1. Set an expiration time (expireLockAfter) that is long enough for your processing to complete
 * 2. Use this when the probability for contension is very low
 * 3. Optimize by reading all columns (withIncludeAllColumn(true)) and merge the mutation
 *      into the release.  This will save 2 calls to cassandra.
 * 4. If the client fails after Step 1.  A subsequent attempt to lock will automatically 
 *      release these stale locks.  You can turn this auto cleanup off by calling
 *      failOnStaleLock(false), handling a StaleLockException and doing manual cleanup by
 *      calling releaseExpiredLocks()
 * 5. An optional TTL can be set on the lock columns which will ensure abandoned/stale locks
 *      will be cleaned up by compactions at some point.
 * 6. You can customize the 'prefix' used for the lock columns.  This will help with storing
 *      the lock columns with data in the same row.  
 * 7. You can customize the unique part of the lock column to include meaningful data such
 *      as the UUID row key from another column family.  This can have the same effect as 
 *      assigning a foreign key to the lock column and is useful for uniqueness constraint.
 * 8. This recipe is not a transaction.  
 * 
 * Take a lock,
 * <code>
 *      ColumnPrefixDistributedRowLock<String> lock = new ColumnPrefixDistributedRowLock<String>(keyspace, columnFamily, "KeyBeingLocked");
 *      try {
 *          lock.acquire();
 *      }
 *      finally {
 *          lock.release();
 *      }
 * </code>
 * 
 * Read, Modify, Write.  The read, modify, write piggybacks on top of the lock calls.
 * 
 * <code>
 *      ColumnPrefixDistributedRowLock<String> lock = new ColumnPrefixDistributedRowLock<String>(keyspace, columnFamily, "KeyBeingLocked");
 *      MutationBatch m = keyspace.prepareMutationBatch();
 *      try {
 *          ColumnMap<String> columns = lock.acquireLockAndReadRow();
 *          
 *          m.withRow("KeyBeingLocked")
 *              .putColumn("SomeColumnBeingUpdated", );
 *              
 *          lock.releaseWithMutation(m);
 *      }
 *      catch (Exception e) {
 *          lock.release();
 *      }
 * </code>
 * 
 * @author elandau
 * 
 * @param <K>
 */
public class ColumnPrefixDistributedRowLock<K> implements DistributedRowLock {
    public static final int      LOCK_TIMEOUT                    = 60;
    public static final TimeUnit DEFAULT_OPERATION_TIMEOUT_UNITS = TimeUnit.MINUTES;
    public static final String   DEFAULT_LOCK_PREFIX             = "_LOCK_";

    private final ColumnFamily<K, String> columnFamily; // The column family for data and lock
    private final Keyspace   keyspace;                  // The keyspace
    private final K          key;                       // Key being locked

    private long             timeout          = LOCK_TIMEOUT;                   // Timeout after which the lock expires.  Units defined by timeoutUnits.
    private TimeUnit         timeoutUnits     = DEFAULT_OPERATION_TIMEOUT_UNITS;
    private String           prefix           = DEFAULT_LOCK_PREFIX;            // Prefix to identify the lock columns
    private ConsistencyLevel consistencyLevel = ConsistencyLevel.CL_LOCAL_QUORUM;
    private boolean          failOnStaleLock  = false;           
    private String           lockColumn       = null;
    private String           lockId           = null;
    private Set<String>      locksToDelete    = Sets.newHashSet();
    private ColumnMap<String> columns         = null;
    private Integer          ttl              = null;                           // Units in seconds
    private boolean          readDataColumns  = false;
    private RetryPolicy      backoffPolicy    = RunOnce.get();
    private long             acquireTime      = 0;
    private int              retryCount       = 0;

    public ColumnPrefixDistributedRowLock(Keyspace keyspace, ColumnFamily<K, String> columnFamily, K key) {
        this.keyspace     = keyspace;
        this.columnFamily = columnFamily;
        this.key          = key;
        this.lockId       = TimeUUIDUtils.getUniqueTimeUUIDinMicros().toString();
    }

    /**
     * Modify the consistency level being used. Consistency should always be a
     * variant of quorum. The default is CL_QUORUM, which is OK for single
     * region. For multi region the consistency level should be CL_LOCAL_QUORUM.
     * CL_EACH_QUORUM can be used but will Incur substantial latency.
     * 
     * @param consistencyLevel
     * @return
     */
    public ColumnPrefixDistributedRowLock<K> withConsistencyLevel(ConsistencyLevel consistencyLevel) {
        this.consistencyLevel = consistencyLevel;
        return this;
    }

    /**
     * Specify the prefix that uniquely distinguishes the lock columns from data
     * column
     * 
     * @param prefix
     * @return
     */
    public ColumnPrefixDistributedRowLock<K> withColumnPrefix(String prefix) {
        this.prefix = prefix;
        return this;
    }

    /**
     * If true the first read will also fetch all the columns in the row as 
     * opposed to just the lock columns.
     * @param flag
     * @return
     */
    public ColumnPrefixDistributedRowLock<K> withDataColumns(boolean flag) {
        this.readDataColumns = flag;
        return this;
    }
    
    /**
     * Override the autogenerated lock column.
     * 
     * @param lockId
     * @return
     */
    public ColumnPrefixDistributedRowLock<K> withLockId(String lockId) {
        this.lockId = lockId;
        return this;
    }

    /**
     * When set to true the operation will fail if a stale lock is detected
     * 
     * @param failOnStaleLock
     * @return
     */
    public ColumnPrefixDistributedRowLock<K> failOnStaleLock(boolean failOnStaleLock) {
        this.failOnStaleLock = failOnStaleLock;
        return this;
    }

    /**
     * Time for failed locks. Under normal circumstances the lock column will be
     * deleted. If not then this lock column will remain and the row will remain
     * locked. The lock will expire after this timeout.
     * 
     * @param timeout
     * @param unit
     * @return
     */
    public ColumnPrefixDistributedRowLock<K> expireLockAfter(long timeout, TimeUnit unit) {
        this.timeout      = timeout;
        this.timeoutUnits = unit;
        return this;
    }

    /**
     * This is the TTL on the lock column being written, as opposed to expireLockAfter which 
     * is written as the lock column value.  Whereas the expireLockAfter can be used to 
     * identify a stale or abandoned lock the TTL will result in the stale or abandoned lock
     * being eventually deleted by cassandra.  Set the TTL to a number that is much greater
     * tan the expireLockAfter time.
     * @param ttl
     * @return
     */
    public ColumnPrefixDistributedRowLock<K> withTtl(Integer ttl) {
        this.ttl = ttl;
        return this;
    }
    
    public ColumnPrefixDistributedRowLock<K> withTtl(Integer ttl, TimeUnit units) {
        this.ttl = (int) TimeUnit.SECONDS.convert(ttl,  units);
        return this;
    }
    
    public ColumnPrefixDistributedRowLock<K> withBackoff(RetryPolicy policy) {
        this.backoffPolicy  = policy;
        return this;
    }

    /**
     * Try to take the lock.  The caller must call .release() to properly clean up
     * the lock columns from cassandra
     * 
     * @return
     * @throws Exception
     */
    @Override
    public void acquire() throws Exception {
        
        Preconditions.checkArgument(ttl == null || TimeUnit.SECONDS.convert(timeout, timeoutUnits) < ttl, "Timeout " + timeout + " must be less than TTL " + ttl);
        
        RetryPolicy retry = backoffPolicy.duplicate();
        retryCount = 0;
        while (true) {
            try {
                long curTimeMicros = getCurrentTimeMicros();
                
                MutationBatch m = keyspace.prepareMutationBatch().setConsistencyLevel(consistencyLevel);
                fillLockMutation(m, curTimeMicros, ttl);
                m.execute();
                
                verifyLock(curTimeMicros);
                acquireTime = System.nanoTime();
                return;
            }
            catch (BusyLockException e) {
                release();
                if(!retry.allowRetry())
                    throw e;
                retryCount++;
            }
        }
    }

    /**
     * Take the lock and return the row data columns.  Use this, instead of acquire, when you 
     * want to implement a read-modify-write scenario and want to reduce the number of calls
     * to Cassandra.
     * 
     * @return
     * @throws Exception
     */
    public ColumnMap<String> acquireLockAndReadRow() throws Exception {
        withDataColumns(true);
        acquire();
        return getDataColumns();
    }
    
    /**
     * Verify that the lock was acquired.  This shouldn't be called unless it's part of a recipe
     * built on top of ColumnPrefixDistributedRowLock.  
     * 
     * @param curTimeInMicros
     * @throws BusyLockException
     */
    public void verifyLock(long curTimeInMicros) throws Exception, BusyLockException, StaleLockException {
        if (lockColumn == null) 
            throw new IllegalStateException("verifyLock() called without attempting to take the lock");
        
        // Read back all columns. There should be only 1 if we got the lock
        Map<String, Long> lockResult = readLockColumns(readDataColumns);

        // Cleanup and check that we really got the lock
        for (Entry<String, Long> entry : lockResult.entrySet()) {
            // This is a stale lock that was never cleaned up
            if (entry.getValue() != 0 && curTimeInMicros > entry.getValue()) {
                if (failOnStaleLock) {
                    throw new StaleLockException("Stale lock on row '" + key + "'.  Manual cleanup requried.");
                }
                locksToDelete.add(entry.getKey());
            }
            // Lock already taken, and not by us
            else if (!entry.getKey().equals(lockColumn)) {
                throw new BusyLockException("Lock already acquired for row '" + key + "' with lock column " + entry.getKey());
            }
        }
    }

    /**
     * Release the lock by releasing this and any other stale lock columns
     */
    @Override
    public void release() throws Exception {
        if (!locksToDelete.isEmpty() || lockColumn != null) {
            MutationBatch m = keyspace.prepareMutationBatch().setConsistencyLevel(consistencyLevel);
            fillReleaseMutation(m, false);
            m.execute();
        }
    }

    /**
     * Release using the provided mutation.  Use this when you want to commit actual data
     * when releasing the lock
     * @param m
     * @throws Exception
     */
    public void releaseWithMutation(MutationBatch m) throws Exception {
        releaseWithMutation(m, false);
    }
    
    public boolean releaseWithMutation(MutationBatch m, boolean force) throws Exception {
        long elapsed = TimeUnit.MILLISECONDS.convert(System.nanoTime() - acquireTime, TimeUnit.NANOSECONDS);
        boolean isStale = false;
        if (timeout > 0 && elapsed > TimeUnit.MILLISECONDS.convert(timeout, this.timeoutUnits)) {
            isStale = true;
            if (!force) {
                throw new StaleLockException("Lock for '" + getKey() + "' became stale");
            }
        }
        
        m.setConsistencyLevel(consistencyLevel);
        fillReleaseMutation(m, false);
        m.execute();
        
        return isStale;
    }
    
    /**
     * Return a mapping of existing lock columns and their expiration times
     * 
     * @return
     * @throws Exception
     */
    public Map<String, Long> readLockColumns() throws Exception {
        return readLockColumns(false);
    }
    
    /**
     * Read all the lock columns.  Will also ready data columns if withDataColumns(true) was called
     * 
     * @param readDataColumns
     * @return
     * @throws Exception
     */
    private Map<String, Long> readLockColumns(boolean readDataColumns) throws Exception {
        Map<String, Long> result = Maps.newLinkedHashMap();
        // Read all the columns
        if (readDataColumns) {
            columns = new OrderedColumnMap<String>();
            ColumnList<String> lockResult = keyspace
                .prepareQuery(columnFamily)
                    .setConsistencyLevel(consistencyLevel)
                    .getKey(key)
                .execute()
                    .getResult();
    
            for (Column<String> c : lockResult) {
                if (c.getName().startsWith(prefix))
                    result.put(c.getName(), readTimeoutValue(c));
                else 
                    columns.add(c);
            }
        }
        // Read only the lock columns
        else {
            ColumnList<String> lockResult = keyspace
                .prepareQuery(columnFamily)
                    .setConsistencyLevel(consistencyLevel)
                    .getKey(key)
                    .withColumnRange(new RangeBuilder().setStart(prefix + "\u0000").setEnd(prefix + "\uFFFF").build())
                .execute()
                    .getResult();

            for (Column<String> c : lockResult) {
                result.put(c.getName(), readTimeoutValue(c));
            }

        }
        return result;    
    }
    
    /**
     * Release all locks. Use this carefully as it could release a lock for a
     * running operation.
     * 
     * @return
     * @throws Exception
     */
    public Map<String, Long> releaseAllLocks() throws Exception {
        return releaseLocks(true);
    }

    /**
     * Release all expired locks for this key.
     * 
     * @return
     * @throws Exception
     */
    public Map<String, Long> releaseExpiredLocks() throws Exception {
        return releaseLocks(false);
    }

    /**
     * Delete locks columns. Set force=true to remove locks that haven't 
     * expired yet.
     * 
     * This operation first issues a read to cassandra and then deletes columns
     * in the response.
     * 
     * @param force - Force delete of non expired locks as well
     * @return
     * @throws Exception
     */
    public Map<String, Long> releaseLocks(boolean force) throws Exception {
        Map<String, Long> locksToDelete = readLockColumns();

        MutationBatch m = keyspace.prepareMutationBatch().setConsistencyLevel(consistencyLevel);
        ColumnListMutation<String> row = m.withRow(columnFamily, key);
        long now = getCurrentTimeMicros();
        for (Entry<String, Long> c : locksToDelete.entrySet()) {
            if (force || (c.getValue() > 0 && c.getValue() < now)) {
                row.deleteColumn(c.getKey());
            }
        }
        m.execute();

        return locksToDelete;
    }

    /**
     * Get the current system time
     * 
     * @return
     */
    private static long getCurrentTimeMicros() {
        return TimeUnit.MICROSECONDS.convert(System.currentTimeMillis(), TimeUnit.MILLISECONDS);
    }

    /**
     * Fill a mutation with the lock column. This may be used when the mutation
     * is executed externally but should be used with extreme caution to ensure
     * the lock is properly released
     * 
     * @param m
     * @param time
     * @param ttl
     */
    public String fillLockMutation(MutationBatch m, Long time, Integer ttl) {
        if (lockColumn != null) {
            if (!lockColumn.equals(prefix+lockId))
                throw new IllegalStateException("Can't change prefix or lockId after acquiring the lock");
        }
        else {
            lockColumn = prefix + lockId;
        }
        
        Long timeoutValue 
              = (time == null)
              ? new Long(0)
              : time + TimeUnit.MICROSECONDS.convert(timeout, timeoutUnits);
              
        m.withRow(columnFamily, key).putColumn(lockColumn, generateTimeoutValue(timeoutValue), ttl);
        return lockColumn;
    }
    
    /**
     * Generate the expire time value to put in the column value.
     * @param timeout
     * @return
     */
    private ByteBuffer generateTimeoutValue(long timeout) {
        if (columnFamily.getDefaultValueSerializer() == ByteBufferSerializer.get() ||
            columnFamily.getDefaultValueSerializer() == LongSerializer.get()) {
            return LongSerializer.get().toByteBuffer(timeout);
        }
        else {
            return columnFamily.getDefaultValueSerializer().fromString(Long.toString(timeout));
        }
    }
    
    /**
     * Read the expiration time from the column value
     * @param column
     * @return
     */
    public long readTimeoutValue(Column<?> column) {
        if (columnFamily.getDefaultValueSerializer() == ByteBufferSerializer.get() ||
            columnFamily.getDefaultValueSerializer() == LongSerializer.get()) {
            return column.getLongValue();
        }
        else {
            return Long.parseLong(column.getStringValue());
        }
    }

    /**
     * Fill a mutation that will release the locks. This may be used from a
     * separate recipe to release multiple locks.
     * 
     * @param m
     */
    public void fillReleaseMutation(MutationBatch m, boolean excludeCurrentLock) {
        // Add the deletes to the end of the mutation
        ColumnListMutation<String> row = m.withRow(columnFamily, key);
        for (String c : locksToDelete) {
            row.deleteColumn(c);
        }
        if (!excludeCurrentLock && lockColumn != null) 
            row.deleteColumn(lockColumn);
        locksToDelete.clear();
        lockColumn = null;
    }


    public ColumnMap<String> getDataColumns() {
        return columns;
    }
    
    public K getKey() {
        return key;
    }
    
    public Keyspace getKeyspace() {
        return keyspace;
    }

    public ConsistencyLevel getConsistencyLevel() {
        return consistencyLevel;
    }

    public String getLockColumn() {
        return lockColumn;
    }
    
    public String getLockId() {
        return lockId;
    }
    
    public String getPrefix() {
        return prefix;
    }
    
    public int getRetryCount() {
        return retryCount;
    }

}>>>>>>> YOURS
/home/ramdisk/experiment3/projects/astyanax/revisions/rev_183d714_75acddc/rev_183d714-75acddc/astyanax-cassandra/src/test/java/com/netflix/astyanax/model/DynamicCompositeTest.java;<<<<<<< MINE
=======
package com.netflix.astyanax.model;


import java.nio.ByteBuffer;
import java.util.UUID;

import org.junit.Test;

import com.netflix.astyanax.serializers.AbstractSerializer;
import com.netflix.astyanax.serializers.AsciiSerializer;
import com.netflix.astyanax.serializers.BytesArraySerializer;
import com.netflix.astyanax.serializers.IntegerSerializer;
import com.netflix.astyanax.serializers.LongSerializer;
import com.netflix.astyanax.serializers.StringSerializer;
import com.netflix.astyanax.serializers.UUIDSerializer;

import static org.junit.Assert.assertArrayEquals;
import static org.junit.Assert.assertEquals;


public class DynamicCompositeTest {


    @Test
    public void testComposite() {
        DynamicComposite dc = new DynamicComposite();
        for ( char ch = 'A'; ch < 'Z'; ch++ ) {
            dc.addComponent( Character.toString( ch ), StringSerializer.get() );
        }
    }


    @Test
    public void testReversedSerialization() {

        AsciiSerializer asciiSerializer = AsciiSerializer.get();
        BytesArraySerializer bytesArraySerializer = BytesArraySerializer.get();

        IntegerSerializer integerSerializer = IntegerSerializer.get();
        LongSerializer longSerializer = LongSerializer.get();

        StringSerializer stringSerializer = StringSerializer.get();

        UUIDSerializer uuidSerializer = UUIDSerializer.get();


        DynamicComposite dc = new DynamicComposite();

        final String string = "test";
        final byte[] bytes = new byte[] { 0x00 };
        final int intValue = 1;
        final long longValue = 1l;
        final UUID uuid = UUID.randomUUID();


        dc.addComponent( string, asciiSerializer, getReversed( asciiSerializer ) );

        dc.addComponent( bytes, bytesArraySerializer, getReversed( bytesArraySerializer ) );

        dc.addComponent( intValue, integerSerializer, getReversed( integerSerializer ) );

        dc.addComponent( longValue, longSerializer, getReversed( longSerializer ) );

        dc.addComponent( string, stringSerializer, getReversed( stringSerializer ) );

        dc.addComponent( uuid, uuidSerializer, getReversed( uuidSerializer ) );

        //serialize to bytes
        ByteBuffer buff = dc.serialize();

        //de-serialize
        DynamicComposite read = DynamicComposite.fromByteBuffer( buff );

        assertEquals(6, read.size());

        assertEquals(string, read.getComponent( 0 ).getValue( asciiSerializer ));

        assertArrayEquals( bytes, ( byte[] ) read.getComponent( 1 ).getValue( bytesArraySerializer ) );

        assertEquals(intValue, read.getComponent( 2 ).getValue( integerSerializer ));

        assertEquals(longValue, read.getComponent( 3 ).getValue( longSerializer ));

        assertEquals(string, read.getComponent( 4 ).getValue( stringSerializer ));

        assertEquals(uuid, read.getComponent( 5 ).getValue( uuidSerializer ));
    }


    private String getReversed( AbstractSerializer serializer ) {
        return serializer.getComparatorType().getTypeName() + "(reversed=true)";
    }
}>>>>>>> YOURS
